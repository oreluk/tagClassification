Id,Title,Body
1,R Error Invalid type (list) for variable,"<p>I am import matlab file and construct a data frame, matlab file contains two columns with and each row maintain a cell that has a matrix, I construct a dataframe to run random forest. But I am getting following error. </p>

<pre><code>Error in model.frame.default(formula = expert_data_frame$t_labels ~ .,  : 
  invalid type (list) for variable 'expert_data_frame$t_labels'
</code></pre>

<p>Here is the code how I import the matlab file and construct the dataframe:</p>

<pre><code>all_exp_traintest &lt;- readMat(all_exp_filepath);
len = length(all_exp_traintest$exp.traintest)/2;
    for (i in 1:len) {
      expert_train_df &lt;- data.frame(all_exp_traintest$exp.traintest[i]);
      labels = data.frame(all_exp_traintest$exp.traintest[i+302]);
      names(labels)[1] &lt;- ""t_labels"";
      expert_train_df$t_labels &lt;- labels;
      expert_data_frame &lt;- data.frame(expert_train_df);
      rf_model = randomForest(expert_data_frame$t_labels ~., data=expert_data_frame, importance=TRUE, do.trace=100);
    }
</code></pre>

<p>Structure of the Matlab input file</p>

<pre><code>[56x12 double]    [56x1 double]
[62x12 double]    [62x1 double]
[62x12 double]    [62x1 double]
[62x12 double]    [62x1 double]
[62x12 double]    [62x1 double]
[74x12 double]    [74x1 double]


&gt; str(all_exp_traintest)
List of 1
 $ exp.traintest:List of 604
  ..$ NA: num [1:56, 1:12] 0 0 0 0 8 1 1 0 0 0 ...
  ..$ NA: num [1:62, 1:12] 2 10 11 13 5 10 13 8 11 8 ...
  ..$ NA: num [1:62, 1:12] 0 0 1 0 0 0 0 0 1 1 ...
  ..$ NA: num [1:62, 1:12] 4 2 1 3 3 20 6 3 2 2 ...
  ..$ NA: num [1:62, 1:12] 2731 2362 2937 1229 1898 ...
  ..$ NA: num [1:74, 1:12] 27 33 34 38 33 35 36 35 47 46 ...
  ..$ NA: num [1:74, 1:12] 106 79 99 94 153 104 146 105 125 146 ...
  ..$ NA: num [1:74, 1:12] 3 9 3 0 1 26 0 4 0 0 ...
  ..$ NA: num [1:51, 1:12] 5 7 3 30 0 0 0 0 0 0 ...
  ..$ NA: num [1:66, 1:12] 0 0 13 0 0 3 2 2 0 2 ...
  ..$ NA: num [1:73, 1:12] 1 0 1 0 0 0 2 1 2 5 ...
  ..$ NA: num [1:73, 1:12] 23 14 20 14 24 22 32 61 84 278 ...
  ..$ NA: num [1:75, 1:12] 1 7 0 1 2 3 3 0 16 10 ...
  ..$ NA: num [1:90, 1:12] 10 7 8 15 25 12 37 31 18 48 ...
  ..$ NA: num [1:90, 1:12] 0 6 3 1 5 7 8 6 1 1 ...
  ..$ NA: num [1:90, 1:12] 0 1 1 2 0 4 9 6 3 4 ...
  ..$ NA: num [1:90, 1:12] 6 0 5 27 11 50 22 8 10 4 ...
  ..$ NA: num [1:90, 1:12] 3 9 13 12 4 0 5 0 5 0 ...
  ..$ NA: num [1:90, 1:12] 1 0 1 0 1 2 1 0 1 2 ...
  ..$ NA: num [1:90, 1:12] 3395 3400 3360 3770 3533 ...
  ..$ NA: num [1:84, 1:12] 0 0 0 0 5 0 0 5 4 2 ...
  ..$ NA: num [1:80, 1:12] 2 3 3 3 4 28 61 26 8 1 ...
  ..$ NA: num [1:81, 1:12] 4 28 22 9 16 43 80 21 19 18 ...
  ..$ NA: num [1:76, 1:12] 1 0 0 1 49 64 60 230 222 267 ...
  ..$ NA: num [1:76, 1:12] 4786 4491 2510 1144 2071 ...
  ..$ NA: num [1:76, 1:12] 80 128 254 109 114 267 152 139 368 363 ...
  ..$ NA: num [1:76, 1:12] 1 5 8 2 14 5 3 13 8 2 ...
  ..$ NA: num [1:76, 1:12] 10 3 8 79 4 4 11 30 2 0 ...
  ..$ NA: num [1:68, 1:12] 0 0 2 0 0 2 6 0 0 4 ...
  ..$ NA: num [1:68, 1:12] 1 4 5 2 2 3 3 1 3 0 ...
  ..$ NA: num [1:68, 1:12] 0 0 1 0 0 0 0 0 0 1 ...
  ..$ NA: num [1:69, 1:12] 39 45 2 0 1 4 3 0 13 0 ...
  ..$ NA: num [1:69, 1:12] 0 4 6 0 0 4 1 6 10 1 ...
  ..$ NA: num [1:69, 1:12] 0 2 5 2 2 2 0 0 3 6 ...
  ..$ NA: num [1:69, 1:12] 3 0 1 1 1 4 7 5 5 1 ...
  ..$ NA: num [1:66, 1:12] 5 0 0 0 0 0 0 1 3 5 ...
  ..$ NA: num [1:66, 1:12] 4 3 3 0 0 4 0 0 0 0 ...
  ..$ NA: num [1:65, 1:12] 0 0 1 0 0 0 5 8 4 1 ...
  ..$ NA: num [1:65, 1:12] 0 5 6 0 2 0 0 1 1 2 ...
  ..$ NA: num [1:69, 1:12] 0 16 5 1 14 0 1 0 0 16 ...
  ..$ NA: num [1:69, 1:12] 0 0 0 0 0 25 2 3 0 0 ...
  ..$ NA: num [1:64, 1:12] 2 0 0 0 0 0 0 0 0 0 ...
  ..$ NA: num [1:42, 1:12] 0 0 0 0 0 0 0 0 0 0 ...
  ..$ NA: num [1:67, 1:12] 0 2 4 10 15 4 1 43 1 7 ...
  ..$ NA: num [1:63, 1:12] 32 6 12 5 92 8 29 7 21 20 ...
  ..$ NA: num [1:63, 1:12] 2 5 12 8 10 13 6 11 10 14 ...
  ..$ NA: num [1:63, 1:12] 3 5 10 9 0 1 8 13 2 14 ...
  ..$ NA: num [1:54, 1:12] 0 0 14 0 0 0 0 0 0 1 ...
  ..$ NA: num [1:82, 1:12] 152 99 63 57 105 44 28 33 43 49 ...
  ..$ NA: num [1:81, 1:12] 0 1 0 0 0 0 0 0 0 0 ...
  ..$ NA: num [1:75, 1:12] 0 1 3 0 0 0 0 0 0 0 ...
  ..$ NA: num [1:75, 1:12] 1 0 0 2 0 1 0 0 0 0 ...
  ..$ NA: num [1:75, 1:12] 1 6 5 5 3 8 1 3 1 0 ...
  ..$ NA: num [1:72, 1:12] 0 0 0 0 1 0 1 2 0 0 ...
  ..$ NA: num [1:62, 1:12] 310 91 4 4 9 0 0 1 0 0 ...
  ..$ NA: num [1:62, 1:12] 239 374 1060 599 805 808 139 150 490 326 ...
  ..$ NA: num [1:49, 1:12] 9 18 10 12 19 5 13 10 2 3 ...
  ..$ NA: num [1:61, 1:12] 2 0 0 0 1 0 0 0 0 0 ...
  ..$ NA: num [1:61, 1:12] 4 10 16 15 8 14 10 23 11 5 ...
  ..$ NA: num [1:61, 1:12] 0 1 4 4 5 3 0 1 1 1 ...
  ..$ NA: num [1:65, 1:12] 165 100 177 65 148 58 188 55 59 62 ...
  ..$ NA: num [1:65, 1:12] 13 0 0 2 2 3 0 0 0 0 ...
  ..$ NA: num [1:66, 1:12] 157 58 101 92 15 21 73 80 78 75 ...
  ..$ NA: num [1:66, 1:12] 8 6 1 0 6 2 2 6 10 9 ...
  ..$ NA: num [1:87, 1:12] 1 2 5 6 8 3 3 3 2 3 ...
  ..$ NA: num [1:83, 1:12] 0 0 0 0 0 0 2 13 0 0 ...
  ..$ NA: num [1:81, 1:12] 0 0 1 0 3 5 3 0 2 7 ...
  ..$ NA: num [1:81, 1:12] 33 81 94 30 5 36 16 90 121 182 ...
  ..$ NA: num [1:81, 1:12] 10 11 16 6 0 0 0 1 0 0 ...
  ..$ NA: num [1:81, 1:12] 7 0 0 2 1 3 1 4 0 0 ...
  ..$ NA: num [1:81, 1:12] 1 0 5 0 2 3 1 0 1 1 ...
  ..$ NA: num [1:95, 1:12] 30 160 116 130 444 515 225 135 108 175 ...
  ..$ NA: num [1:95, 1:12] 12 1 0 10 3 3 0 4 0 0 ...
  ..$ NA: num [1:95, 1:12] 1 0 0 0 3 3 1 0 0 0 ...
  ..$ NA: num [1:95, 1:12] 11 42 61 23 41 56 81 6 83 82 ...
  ..$ NA: num [1:95, 1:12] 1 2 5 3 6 4 2 8 28 1 ...
  ..$ NA: num [1:95, 1:12] 283 192 377 216 207 261 394 262 262 554 ...
  ..$ NA: num [1:94, 1:12] 0 0 0 0 0 0 0 0 0 0 ...
  ..$ NA: num [1:72, 1:12] 0 0 0 0 0 0 0 0 0 0 ...
  ..$ NA: num [1:72, 1:12] 5 3 0 2 13 27 6 2 12 36 ...
  ..$ NA: num [1:72, 1:12] 0 2 2 0 1 0 1 4 2 2 ...
  ..$ NA: num [1:72, 1:12] 0 0 1 0 3 1 0 4 1 0 ...
  ..$ NA: num [1:67, 1:12] 27 7 18 1 2 0 0 0 0 0 ...
  ..$ NA: num [1:67, 1:12] 10 2 1 10 7 0 0 1 1 4 ...
  ..$ NA: num [1:67, 1:12] 14 17 9 20 13 20 18 13 10 7 ...
  ..$ NA: num [1:64, 1:12] 0 0 0 0 4 0 0 0 3 0 ...
  ..$ NA: num [1:64, 1:12] 3 0 1 0 2 7 13 14 4 2 ...
  ..$ NA: num [1:64, 1:12] 0 0 0 0 0 0 0 0 2 0 ...
  ..$ NA: num [1:72, 1:12] 59 61 55 120 49 202 325 244 377 551 ...
  ..$ NA: num [1:72, 1:12] 0 0 0 0 0 0 0 0 1 0 ...
  ..$ NA: num [1:72, 1:12] 0 3 1 0 1 0 0 0 4 0 ...
  ..$ NA: num [1:72, 1:12] 5 12 6 9 15 10 15 27 15 9 ...
  ..$ NA: num [1:72, 1:12] 7 0 3 0 0 1 1 1 1 0 ...
  ..$ NA: num [1:72, 1:12] 0 0 0 0 89 0 19 3 3 2 ...
  ..$ NA: num [1:61, 1:12] 5 3 5 3 3 29 46 140 49 24 ...
  ..$ NA: num [1:63, 1:12] 23 0 0 0 0 60 7 73 13 19 ...
  ..$ NA: num [1:95, 1:12] 7 96 28 2 9 5 8 190 166 1 ...
  ..$ NA: num [1:95, 1:12] 0 0 1 1 0 0 0 0 0 0 ...
  ..$ NA: num [1:95, 1:12] 4 0 2 6 6 11 6 5 6 9 ...
  .. [list output truncated]
 - attr(*, ""header"")=List of 3
  ..$ description: chr ""MATLAB 5.0 MAT-file, Platform: MACI64, Created on: Sun Dec  9 17:35:24 2012                                         ""
  ..$ version    : chr ""5""
  ..$ endian     : chr ""little""
</code></pre>

<p>After loading the matlab file into R</p>

<pre><code>all_exp_traintest$exp.traintest[1]
$&lt;NA&gt;
      [,1] [,2]  [,3]   [,4]   [,5]    [,6]     [,7]      [,8]       [,9]       [,10]        [,11]        [,12]
 [1,]    0  0.0  0.00  0.000 0.5000 0.03125 0.015625 0.0000000 0.00000000 0.000000000 0.0000000000 0.0000000000
 [2,]    0  0.0  0.00  1.000 0.0625 0.03125 0.000000 0.0000000 0.00000000 0.000000000 0.0000000000 0.0000000000
 [3,]    0  0.0  2.00  0.125 0.0625 0.00000 0.000000 0.0000000 0.00000000 0.000000000 0.0000000000 0.0000000000
 [4,]    0  4.0  0.25  0.125 0.0000 0.00000 0.000000 0.0000000 0.00000000 0.000000000 0.0000000000 0.0009765625
 [5,]    8  0.5  0.25  0.000 0.0000 0.00000 0.000000 0.0000000 0.00000000 0.000000000 0.0019531250 0.0000000000
 [6,]    1  0.5  0.00  0.000 0.0000 0.00000 0.000000 0.0000000 0.00000000 0.003906250 0.0000000000 0.0004882812
 [7,]    1  0.0  0.00  0.000 0.0000 0.00000 0.000000 0.0000000 0.00781250 0.000000000 0.0009765625 0.0009765625
 [8,]    0  0.0  0.00  0.000 0.0000 0.00000 0.000000 0.0156250 0.00000000 0.001953125 0.0019531250 0.0000000000
 [9,]    0  0.0  0.00  0.000 0.0000 0.00000 0.031250 0.0000000 0.00390625 0.003906250 0.0000000000 0.0004882812
[10,]    0  0.0  0.00  0.000 0.0000 0.06250 0.000000 0.0078125 0.00781250 0.000000000 0.0009765625 0.0000000000
[11,]    0  0.0  0.00  0.000 0.1250 0.00000 0.015625 0.0156250 0.00000000 0.001953125 0.0000000000 0.0000000000
[12,]    0  0.0  0.00  0.250 0.0000 0.03125 0.031250 0.0000000 0.00390625 0.000000000 0.0000000000 0.0004882812
[13,]    0  0.0  0.50  0.000 0.0625 0.06250 0.000000 0.0078125 0.00000000 0.000000000 0.0009765625 0.0000000000
[14,]    0  1.0  0.00  0.125 0.1250 0.00000 0.015625 0.0000000 0.00000000 0.001953125 0.0000000000 0.0024414062
[15,]    2  0.0  0.25  0.250 0.0000 0.03125 0.000000 0.0000000 0.00390625 0.000000000 0.0048828125 0.0014648438
[16,]    0  0.5  0.50  0.000 0.0625 0.00000 0.000000 0.0078125 0.00000000 0.009765625 0.0029296875 0.0039062500
[17,]    1  1.0  0.00  0.125 0.0000 0.00000 0.015625 0.0000000 0.01953125 0.005859375 0.0078125000 0.0151367188
[18,]    2  0.0  0.25  0.000 0.0000 0.03125 0.000000 0.0390625 0.01171875 0.015625000 0.0302734375 0.0019531250
[19,]    0  0.5  0.00  0.000 0.0625 0.00000 0.078125 0.0234375 0.03125000 0.060546875 0.0039062500 0.0029296875
[20,]    1  0.0  0.00  0.125 0.0000 0.15625 0.046875 0.0625000 0.12109375 0.007812500 0.0058593750 0.0253906250
[21,]    0  0.0  0.25  0.000 0.3125 0.09375 0.125000 0.2421875 0.01562500 0.011718750 0.0507812500 0.0253906250
[22,]    0  0.5  0.00  0.625 0.1875 0.25000 0.484375 0.0312500 0.02343750 0.101562500 0.0507812500 0.0063476562
[23,]    1  0.0  1.25  0.375 0.5000 0.96875 0.062500 0.0468750 0.20312500 0.101562500 0.0126953125 0.0009765625
[24,]    0  2.5  0.75  1.000 1.9375 0.12500 0.093750 0.4062500 0.20312500 0.025390625 0.0019531250 0.0000000000
[25,]    5  1.5  2.00  3.875 0.2500 0.18750 0.812500 0.4062500 0.05078125 0.003906250 0.0000000000 0.0019531250
[26,]    3  4.0  7.75  0.500 0.3750 1.62500 0.812500 0.1015625 0.00781250 0.000000000 0.0039062500 0.0029296875
[27,]    8 15.5  1.00  0.750 3.2500 1.62500 0.203125 0.0156250 0.00000000 0.007812500 0.0058593750 0.0009765625
[28,]   31  2.0  1.50  6.500 3.2500 0.40625 0.031250 0.0000000 0.01562500 0.011718750 0.0019531250 0.0000000000
[29,]    4  3.0 13.00  6.500 0.8125 0.06250 0.000000 0.0312500 0.02343750 0.003906250 0.0000000000 0.0083007812
[30,]    6 26.0 13.00  1.625 0.1250 0.00000 0.062500 0.0468750 0.00781250 0.000000000 0.0166015625 0.0000000000
[31,]   52 26.0  3.25  0.250 0.0000 0.12500 0.093750 0.0156250 0.00000000 0.033203125 0.0000000000 0.0048828125
[32,]   52  6.5  0.50  0.000 0.2500 0.18750 0.031250 0.0000000 0.06640625 0.000000000 0.0097656250 0.0034179688
[33,]   13  1.0  0.00  0.500 0.3750 0.06250 0.000000 0.1328125 0.00000000 0.019531250 0.0068359375 0.0229492188
[34,]    2  0.0  1.00  0.750 0.1250 0.00000 0.265625 0.0000000 0.03906250 0.013671875 0.0458984375 0.0297851562
[35,]    0  2.0  1.50  0.250 0.0000 0.53125 0.000000 0.0781250 0.02734375 0.091796875 0.0595703125 0.0771484375
[36,]    4  3.0  0.50  0.000 1.0625 0.00000 0.156250 0.0546875 0.18359375 0.119140625 0.1542968750 0.0004882812
[37,]    6  1.0  0.00  2.125 0.0000 0.31250 0.109375 0.3671875 0.23828125 0.308593750 0.0009765625 0.0000000000
[38,]    2  0.0  4.25  0.000 0.6250 0.21875 0.734375 0.4765625 0.61718750 0.001953125 0.0000000000 0.0048828125
[39,]    0  8.5  0.00  1.250 0.4375 1.46875 0.953125 1.2343750 0.00390625 0.000000000 0.0097656250 0.0000000000
[40,]   17  0.0  2.50  0.875 2.9375 1.90625 2.468750 0.0078125 0.00000000 0.019531250 0.0000000000 0.0000000000
[41,]    0  5.0  1.75  5.875 3.8125 4.93750 0.015625 0.0000000 0.03906250 0.000000000 0.0000000000 0.0000000000
[42,]   10  3.5 11.75  7.625 9.8750 0.03125 0.000000 0.0781250 0.00000000 0.000000000 0.0000000000 0.0004882812
[43,]    7 23.5 15.25 19.750 0.0625 0.00000 0.156250 0.0000000 0.00000000 0.000000000 0.0009765625 0.0078125000
[44,]   47 30.5 39.50  0.125 0.0000 0.31250 0.000000 0.0000000 0.00000000 0.001953125 0.0156250000 0.0000000000
[45,]   61 79.0  0.25  0.000 0.6250 0.00000 0.000000 0.0000000 0.00390625 0.031250000 0.0000000000 0.0000000000
[46,]  158  0.5  0.00  1.250 0.0000 0.00000 0.000000 0.0078125 0.06250000 0.000000000 0.0000000000 0.0004882812
[47,]    1  0.0  2.50  0.000 0.0000 0.00000 0.015625 0.1250000 0.00000000 0.000000000 0.0009765625 0.0000000000
[48,]    0  5.0  0.00  0.000 0.0000 0.03125 0.250000 0.0000000 0.00000000 0.001953125 0.0000000000 0.0000000000
[49,]   10  0.0  0.00  0.000 0.0625 0.50000 0.000000 0.0000000 0.00390625 0.000000000 0.0000000000 0.0000000000
[50,]    0  0.0  0.00  0.125 1.0000 0.00000 0.000000 0.0078125 0.00000000 0.000000000 0.0000000000 0.0000000000
[51,]    0  0.0  0.25  2.000 0.0000 0.00000 0.015625 0.0000000 0.00000000 0.000000000 0.0000000000 0.0000000000
[52,]    0  0.5  4.00  0.000 0.0000 0.03125 0.000000 0.0000000 0.00000000 0.000000000 0.0000000000 0.0000000000
[53,]    1  8.0  0.00  0.000 0.0625 0.00000 0.000000 0.0000000 0.00000000 0.000000000 0.0000000000 0.0000000000
[54,]   16  0.0  0.00  0.125 0.0000 0.00000 0.000000 0.0000000 0.00000000 0.000000000 0.0000000000 0.0000000000
[55,]    0  0.0  0.25  0.000 0.0000 0.00000 0.000000 0.0000000 0.00000000 0.000000000 0.0000000000 0.0000000000
[56,]    0  0.5  0.00  0.000 0.0000 0.00000 0.000000 0.0000000 0.00000000 0.000000000 0.0000000000 0.0000000000
</code></pre>
"
2,"What creates "".rds"" temporary files in R?","<p>I'm running a server process in R that is restarted often, and which uses many R packages. Right now, each restart creates a temporary directory that contains a couple of files with the ""rds"" extension. I'd really like to know which R package or feature causes these .rds files to be created (so I can disable that). Thanks!</p>

<p>The filenames look like this:</p>

<p>libloc_%2fusr%2flib%2fR%2flibraryVersion,Priority,Depends,Imports,LinkingTo,Suggests,Enhances,OS_type,License,Archs,Built.rds</p>

<p>and the beginning of the content look like this:</p>

<pre><code>      [,1]         [,2]                 [,3]          [,4]          [,5]                                              
 [1,] ""KernSmooth"" ""/usr/lib/R/library"" ""2.23-6""      ""recommended"" ""R (&gt;= 2.5.0), stats""                             
 [2,] ""MASS""       ""/usr/lib/R/library"" ""7.3-13""      ""recommended"" ""R (&gt;= 2.13.0), grDevices, graphics, stats, utils""
 [3,] ""Matrix""     ""/usr/lib/R/library"" ""0.999375-50"" ""recommended"" ""R (&gt;= 2.10.0), stats, methods, utils, lattice""   
 [4,] ""base""       ""/usr/lib/R/library"" ""2.13.0""      ""base""        NA                                                
 [5,] ""class""      ""/usr/lib/R/library"" ""7.3-2""       ""recommended"" ""R (&gt;= 2.5.0), stats, utils""                      
 [6,] ""compiler""   ""/usr/lib/R/library"" ""2.13.0""      ""base""        NA                                                
 [7,] ""datasets""   ""/usr/lib/R/library"" ""2.13.0""      ""base""        NA                                                
 [8,] ""grDevices""  ""/usr/lib/R/library"" ""2.13.0""      ""base""        NA                                                
 [9,] ""graphics""   ""/usr/lib/R/library"" ""2.13.0""      ""base""        NA                                                
[10,] ""grid""       ""/usr/lib/R/library"" ""2.13.0""      ""base""        NA                                                
[11,] ""lattice""    ""/usr/lib/R/library"" ""0.19-26""     ""recommended"" ""R (&gt;= 2.11.0)""                                   
[12,] ""methods""    ""/usr/lib/R/library"" ""2.13.0""      ""base""        NA                                                
[13,] ""nnet""       ""/usr/lib/R/library"" ""7.3-1""       ""recommended"" ""R (&gt;= 2.5.0), stats, utils""                      
[14,] ""spatial""    ""/usr/lib/R/library"" ""7.3-2""       ""recommended"" ""R (&gt;= 2.5.0), graphics, stats, utils""            
[15,] ""splines""    ""/usr/lib/R/library"" ""2.13.0""      ""base""        NA                                                
[16,] ""stats""      ""/usr/lib/R/library"" ""2.13.0""      ""base""        NA                                                
[17,] ""stats4""     ""/usr/lib/R/library"" ""2.13.0""      ""base""        ""methods, graphics, stats""                        
[18,] ""tcltk""      ""/usr/lib/R/library"" ""2.13.0""      ""base""        NA                                                
[19,] ""tools""      ""/usr/lib/R/library"" ""2.13.0""      ""base""        NA                                                
[20,] ""utils""      ""/usr/lib/R/library"" ""2.13.0""      ""base""        NA                                                
</code></pre>
"
3,How to generate a lower frequency version of a signal in Matlab?,"<p>With a sine input, I tried to modify it's frequency cutting some lower frequencies in the spectrum, shifting the main frequency towards zero. As the signal is not fftshifted I tried to do that by eliminating some samples at the begin and at the end of the fft vector:</p>

<pre><code>interval = 1;
samplingFrequency = 44100;
signalFrequency = 440;
sampleDuration = 1 / samplingFrequency;
timespan = 1 : sampleDuration : (1 + interval);
original = sin(2 * pi * signalFrequency * timespan);
fourierTransform = fft(original);
frequencyCut = 10; %% Hertz
frequencyCut = floor(frequencyCut * (length(pattern) / samplingFrequency) / 4); %% Samples
maxFrequency = length(fourierTransform) - (2 * frequencyCut);
signal = ifft(fourierTransform(frequencyCut + 1:maxFrequency), 'symmetric');
</code></pre>

<p>But it didn't work as expected. I also tried to remove the center part of the spectrum, but it wielded a higher frequency sine wave too.</p>

<p>How to make it right?</p>
"
4,In R how to make the Y-axis values in percenatge,"<p>I am plotting time series graph with forecasting. I need to take the Y-axis values as in percentage. How to proceed for this. Please specify. </p>

<pre><code>val &lt;- sqlQuery(dbhandle, 'select COUNT(*) from Admission where YEAR(YearOFRegistration)=2006 and YEAR(Admission)=2006); 
Ttl &lt;- sqlQuery(dbhandle, 'select COUNT(*) from Admission where YEAR(YearOFRegistration)=2006');
 firstyr&lt;-(val/Ttl)*100
</code></pre>

<p>Same way I am calculating for each year. </p>

<pre><code>&gt;  YrTimeSeries &lt;- c(firstyrs,secyr,thirdyr,forthyr,fifthyr)
tsValue&lt;-ts(YrTimeSeries,frequency=1,start=2006)
library(forecast)
plot(forecast(tsValue,h=5))
</code></pre>

<p><strong>Forecast values are</strong></p>

<pre><code>Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95
2011        86.9993 72.19680 101.8018 64.36083 109.6378
2012        86.9993 66.06645 107.9321 54.98528 119.0133
2013        86.9993 61.36233 112.6363 47.79094 126.2077
2014        86.9993 57.39653 116.6021 41.72576 132.2728
2015        86.9993 53.90256 120.0960 36.38220 137.6164
</code></pre>

<p>I am doing forecasting for next 6 years. I have included 2006 to 2010 years data and calculated in percentages. What I am worried about is I want to take predictive values in percentage of children taken addmision.Please elaborate where I am going wrong.</p>
"
5,"With sample variance, find interval for population variance","<p>The variance of in a random sample of 16 is 2.5. What is a 95% confidence interval for the variance of the population, assuming the population is normally distributed.</p>
"
6,How to add a calculated/computed column in numpy?,"<p>Suppose I have a numpy array:</p>

<pre><code>1 10
2 20
3 0
4 30
</code></pre>

<p>and I want to add a third column where each row is the sum (or some arbitrary calculation) of the first two columns in that row:</p>

<pre><code>1 10 11
2 20 22
3 0  3
4 30 34
</code></pre>

<p>How do I do that?</p>
"
7,solutions for factor levels when using linear models in R,"<p>I am running linear models to look at the significance of independent factors involved. 
The example model is: `</p>

<pre><code>mymod1 &lt;- lm(temp ~ bgrp+psex+tb,data=mydat)
summary(mymod1)`
</code></pre>

<p>I look at the summary to check out the significance of each factor:</p>

<pre><code>lm(formula = temp ~ bgrp + psex + tb, data = mydat)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.6877 -0.2454  0.0768  0.3916  1.6561 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 37.324459   0.186081 200.581  &lt; 2e-16 ***
bgrp         0.256794   0.066167   3.881 0.000115 ***
psex         0.144669   0.055140   2.624 0.008913 ** 
tb           0.019818   0.009342   2.121 0.034287 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

Residual standard error: 0.6888 on 621 degrees of freedom
  (5 observations deleted due to missingness)
Multiple R-squared: 0.03675,    Adjusted R-squared: 0.03209 
F-statistic: 7.897 on 3 and 621 DF,  p-value: 3.551e-05
</code></pre>

<p>Now, I would like to look at the <em>solutions</em> of the two levels of bgrp (1 and 2) and psex (1 and 2).</p>

<p>I would appreciate if you could help me with this.</p>

<p>Thanking you in advance,</p>

<p>Baz </p>

<p>EDIT:</p>

<p>I ran the first model you suggested and got the following:</p>

<pre><code>mydat$bgrp &lt;- as.factor(mydat$bgrp)

&gt; summary(lm(temp ~ bgrp+psex+tb-1,data=mydat))

Call:
lm(formula = temp ~ bgrp + psex + tb - 1, data = apirt)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.6877 -0.2454  0.0768  0.3916  1.6561 

Coefficients:
       Estimate Std. Error t value Pr(&gt;|t|)    
bgrp1 37.725922   0.135486 278.449  &lt; 2e-16 ***
bgrp2 37.982716   0.129558 293.171  &lt; 2e-16 ***
psex2  0.144669   0.055140   2.624  0.00891 ** 
tb     0.019818   0.009342   2.121  0.03429 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

Residual standard error: 0.6888 on 621 degrees of freedom
  (5 observations deleted due to missingness)
Multiple R-squared: 0.9997,     Adjusted R-squared: 0.9997 
F-statistic: 4.788e+05 on 4 and 621 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>From the above coefficient table, bgrp1 and bgrp2 seem to make sense: bgrp1 represents the maternal lines with larger litter sizes, lighter offsprings, which results in lower rectal temperature (37.70 degrees C) of the offspring. On the other hand, bgrp2 represents the terminal lines with smaller litter size, heavier offsprings, which results in higher a rectal temperature (37.98 degrees C).
 I am just wondering, if the same could be done for psex1 and psex2, but what is presented in the table of coefficients could be due to what you said earlier. </p>

<p>EDIT:
Hi Mark,</p>

<p>I tried the two options you suggested and I could see that bgrp1 and psex1 are taking on the same values:</p>

<pre><code>&gt; mybgrp &lt;- lm(formula = temp ~ bgrp+psex+tb-1, data = mydat)
&gt; mybgrp

Call:
lm(formula = temp ~ bgrp + psex + tb - 1, data = mydat)

Coefficients:
   bgrp1     bgrp2     psex2        tb  
37.72592  37.98272   0.14467   0.01982  

&gt; summary(mybgrp)

Call:
lm(formula = temp ~ bgrp + psex + tb - 1, data = mydat)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.6877 -0.2454  0.0768  0.3916  1.6561 

Coefficients:
       Estimate Std. Error t value Pr(&gt;|t|)    
bgrp1 37.725922   0.135486 278.449  &lt; 2e-16 ***
bgrp2 37.982716   0.129558 293.171  &lt; 2e-16 ***
psex2  0.144669   0.055140   2.624  0.00891 ** 
tb     0.019818   0.009342   2.121  0.03429 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

Residual standard error: 0.6888 on 621 degrees of freedom
  (5 observations deleted due to missingness)
Multiple R-squared: 0.9997,     Adjusted R-squared: 0.9997 
F-statistic: 4.788e+05 on 4 and 621 DF,  p-value: &lt; 2.2e-16 

&gt; mypsex &lt;- lm(formula = temp ~ psex+bgrp+tb-1, data = mydat)
&gt; mypsex

Call:
lm(formula = temp ~ psex + bgrp + tb - 1, data = mydat)

Coefficients:
   psex1     psex2     bgrp2        tb  
37.72592  37.87059   0.25679   0.01982  

&gt; summary(mypsex)

Call:
lm(formula = temp ~ psex + bgrp + tb - 1, data = mydat)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.6877 -0.2454  0.0768  0.3916  1.6561 

Coefficients:
       Estimate Std. Error t value Pr(&gt;|t|)    
psex1 37.725922   0.135486 278.449  &lt; 2e-16 ***
psex2 37.870591   0.135908 278.649  &lt; 2e-16 ***
bgrp2  0.256794   0.066167   3.881 0.000115 ***
tb     0.019818   0.009342   2.121 0.034287 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

Residual standard error: 0.6888 on 621 degrees of freedom
  (5 observations deleted due to missingness)
Multiple R-squared: 0.9997,     Adjusted R-squared: 0.9997 
F-statistic: 4.788e+05 on 4 and 621 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>Thanks!</p>
"
8,csv file with multiple time-series,"<p>I've imported a csv file with lots of columns and sections of data.</p>

<pre><code>v &lt;- read.csv2(""200109.csv"", header=TRUE, sep="","", skip=""6"", na.strings=c(""""))
</code></pre>

<p>The layout of the file is something like this:</p>

<pre><code>Dataset1
time, data, .....
0       0
0       &lt;NA&gt;
0       0

Dataset2
time, data, .....
00:00   0
0       &lt;NA&gt;
0       0
</code></pre>

<p>(The headers of the different datasets is exactly the same.</p>

<p>Now, I can plot the first dataset with:</p>

<pre><code>plot(as.numeric(as.character(v$Calls.served.by.agent[1:30])), type=""l"")
</code></pre>

<p>I am curious if there is a better way to:</p>

<ol>
<li><p>Get all the numbers read as numbers, without having to convert.</p></li>
<li><p>Address the different datasets in the file, in some meaningfull way.</p></li>
</ol>

<p>Any hints would be appreciated. Thank you.</p>

<p><hr /></p>

<p>Status update:</p>

<p>I haven't really found a good solution yet in R, but I've started writing a script in Lua to seperate each individual time-series into a seperate file. I'm leaving this open for now, because I'm curious how well R will deal with all these files. I'll get 8 files per day.</p>
"
9,Installing Numpy on 64bit Windows 7 with Python 2.7.3,"<p>It looks like the only 64 bit windows installer for Numpy is for Numpy version 1.3.0 which only works with Python 2.6</p>

<p><a href=""http://sourceforge.net/projects/numpy/files/NumPy/"" rel=""nofollow"">http://sourceforge.net/projects/numpy/files/NumPy/</a></p>

<p>It strikes me as strange that I would have to roll back to Python 2.6 to use Numpy on Windows, which makes me think I'm missing something.</p>

<p>Am I?</p>
"
10,On the semantics of the gamma distribution,"<p>Good Morning.</p>

<p>I have recently been faced with modeling a quantity that is best modeled via a Gamma distribution. I have noticed that, in the characterization of the distribution via the parameters $k$ and $θ$, the mean of the distribution is $kθ$, whereas the ""top"" of the distribution, representing the point that is sampled with the highest probability, is found at point $θ( k - 1)$.</p>

<p>This contradicts my intuition. If the mean represents the expected value of the distribution, then why is it not placed at the ""top"" of the distribution, much like in the normal distribution?</p>

<p>Thanks,</p>

<p>Jason</p>
"
11,Extracting values after pattern,"<p>A beginner question...</p>

<p>I have a list like this:</p>

<pre><code>x &lt;- c(""aa=v12, bb=x21, cc=f35"", ""xx=r53, bb=g-25, yy=h48"", ""nn=u75, bb=26, gg=m98"")
</code></pre>

<p>(but many more lines)</p>

<p>I need to extract what is between ""bb="" and "","". I.e. I want:  </p>

<pre><code>x21  
g-25  
26  
</code></pre>

<p>Having read many similar questions here, I suppose it is stringr with str_extract I should use, but somehow I can't get it to work. Thanks for all help.</p>

<p>/Chris</p>
"
12,Milliseconds puzzle when calling strptime in R,"<pre><code>options(digits.secs = 3);

&gt; strptime(""2007-03-30 15:00:00.007"", format = ""%Y-%m-%d %H:%M:%OS"");
[1] ""2007-03-30 15:00:00.007""
&gt; strptime(""2007-03-30 15:00:00.008"", format = ""%Y-%m-%d %H:%M:%OS"");
[1] ""2007-03-30 15:00:00.008""
&gt; strptime(""2007-03-30 15:00:00.009"", format = ""%Y-%m-%d %H:%M:%OS"");
[1] ""2007-03-30 15:00:00.008""
&gt; strptime(""2007-03-30 15:00:00.010"", format = ""%Y-%m-%d %H:%M:%OS"");
[1] ""2007-03-30 15:00:00.01""
&gt; strptime(""2007-03-30 15:00:00.011"", format = ""%Y-%m-%d %H:%M:%OS"");
[1] ""2007-03-30 15:00:00.010""
&gt; strptime(""2007-03-30 15:00:00.999"", format = ""%Y-%m-%d %H:%M:%OS"");
[1] ""2007-03-30 15:00:00.998""
</code></pre>

<p>I'm confused why there's one millisecond's difference from ""009"", then again from ""011"".</p>

<p>Thank you for your help!</p>
"
13,Where does the guassian function/normal or bell curve come from?,"<p>I am confused as to where the function for the normal distribtuion comes from.  Where does the e and pi come from?  In my textbook I am presented with the function,but I am unsure about where it came from.  I have spent hours on google and have yet to find a good proof that i can understand.  Come someone provide a source that shows an elementary proof of the normal curve function.  </p>
"
14,What happens when you increment an integer beyond its max value?,"<p>In Java what happens when you increment an int (or byte/short/long) beyond it's max value ? Does it wrap around to the max negative value ?</p>

<p>Does AtomicInteger.getAndIncrement() also behave in the same manner ?</p>
"
15,draw boxplot based on frequency table by R,"<p>I am wondering how to convert a frequency table like following to boxplot by ggplot2 to show distribution of each Group(Group as x axis and Frequency as y axis). It stuck me for a while since boxplot examples I found by google, y axis values are all need to be given by each data point, not frequency.</p>

<pre><code>    Group1  Group2  …   Group60
1-100   8   5       15
101-200 12  7       21
201-300 15  11      33
301-400 25  12      35
401-500 40  15      45
501-600 43  18      55
…               


data=read.table(file=""data.csv"",sep="","",header=TRUE,row.names=1)
df=melt(data)
df$variable=factor(df$variable)
names(df)=c(""Group"",""freq"")
</code></pre>

<p>then ?</p>
"
16,Coloring only the median in the boxplot,"<p>Is there a way to color only the median line of the boxplot and not the whole boxplot.</p>

<p>When I try this :</p>

<pre><code>boxplot(matrix,col=""red"")
</code></pre>

<p>then the whole box gets red colored.
I want to color only the median line of the boxplot.</p>

<p>Is there a way to do it in R ?? </p>
"
17,UTF-8 file output in R,"<p>I'm using R 2.15.0 on Windows 7 64-bit.  I would like to output unicode (CJK) text to a file.</p>

<p>The following code shows how a Unicode character sent to write on a UTF-8 file connection does not work as (I) expected:</p>

<pre><code>rty &lt;- file(""test.txt"",encoding=""UTF-8"")
write(""在"", file=rty)
close(rty)
rty &lt;- file(""test.txt"",encoding=""UTF-8"")
scan(rty,what=character())
close(rty)
</code></pre>

<p>As shown by the output of scan:</p>

<pre><code>Read 1 item 
[1] ""&lt;U+5728&gt;""
</code></pre>

<p>The file was not written with the UTF character itself, but some kind of ANSI-compliant fallback.  Can I make it work right the first time (i.e. with a text file that has ""在"" in it instead), or can I work some extra magic to convert the output to Unicode with the proper character replacing the code string?</p>

<p>Thanks.</p>

<p>[More info: the same code behaves <em>properly</em> in Cygwin, R 2.14.2, while 2.14.2 on Win7 is also broken.  Is this on my end somewhere?]</p>
"
18,"survival package, right censored data","<p>I account for right censored data in the analysis of my dataset. I am using the <code>survival</code> package - given cancer treatment tactics and when the patient last checked in with my clients clinic. </p>

<p>Is there a suggested method or manipulation to the standard <code>survival</code> package to account for <em>right-censored</em> data?</p>

<p>Our rows are unique individual patients...</p>

<p>Here are our columns that are filled out:</p>

<ul>
<li>List item</li>
<li>our treatment type (constant)</li>
<li>days since original diagnosis</li>
<li>'censored' which is the number of patients who were last heard on this day. Hence, We are          now uncertain if they are still alive or dead seen as they stopped attending the clinic. They should be removed from the probability estimate at all points in future. </li>
<li># of patients who died on that day (from original diagnosis)</li>
</ul>

<p>So do you recommend a manipulation of the standard survival package? Or using another package? I have seen <code>survSNP</code>, <code>survPRESMOOTH</code> and <code>survBIVAR</code> that may perhaps help. I want to avoid recalculations of the individual columns/fields and creating new objects of the R algorithm seeing as this is a small part of a very large dataset.</p>
"
19,Generate vectors using R,"<p>I would like to ask,if some of You dont know any simple way to solve this kind of problem:
I need to generate all combinations of A numbers taken from a set B (0,1,2...B), with their sum = C.</p>

<p>ie if <code>A=2</code>, <code>B=3</code>, <code>C=2</code>:</p>

<p>Solution in this case:</p>

<pre><code>(1,1);(0,2);(2,0)
</code></pre>

<p>So the vectors are length 2 (A), sum of all its items is 2 (C), possible values for each of vectors elements come from the set {0,1,2,3} (maximum is B).</p>
"
20,Preparing data for a CDF Plot using Statistics::Descriptive module?,"<p>Does anyone know how to prepare data to plot a CDF (I have a bunch of floating point numbers)? I was planning on using gnuplot and on first look, the <a href=""http://search.cpan.org/perldoc/Statistics%3a%3aDescriptive"" rel=""nofollow"">Statistics::Descriptive</a> module seemed the best fit but looks like I might need  some help here.</p>
"
21,How to calculate annuity (in PHP)?,"<p>I'm not even sure if I'm using the right term, however I've got the following:</p>

<p>I've borrowed 25.000, which I'm going to pay back in 2 years (24 months). Per month I should pay 0.491% of interest and per year I should pay 5.9% interest.</p>

<p>What I want to know is, how much money do I need to pay each month. I already know the answer for this specific question, which should be 1106.80 leaving me in the end with 0.02 'something'. Probably that I pay 2 cents too much in the last month.</p>

<p>How do I go about this?</p>
"
22,Python/Numpy - Extract a 2-d Sub-array from a 2-d array using variables,"<p>Ok, So I have a 2-d array of data which has the shape(23025, 1000), it's called 'allfiles'.</p>

<p>I need to go through the array 50 columns at a time and extract them to a sub-array for manipulation. The problem is when i address the array using the code below, it doesn't seem to recognize the variables (a and b). the code i have at the moment is shown below.</p>

<pre><code>    q = 50
    a = np.shape(allfiles)[1] # a = 1000
    for i in range(a):
        b = a + q
        data = allfiles[:,a:b]
</code></pre>

<p>When i replace the variables with number, i.e...</p>

<pre><code>    data = allfiles[:,30:80]
</code></pre>

<p>It seems to work. So, my question is - is there a way i can pass variables to the array index? If not is there a better way i can create a subarray using variables?</p>

<p>I have tried to find this problem on stack overflow with no luck, but i'm sure i'm not the first person to have this trouble?</p>

<p>Cheers guys,
Morgan</p>
"
23,How to get column data in R,"<p>I am new to R.  I want all of the data in C column only from this table and store it in a variable called ""x"".  Is there a way to do this in R?</p>

<p>something like this ""x=?????""</p>

<pre><code>A  B   C 
1  1   1 
3  5   6 
3  1   3 
5  1   4 
6  4   2 
5  1   6 
</code></pre>
"
24,optimization math computation (multiplication and summing),"<p>Suppose you want to compute the sum of the square of the differences of items:</p>

<p>$\sum_{i=1}^{N-1} (x_i - x_{i+1})^2$</p>

<p>the simplest code (the input is  <code>std::vector&lt;double&gt; xs</code>, the ouput <code>sum2</code>) is:</p>

<pre><code>double sum2 = 0.;
double prev = xs[0];
for (vector::const_iterator i = xs.begin() + 1;
 i != xs.end(); ++i)
{
sum2 += (prev - (*i)) * (prev - (*i)); // only 1 - with compiler optimization
prev = (*i);
}
</code></pre>

<p>I hope that the compiler do the optimization in the comment above. If <code>N</code> is the length of <code>xs</code> you have <strong><code>N-1</code> multiplications and <code>2N-3</code> sums</strong> (sums means <code>+</code> or <code>-</code>).</p>

<p>Now suppose you know this variable:</p>

<p>$x_1^2 + x_N^2 + 2 \sum_{i=2}^{N-1} x_i^2$</p>

<p>and call it <code>sum</code>. Expanding the binomial square:</p>

<p>$sum_i^{N-1} (x_i-x_{i+1})^2 = <code>sum</code> - 2\sum_{i=1}^{N-1} x_i x_{i+1}$</p>

<p>so the code becomes:</p>

<pre><code>double sum2 = 0.;
double prev = xs[0];
for (vector::const_iterator i = xs.begin() + 1;
 i != xs.end(); ++i)
{
sum2 += (*i) * prev;
prev = (*i);
}
sum2 = -sum2 * 2. + sum;
</code></pre>

<p>Here I have <strong>N multiplications and N-1 additions</strong>. In my case N is about 100.</p>

<p>Well, compiling with <code>g++ -O2</code> I got no speed up (I try calling the inlined function 2M times), why?</p>
"
25,collaborative filtering approach for tips/recommendations related to registered courses,"<p>I am looking at a specific problem where I need to build a recommender. 
The generalized problem is as follows,
Each user has registered for (say) x courses (c1, c2, c3, .. cx)
Depending on each course, I need to provide (say) top 5 tips/recommendations to the user (e.g. study materials that could be useful etc)
I need collaborative elements to be applied to learn what recommendations are proving helpful to users.<br/>
I looked at the recommendation engines like Apache Mahout Taste, but I am unable to model my problem in a way that it looks like the examples shown. (The extra filtering criteria where a user is associated with one or more courses and each recommendation/tip could be associated with one or more courses is throwing me off.)<br/>
Please let me know if there is any good way of modeling such a problem? Any pointers to documentation/examples would be very appreciated.
I am just starting my research in this area so please bear with me if I have misunderstood any concepts.</p>

<p>Thanks,<br/>
Vivek</p>
"
26,Is roll-a-dice script evenly distributed ? (with Math.random()),"<p>For this piece of script:</p>

<pre><code>var die = Math.floor(Math.random()*6 + 1);
</code></pre>

<p>It's expected to generate a random number between 1 to 6.</p>

<p>The die number 6 before rounded floor is 6.0 to 6.999...999</p>

<p>However, the die number 1 before rounded floor is 1.00...001 to 1.99...999</p>

<hr>

<p>Plus, because it is (random_nummber * 6) (""carry"" in number system?)</p>

<p>Is it possible that number of generated in (1 to 1.9999) is different to (2 to 2.999)?</p>

<p>(possible the difference is 1)</p>

<hr>

<p>Is this substantial/acceptable in real world? e.g. to fairly pick a customer for jack-pop. OR calculate possibility in gambling.</p>

<p>Or, did I do something wrong?</p>

<p>p.s. I'm not a math/science student, so I may miss a lot of math concepts.</p>
"
27,Sample complexity of PAC learning all k-DNFs over the uniform distribution,"<p>Is sample complexity of PAC learning all $k$-DNFs over the uniform distribution known (that is all DNFs with all terms of size at most $k$ and without restriction on the number of terms)? 
The only bounds I'm aware of are </p>

<ol>
<li>the obvious upper bound of $O(n^k/\epsilon)$ (which is true even for distribution independent learning)</li>
<li>the obvious lower bound of $\Omega(2^k)$ (even for a constant $\epsilon$).</li>
<li>the upper bound of $\tilde{O}(k^{k \log{1/\epsilon}})$ which is implied by Mansour's (1992) paper on learning DNF (the algorithm there uses membership queries but the structural result can also be used to get a sample upper bound).</li>
</ol>

<p>I'm primarily interested in lower bounds.
Also, anyone knows anything interesting about the same question for monotone $k$-DNF? For this case I don't know if even the trivial lower bound is true. Although slightly weaker bounds can be obtained by scaling lower bounds for learning monotone functions from Blum-Birch-Langford paper.</p>
"
28,Solving for optimal alignment of 3d polygonal mesh,"<p>I'm trying to implement a geometry templating engine.  One of the parts is taking a prototypical polygonal mesh and aligning an instantiation with some points in the larger object.</p>

<p>So, the problem is this: given 3d point positions for some (perhaps all) of the verts in a polygonal mesh, find a scaled rotation that minimizes the difference between the transformed verts and the given point positions.  I also have a centerpoint that can remain fixed, if that helps.  The correspondence between the verts and the 3d locations is fixed.</p>

<p>I'm thinking this could be done by solving for the coefficients of a transformation matrix, but I'm a little unsure how to build the system to solve.</p>

<p>An example of this is a cube.  The prototype would be the unit cube, centered at the origin, with vert indices:</p>

<pre><code>4----5
|\    \
| 6----7
| |    |
0 |  1 |
 \|    |
  2----3
</code></pre>

<p>An example of the vert locations to fit:</p>

<ul>
<li>v0: 1.243,2.163,-3.426 </li>
<li>v1: 4.190,-0.408,-0.485 </li>
<li>v2: -1.974,-1.525,-3.426 </li>
<li>v3: 0.974,-4.096,-0.485 </li>
<li>v5: 1.974,1.525,3.426 </li>
<li>v7: -1.243,-2.163,3.426</li>
</ul>

<p>So, given that prototype and those points, how do I find the single scale factor, and the rotation about x, y, and z that will minimize the distance between the verts and those positions?  It would be best for the method to be generalizable to an arbitrary mesh, not just a cube.</p>
"
29,loop a statistic test,"<p>I' m quite new in R. I have two data.frame that look like this: </p>

<p>DF1  </p>

<blockquote>
<pre><code> List_name     Smokers   Not_smokers    
  List1          30         100
  List2          10          50
  List3          3           10
  List4          12          85
  List5          56          60
  List6          90         120
</code></pre>
</blockquote>

<p>DF2</p>

<blockquote>
<pre><code> List_name      Male       Female    
  List1          23          123
  List2          45          654
  List3          35          110
  List4          145         850
  List5          89          234
  List6          56          765
</code></pre>
</blockquote>

<p>I would like to apply a prop.test between for ex: the first line of DF2 and each single line of DF1 and then the second line of DF2 and each single line of DF1, then the third line of DF2 and each single line of DF1 and so on until the end of all the lines of DF2. So, since  DF1 has dimension: 100(rows)X2(columns) and DF2 has 30(rows)X2(columns) finally I would have 30X100 tests so 3000 tests.  </p>

<p>What I tried is the following (using a function from another posted question): </p>

<pre><code>for (i in 1:length(DF2)){ 
   test &lt;- apply(DF1, 1, function(x) prop.test(rbind(x, as.numeric(DF2[[i]]), correct=TRUE,  alternative=""two.sided"", conf.level=.99))
}
</code></pre>

<p>But it doesn't work. Obviously there are errors that I'm not able to solve due to the fact that I'm a beginner. Another issue is that each test I perform will be saved in a separate variable and/or printed on a separate file. </p>

<p>Can anyone help me please?
If the question is not clear let me know. I'll edit it in a second.</p>

<p>Best, </p>

<p>F</p>
"
30,How do I control space between bars?,"<p>I am plotting bar chart using the following code: </p>

<pre><code>heights1=c(5,5,4.5,4)
barplot(heights1, main=""Language "", names.arg=c(""Hindi R/W"", ""Speak"" , ""English 
R/W"",""Speak""), ylab="" level "", xlab=""Language starting with mostly used"",  
cex.names=0.8, col=c(""darkblue"",""red""))
</code></pre>

<p>The output comes like this:</p>

<p><img src=""http://i.stack.imgur.com/yaEbQ.jpg"" alt=""enter image description here""></p>

<p>But What I want is that the ""Hindi R/w"" and ""Speak"" should combine <strong>without any gap</strong>, and then a space comes and then  ""English R/w"" and ""Speak"" should combine. How do I do this?    </p>
"
31,Bootstrap-related issue,"<p>Say I re-sample $N$ items with replacement from a numbered item sample of size $N$. What is the average number of data items that are not selected in each such sampling? </p>
"
32,Is there any way to force zoo::rollmean function to return a vector that is the same length as it's input? (or maybe use other function?),"<pre><code>input = cbind(c(3,7,3,5,2,9,1,4,6,4,7,3,7,4))
library(zoo)
output = cbind(rollmean(input,4))
print(input)
print(output)
</code></pre>

<p>output:</p>

<pre><code>      [,1]
 [1,]    3
 [2,]    7
 [3,]    3
 [4,]    5
 [5,]    2
 [6,]    9
 [7,]    1
 [8,]    4
 [9,]    6
[10,]    4
[11,]    7
[12,]    3
[13,]    7
[14,]    4
      [,1]
 [1,] 4.50
 [2,] 4.25
 [3,] 4.75
 [4,] 4.25
 [5,] 4.00
 [6,] 5.00
 [7,] 3.75
 [8,] 5.25
 [9,] 5.00
[10,] 5.25
[11,] 5.25
</code></pre>

<p>but when I try to cbind it:</p>

<pre><code>Error in cbind(input, output) :
  number of rows of matrices must match (see arg 2)
Calls: print -&gt; cbind
Execution halted
</code></pre>

<p>I'd like to use a function that would be smart enough and do not give up if it doesn't get data on both ends of a vector and calculating output then according to only the data it is having. so for example in input[1] it will calculate only mean from right</p>
"
33,Fitting a linear model,"<p>I have a data frame that looks like</p>

<pre><code>&gt; t
          Institution Subject Class       ML1     ML1SD
aPhysics0           A Physics     0 0.8730469 0.3329205
aPhysics1           A Physics     1 0.8471074 0.3598839
aPhysics2           A Physics     2 0.8593750 0.3476343
aPhysics3           A Physics     3 0.8875000 0.3159806
aPhysics4           A Physics     4 0.7962963 0.4027512
</code></pre>

<p>And I want to fit a linear function to <code>ML1</code> against <code>Class</code>, but when I call</p>

<pre><code>&gt; lm(ML1 ~ Class, data=t)
</code></pre>

<p>I get:</p>

<pre><code>Call:
lm(formula = ML1 ~ Class, data = t)

Coefficients:
(Intercept)       Class1       Class2       Class3       Class4  
    0.87305     -0.02594     -0.01367      0.01445     -0.07675  
</code></pre>

<p>Which I don't really understand, because it looks like it is giving me multiple gradient values for each value of <code>Class</code>, but there are 5 <code>Class</code> values (0-4). But what I want is a single intercept and a single gradient value.</p>

<p>Also, when I call <code>lm</code> with <code>weights = 1/ML1SD^2</code> it does not change any of the values.</p>

<p>What am I doing wrong?</p>
"
34,"Mean equal to standard deviation, with positive values","<p>I read the question and the answers regarding the idea of the mean being equal to the standard deviation, in which the mean can always be adjusted to equal anything.  But, if the data is restricted to only positive values, then it seems the normal distribution would not work, while the exponential distribution and Poisson would work.  Any ideas?</p>
"
35,Use Math.Pow and keep the sign?,"<p>Is there anyway I can do this without using ifs?  Like I want to do Math.Pow(-5f,2f) to come out as -25 not 25?</p>
"
36,Error while compiling RInside code,"<p>I want to compile a R code using RInside. But I am getting errors while using the function read.csv. The code snippet is given below:  </p>

<pre><code>include ""RInside.h""
include &lt;iomanip&gt;  
include &lt;iostream&gt;  
include &lt;fstream&gt;  
include &lt;string&gt;    
include &lt;vector&gt;   
include &lt;sstream&gt;    
using namespace std;

int main(int argc,char*argv[])
{   
 RInside R(argc,argv);  
 SEXP ans;  
 R.parseEvalQ(""library(plotrix)"");  
 R.parseEvalQ(""fileContents&lt;-read.csv(""/home/nibha/manoj/test.csv"")"");  
 R.parseEvalQ(""nr &lt;-nrow (filecontents)"");  
 R.parseEvalQ(""nc &lt;-ncol (filecontents)"");  
}  
</code></pre>

<p>I am getting the errors as follows:  </p>

<pre><code>: In function ‘int main(int, char**)’:  
prog3.cpp:14: error: ‘home’ was not declared in this scope  
prog3.cpp:14: error: ‘nibha’ was not declared in this scope  
prog3.cpp:14: error: ‘manoj’ was not declared in this scope  
prog3.cpp:14: error: ‘test’ was not declared in this scope  
prog3.cpp:20: error: ‘myfile’ was not declared in this scope  
</code></pre>
"
37,php operator precedence,"<p>I'm having trouble understanding how php calculates standard math functions. In a specific example I have this calculation:</p>

<p>225 + 154 * 256 + 138 * 256 * 256 + 81 * 256 * 256 * 256
(thats correct, no brackets)</p>

<p>which when executed with php produces this number:
1,368,038,113</p>

<p>Now when I look at this logically and work through the sum from left to right, this number doesn't even come close. using a cheap simple calculator, it gives up trying to calculate it before the last two multiplications of 256 because the number gets too big.</p>

<p>How is it possible to end up with such a relatively small number from a calculation with 6 multiplications by 256?</p>

<p>A breakdown of how php actually would work out this answer would be great.</p>

<p>p.s. i read through this page: <a href=""http://www.homeandlearn.co.uk/php/php2p8.html"" rel=""nofollow"">http://www.homeandlearn.co.uk/php/php2p8.html</a> which still didn't help me with the above.</p>
"
38,Max Length for a Vector in R,"<p>According to the R 'Memory-limits' documentation, it isn't possible to allocate a vector of length longer than 2^31-1. This is because the integer used as an index can only use 31 bits (one bit for the sign). But on a 64-bit system, I should be able to allocate longer vectors.
Why does R impose this same max length on 64-bit systems? Is there a way to circumvent the limit?</p>
"
39,Get full properties of R object,"<p>There is a function I have used before to get the full properties of an object in R (I used it before to find out why two dataframes which looked identical failed the checkEquals RUnit check).</p>

<p>The structure (<code>str()</code>) and attributes (<code>attributes()</code>) look identical. Can anyone point me in the right direction as to what the function which shows the full properties of an object is please?</p>
"
40,calculate (complicated) array of decimal numbers in C#,"<p>Hello everyone and Merry Christmas for the ppl celebrating today.<br>
I have issue and maybe someone can help me.<br>
I have a listbox where users can enter decimal numbers.<br>
Lets say they'll enter 5 numbers:</p>

<pre><code>1.1
1.2
1.3
1.4 
1.5
</code></pre>

<p>I need to get the sum of all the variations in those 5 numbers. 
For example sum of <code>1.1 and 1.2</code> then <code>1.1 1.2 1.3</code> then <code>1.1 1.2 1.3 1.4</code>, then <code>1.2 1.4 1.5</code> then <code>1.1 1.3 1.5</code>.<br>
I started something but that goes through all the variations only skipping one number at a time:</p>

<pre><code>List&lt;Double[]&gt; listNumber = new List&lt;double[]&gt;();            
Double[] array;            
for (int i = 0; i &lt; listBox1.Items.Count; i++)
{
    array = new Double[listBox1.Items.Count];                
    for (int k = 0; k &lt; listBox1.Items.Count; k++)
    {
        if (!k.Equals(i))
        {
            array[k] = (Convert.ToDouble(listBox1.Items[k]));                       
        }
    }
    listNumber.Add(array);
}   
</code></pre>

<p>I need to find a way how to calculate the way I want, if anyone can give me sime idea it will be great Christmas gift :)
Thanks in advance, Laziale</p>
"
41,R read in big data and logistic regresion,"<p>Situation:
1GB CSV file, 100000 rows, 4000 independent numeric variable, 1 depandent variable.
R on Windows Citrix Server, with 16GB memory.</p>

<p>Problem:
It took me 2 hour! to do 
read.table(""full_data.csv"", header=T, sep"","")</p>

<p>and the </p>

<p>glm process crashes, the program is not responding, and I have to shut it down in Task Manager.</p>

<p>Help needed.</p>

<p>Tony</p>
"
42,"Calculate error, MSE and MAPE?","<p>I created this program to estimate the Mean Squared Error (MSE), and Mean absolute percent error (MAPE):
Is everything all right with this?
<code>pune</code> is an .csv file with 22 data points.</p>

<pre><code>pune &lt;- read.csv(""C:/Users/ervis/Desktop/Te dhenat e konsum energji/pune.csv"", header=T,dec="","", sep="";"")
pune &lt;- data.matrix(pune,rownames.force=NA)
m1 &lt;- seq(from = 14274.19, to = 14458.17, length.out = 10000)
MSE1 &lt;- numeric(length = 10000)
for(i in seq_along(MSE1)) {
 MSE1[i] &lt;- 1 / length(pune) * sum((pune-m1[i]) ^ 2)
}
MAPE1 &lt;- numeric(length = 10000)
for(i in seq_along(MAPE1)) {
 MAPE1[i] &lt;- 1 / length(pune) * sum(abs((pune-m1[i]) / pune))
}
</code></pre>

<p>Am I right?</p>
"
43,How to use a parabola formula in AS3 for firing an arrow that will always intercept a given point,"<p>First note: mathematically, I'm not that skilled at all.</p>

<p>I played a game on iPhone a while back where you press a point, and an arrow fires from your castle which will always intersect the point you pressed. I wanted to make a similar game, thinking it would be an easy quick make; then I ran into the realization that the mathematics for this is actually beyond my skill level.</p>

<p>I'm assuming they're using a parabola formula or something which would determine the velocity and angle needed when the arrow is launched for the arrow to always intersect the clicked point.</p>

<p>I only vaguely remember how parabolas work from school and have no chance of working out any formulas.</p>

<p>Any mathematical help or ideas that might be easier to implement would be great.</p>

<p>I want to end up with a function in my castle like so:</p>

<pre><code>package
{
    import avian.framework.objects.AvElement;

    public class Castle extends AvElement
    {
        /**
         * Fires an arrow from this
         * @param ix The x intersection point
         * @param iy The y intersection point
         */
        public function fire(ix:Number, iy:Number):void
        {
            var ar:Arrow = new Arrow();

            ar.x = x;
            ar.y = y;

            // define angle and velocity based on ix, iy
            // ar.fireAngle = ??
            // ar.fireVelocity = ??

            parent.addChild(ar);
        }
    }
}
</code></pre>

<p><strong>Update</strong> as per questions in comments:</p>

<p>There will be no forces applied to the arrow such as wind, friction, etc. Also, the starting point of the arrow is fixed throughout the game (at the castle).</p>

<p>Here is an example image for slightly more clarity:
<img src=""http://i.stack.imgur.com/Uu0Ml.gif"" alt=""Arrow path""></p>

<p>To be as clear as possible:</p>

<ol>
<li>Arrow always begins its journey from a fixed point (say: 40, 120).</li>
<li>The arrow must always intercept a given coordinate.</li>
<li>A realistic as possible path is something I'd like to achieve (obviously I can just fire an arrow straight to intercept any point, but the <strong>goal</strong> is to have the arrow first rise, then descend; passing through the desired coordinate at the most realistic point in its journey).</li>
</ol>

<p><strong>Note:</strong> To avoid the issue of there being infinite possible parabolas - the velocity of the arrow can be fixed - just look at defining the angle the arrow can leave at.</p>
"
44,Software that shows network activity?,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://superuser.com/questions/22569/monitor-all-and-any-internet-traffic-from-my-home-pc-what-should-i-use"">Monitor all and any internet traffic from my home PC - what should I use?</a>  </p>
</blockquote>



<p>I am looking for a software that monitors, shows and (hopefully) logs network activity like open connections, bytes transferred per connection and the like. Any ideas?</p>

<p>I am using Windows 7.</p>
"
45,What is the best way to evaluate mathematical expression in C++?,"<p>What is the best way to evaluate any custom math expression, for example</p>

<pre><code>3+sqrt(5)+pow(3)+log(5)
</code></pre>

<p>I know that embedding Python into C++ can do that; is there any better way?</p>

<p>Thanks!</p>
"
46,Saving multiple boxplots,"<p>I've made a loop to create multiple boxplots. The thing is, I want to save all the boxplots without overwriting each other. Any suggestions?</p>

<p>This is my current code:</p>

<pre><code>boxplot &lt;- list()
for (x in 1:nrow(checkresults)){
    boxplots &lt;- boxplot(PIM[,x], MYC [,x], OBX[,x], WDR[,x], EV[,x], 
                        main=colnames(PIM)[x], 
                        xlab=""PIM, MYC, OBX, WDR, EV"")
}
</code></pre>
"
47,Can symbolic AI 'learn' a data model?,"<p>Perceptrons, a simple form of supervised machine learning, must be trained with a set of known good inputs before they can ""learn"" by adjusting internal weights assigned to inputs, based on the accuracy of its results.</p>

<p>Similarly, we know that <a href=""http://www.cs.indiana.edu/~gasser/Salsa/rl.html"" rel=""nofollow"">reinforcement learning</a> and <a href=""http://www.willamette.edu/~gorr/classes/cs449/Unsupervised/unsupervised.html"" rel=""nofollow"">unsupervised</a> neural networks are able to learn without any known model of the problem; they can be designed to collect information about an environment only by interacting with it.</p>

<p>Can symbolic AI be used to design a system that can achieve this ""naive learning,"" or is this a property exclusive to certain soft computing techniques?</p>
"
48,Representing code algebraically,"<p>I have a number of small algorithms that I would like to write up in a paper. They are relatively short, and concise. However, instead of writing them in pseudo-code (à la Cormen or even Knuth), I would like to write an algebraic representation of them (more linear and better LaTeX rendering) . However, I cannot find resources as to the best notation for this, if there is anything: e.g. how do I represent a loop? If? The addition of a tuple to a list?</p>

<p>Has any of you encountered this problem, and somehow solved it?</p>

<p>Thanks.</p>

<p><strong>EDIT</strong>: Thanks, people. I think I did a poor job at phrasing the question. Here goes again, hoping I make it clearer: what is the common notation for talking about loops and if-then clauses in a mathematical notation? For instance, I can use <code>$acc \leftarrow acc \cup \langle i,i+1 \rangle$</code> to represent the ""add"" method of a list.</p>
"
49,"plotting a very asymmetric ""pixel matrix"" with R","<p>All the examples i find that use plot.im or similar, use matrices that are squared or almost squared. I am trying to plot pixel matrices with unusual sizes, for example 18x3000. My problem is that since the x/y ratio is so small, i only see a line in my plots. Is there any way to 'scale' the Y side of the image?</p>

<p>here is an example <a href=""http://ubuntuone.com/0WchsKI5hd8YEqtTe7xAno"" rel=""nofollow"">http://ubuntuone.com/0WchsKI5hd8YEqtTe7xAno</a></p>

<p>Thanks! </p>
"
50,NaN Multiplication Result When Negative Number is Contained within a Vector,"<p>I am experience a problem calculating the result when putting a vector to the power of a decimal. The problem is more specifically that negative numbers in the vector come back as NAN. This also happens if I assign the negative number to a variable. Some examples to explain:</p>

<pre><code>&gt; -5.718301^2.85
[1] -143.9498

&gt; test&lt;--5.718301
&gt; test
[1] -5.718301
&gt; test^2.85
[1] NaN

&gt; c(5.718301)^2.85
[1] 143.9498
&gt; c(-5.718301)^2.85
[1] NaN
</code></pre>

<p>Can anyone explain to me why this happens? And any way around it? The vector in question may have positive and negative values, therefore, I can't assign a negative sign after.</p>

<p>Thanks in advance,
Aran</p>
"
51,Retrieve Edges from Normal Vector,"<p>Im reading a Wavefront .obj file where the normals are provided. And I wish to calculate the tangent manually.</p>

<p>Is there any way I could retrieve the edge1 and edge2 that create the normal vector?</p>
"
52,R - Contour Map with multiple layers (lattice),"<p>Here is my code and associated variable structures.</p>

<pre><code>Correlation_Plot = contourplot(cor_Warra_SF_SST_JJA, region=TRUE, at=seq(-0.8, 0.8, 0.2), 
labels=FALSE, row.values=(lon_sst), column.values=lat_sst,
xlab='longitude', ylab='latitude')

Correlation_Plot = Correlation_Plot + layer({ ok &lt;- (cor_Warra_SF_SST_JJA&gt;0.6);
            panel.text(cor_Warra_SF_SST_JJA[ok]) })
Correlation_Plot

     # this is the longitude (from -179.5 to 179.5) , 360 data in total
    &gt; str(lon_sst) 
     num [1:360(1d)] -180 -178 -178 -176 -176 ...

     # this is the latitude (from -89.5 to 89.5), 180 data in total 
    &gt; str(lat_sst) 
     num [1:180(1d)] -89.5 -88.5 -87.5 -86.5 -85.5 -84.5 -83.5 -82.5 -81.5 -80.5 ...

     # This is data set corresponding to longitude and latitude  
     &gt; dim(cor_Warra_SF_SST_JJA) 
       [1] 360 180
</code></pre>

<p><img src=""http://i.stack.imgur.com/bxNRR.jpg"" alt=""enter image description here""></p>

<p>I tried to use <code>layer()</code> to show the label just for contour bigger than 0.6, but it doesn't work.</p>

<ol>
<li><p>Is it possible to increase the colour contrasts in the legend so it can be really clear what colour responds to what level. What are colour options, i can't find them?</p></li>
<li><p>The most important is I want to draw a thicker black line for a specified contour interval (e.g. for +/- 0.2)? I think ican do it with <code>layer()</code> as well, but i am not sure what <code>panel</code> function should i use.</p></li>
<li><p>Also, i tried to fill in the continent with a solid colour, but i can't find any thing with it.
I have tried use map, but it doesn't work for lattice.</p></li>
</ol>

<p>Thanks for your help.</p>
"
53,Ballistics Library,"<p>Does anyone know if there has been a port of the GNU Ballistics library to C#, java, vb, etc?  Is there a similar library out there in any other language, even C++?</p>
"
54,Understanding predict( ) in R,"<p>I am reading through predict() in R and am confused:</p>

<p>There is a dataset Spam from which we have created a train data and test data using random sampling. We have used the trainSpam(training data set to train the system). We want to see how good the model is, by testing on the test dataset(testSpam).</p>

<pre><code>predictionModel = glm(numType ~ charDollar, family = ""binomial"", data = trainSpam)

predictionTest = predict(predictionModel, testSpam) 
predictedSpam = rep(""nonspam"", dim(testSpam)[1]) 
predictedSpam[predictionModel$fitted &gt; 0.5] = ""spam"" #Here is my problem
table(predictedSpam, testSpam$type)
</code></pre>

<p>In the line where we say:</p>

<pre><code>predictedSpam[predictionModel$fitted &gt; 0.5] = ""spam"" 
</code></pre>

<p>How does <code>predictionModel$fitted</code> predict spams in the test data. It seems to be using predictionModel$fitted from the training data. Then we go on to compare with the spams of test data. Can someone explain?</p>

<p>Here is what I understood. In the line:</p>

<blockquote>
  <p>predictionModel = glm(numType ~ charDollar, family = ""binomial"", data
  = trainSpam)</p>
</blockquote>

<p>We create a model using the trainSpam data.</p>

<p>In the next line:</p>

<blockquote>
  <p>predictionTest = predict(predictionModel, testSpam)</p>
</blockquote>

<p>We create predictionTest using the same model but the test data.</p>

<p>In the next line:</p>

<blockquote>
  <p>predictedSpam = rep(""nonspam"", dim(testSpam)[1])</p>
</blockquote>

<p>We created a vector with all values ""nonspam""</p>

<p>In the next line:</p>

<blockquote>
  <p>predictedSpam[predictionModel$fitted > 0.5] = ""spam""</p>
</blockquote>

<p>We are using the predictionModel$fitted, which has been fitted over the training data to decide which of the rows are to be classified as spam. Shouldn't we rather use something like predictionTest to identify the spams?</p>

<p>My idea of what it should be is:</p>

<pre><code>&gt; predictionModel = glm(numType ~ charDollar, family = ""binomial"", data = trainSpam)

&gt; predictionTest = predict(predictionModel, testSpam,type=""response"")
&gt; predictedSpam = rep(""nonspam"", dim(testSpam)[1])
&gt; predictedSpam[predictionTest &gt; 0.5] = ""spam""
&gt; table(predictedSpam, testSpam$type)
</code></pre>
"
55,python multiprocess pinning,"<p>I am currently using python multiprocess to do some simple parallel programming.
I use an async decorator </p>

<pre><code>def async(decorated):
    module = getmodule(decorated)
    decorated.__name__ += '_original'
    setattr(module, decorated.__name__, decorated)
    def send(*args, **opts):
        return async.pool.apply_async(decorated, args, opts)
    return send
</code></pre>

<p>and then</p>

<pre><code>@async
def evalfunc(uid, start, end):
        veckernel(Posx, Posy, Posz, Quant, Delta)
        return (uid, GridVal)

def runit(outdir):
    async.pool = Pool(8)
    results = []
    for uid in range(8):
        result = evalfunc(uid,Chunks[uid], Chunks[uid+1])
            results.append(result)
</code></pre>

<p>If I run this on a 8 processor or 8 cores machine it essentially uses only two cores. Why is that? Is there a way to do proper core pinning like with pthreads?</p>

<p>Thanks a lot,
Mark</p>
"
56,Compiling C code with R header files,"<p>I need R header files to compile something. I installed R on ubuntu via:</p>

<pre><code>sudo apt-get r-base-dev
</code></pre>

<p>R works fine. But I can't find the  R headers such as Rmath.h. I get this error when I try to compile:</p>

<pre><code>gcc -g -std=c99 -I/usr/include -O3 -lRmath -lm -lpthread -o qcpg qcpg.c
In file included from qcpg.c:1:0:
qcpg.h:19:19: fatal error: Rmath.h: No such file or directory
compilation terminated.
make: *** [qcpg] Error 1
</code></pre>

<p>Any ideas??</p>
"
57,Passing a list of files to XLConnect,"<p>I've written a script based on a for-loop to read in columns of multiple .xls files, combine them to a single data frame, search for negative values and write a .txt file with these values and the name of the file.<br>
The script works basically, but I have several hundred files to process, and it's a bit slow. This version of the script is only a basic framework for later statistical analysis, and I want to parallelize the execution to speed it up.<br>
I've tried to avoid the for-loop by applying the function via lapply and the plyr-package, but had problems passing the file list to ""readWorkSheetFromFile"" (Error in path.expand (filename) : invalid 'path' argument).</p>

<p>Here is the working script:  </p>

<pre><code>require(XLConnect)
setwd(choose.dir())

input = list.files(pattern = "".xls$"")

# creates empty data frame 
df = data.frame(Name=NULL, PCr=NULL, bATP=NULL, Pi=NULL)

for(i in seq(along=input)){
    data = data.frame(readWorksheetFromFile(input[i], sheet=""Output Data"", 
    startRow=2, startCol=c(10, 13, 16), endCol=c(10, 13, 16), header=TRUE))

    head(data, n = -1L)

    colnames(data) = c(""PCr"", ""bATP"", ""Pi"")
    data$Name = file.path(input[i])

    attach(data)
    df = rbind(data, df)
    attach(df)
    rm(data)
}

# searches for negative values in df and writes to txt file 
neg_val = subset(df, bATP&lt;0 | Pi&lt;0 | PCr&lt;0)
write.table(neg_val, file = ""neg_val.txt"", sep = ""\t"", quote=F)
</code></pre>

<p>Any solutions to this problem, or other suggestions to speed up execution? </p>

<p>Thanks,
Markus   </p>
"
58,Bolding Greek letters in R,"<p>I want to add a greek letter to my the margin of a figure, and I used the line of code below:  </p>

<pre><code>mtext(side=2,line=6,text=expression(paste(beta,""-Actin Normalized"")),font=2,cex=2)
</code></pre>

<p>However, when I use the mtext function in combination with greek letters it no longer bolds the line of text.  Is there any good way to add margin text with greek letters and retain bold font?  </p>

<p>Thanks.</p>
"
59,legends are overflowing from the ggplot chart,"<p>I am using ggplot2 to chart bunch of items in one chart. Sincere there could be 1 to 60 or greater items, Iam using </p>

<pre><code>ggplot(df, aes(Date, Value, colour=item, group=item)) + geom_point(size=0.4)    + guides(col = guide_legend(nrow = 30, byrow=TRUE))
</code></pre>

<p>to make sure I have 30 rows. If it is greater than 30 items, next legend item shows ups on the second column and so forth.</p>

<p>But If I have fewer items like 2 or 3, legends are overthrown from the chart window.</p>

<p>Is there an option to force the legend to justify vertically with ggplot?</p>
"
60,Negative binomial in GEE,"<p>For R packages implementing GEE such as <code>gee</code>, <code>geepack</code>, it seems that the negative binomial family is not included. I have two questions:</p>

<ol>
<li><p>Are there any other R packages for GEE that I am not aware of?</p></li>
<li><p>If not, is there a simple step to allow the creation of a family, i.e providing the link function (<code>log mu</code>) and the variance function (<code>mu + mu^2/theta</code>), assuming <code>theta</code> is specified (otherwise the NB is not a GLM) and then to let the <code>gee</code> or <code>geepack</code> codes do the business in a similar fashion to <code>glm</code>?</p></li>
</ol>
"
61,Languages/Methods to Learn for Scientific Computing?:,"<p>I'm a second-semester Junior working towards a Computer Science degree with a Scientific Computing concentration and a Mathematics degree with a concentration on Applied Discrete Mathematics. So, number crunching and such rather than a bunch of regular expressions, interface design, and networking.</p>

<p>I've found that I'm not learning new relevant languages from my coursework and am interested in what the community would recommend me to learn. I know as far as programming methods go, I need to learn more about parallelizing programs, but if there's anything else you can recommend, I would appreciate it.</p>

<p>Here's a list of the languages with which I am very experienced (web technologies omitted as they barely apply here). Any recommendations for additional languages I should learn would be very much appreciated!:</p>

<ul>
<li>Java</li>
<li>C</li>
<li>C++</li>
<li>Fortran77/90/95</li>
<li>Haskell</li>
<li>Python</li>
<li>MATLAB</li>
</ul>
"
62,an array of arrays varied in length in R,"<p>I use R for my statistical analysis.
I wanna group my data in an array based on the ID column. This results in having an array of unique IDs which each cell includes a data array of correspondence ID. Since the number of the data per ID is not similar, therefor each array in each cell has different length.</p>

<p>So I wonder how I can create an array of arrays varied in length using R?</p>

<p>I already having the following codes but get an error:</p>

<pre><code>#number of unique IDs
size&lt;-unique(data[,1]);


for (i in 1:length (gr))
  {
  index&lt;- which(data[,1]==gr[i]);
    data_c[[i,1]]&lt;-data[index,];
}
</code></pre>

<p>Here is the error</p>

<blockquote>
  <p>more elements supplied than there are to replace</p>
</blockquote>

<p>Thanks in advance for any comment.</p>

<p>I explain my problem by an example:</p>

<p>I have following data called it DATA_ALL:</p>

<pre><code>DATA_ALL[]=
       id     age   T1     T2    T3     T4 
       1       20     1      0     0      0 
       1       20    NA      0    NA      0 
       1       20     0      0     0      0 
       5       30     1     NA     0      0 
       5       30     0      0     0      1 
       6       40     0      1     0      0 
</code></pre>

<p>I want to group the data of each id and put all in an array (array of arrays):</p>

<pre><code>DATA_GROUPED []=
               id    data     
       1       1     X1[]=[an array includes all data from DATA_ALL where the id=1]     
       2       5     X2[]=[an array includes all data from DATA_ALL where the id=5]
       3       6     X3[]=[an array includes all data from DATA_ALL where the id=6]
</code></pre>

<p>Please note that the length of X1!=X2!=X3</p>

<p>So how I can create the DATA_GROUPED[] matrix??</p>
"
63,How hard is it to implement a chess engine?,"<p>I'm wondering how hard it would be to implement a <strong>chess engine</strong>. Are there already open-source implementations? </p>

<p>It seems that you'd need a scoring function for a given board constellation, and a very fast way of exploring several likely future board constellations. Exploring all possible future moves is of course impossible, so one could greedily follow the most promising moves, or use approximate techniques like <a href=""http://en.wikipedia.org/wiki/Simulated%5Fannealing"" rel=""nofollow"">simulated annealing</a> to follow likely moves probabilistically.</p>

<p>Do you think that is within the scope of a <a href=""http://en.wikipedia.org/wiki/Machine%5Flearning"" rel=""nofollow"">machine learning</a> graduate student project -- assuming there was an open-source implementation that the students could use, that does the basic things like returning the next possible moves for a given figure? Probably too hard? </p>

<p>It would be a fun project to have different teams work on chess engines and then let them play against each other ... </p>
"
64,Implementation of differentiation in C#,"<p>I have the following differentiation, I need to implement it in C#:</p>

<pre><code>W(t)=d/dt(log(A(t)))
</code></pre>

<p>Where <code>A(t)</code> is a array of double data.</p>

<p>How can I obtain the resulting <code>W(t)</code> array from the derivative above?</p>

<p>Thanks </p>

<p>edit:</p>

<pre><code>public double[,] derivative()
{
    dvdt = new double[envelope.GetLength(0), envelope.GetLength(1)];
    int h = 1;

    for (int j = 0; j &lt; envelope.GetLength(0); j++)
    {
        for (int i = 0; i &lt; envelope.GetLength(1)-1 ; i++)
        {
            dvdt[j, i] = (envelope[j, i + h] - envelope[j, i]) / (h);
        }
    }
    return dvdt;
}
</code></pre>

<p>I found this library <a href=""http://autodiff.codeplex.com/"" rel=""nofollow"">http://autodiff.codeplex.com/</a> but i cant understand how the sample code is working and how i can apply it to my problem eh</p>
"
65,read json into R,"<p>After a couple of days struggle, I decided to ask to experts of stackoverflow.</p>

<p>I wanted to export this (for example) ""<a href=""http://www.appannie.com/app/ios/instagram/ranking/history/chart_data/?s=2010-10-06&amp;e=2012-06-04&amp;c=143441&amp;f=ranks&amp;d=iphone"" rel=""nofollow"">http://www.appannie.com/app/ios/instagram/ranking/history/chart_data/?s=2010-10-06&amp;e=2012-06-04&amp;c=143441&amp;f=ranks&amp;d=iphone</a>"" into R. </p>

<p>So here's what I tried.</p>

<pre><code>library(RJSONIO)
library(rjson)
library(RCurl)
appannie &lt;- getURL(""http://www.appannie.com/app/ios/instagram/ranking/history/chart_data/.json?s=2010-10-06&amp;e=2012-06-04&amp;c=143441&amp;f=ranks&amp;d=iphone"")
fromJSON(appannie)
</code></pre>

<p>But this gives me an error of ""No data to parse"".</p>

<p>But i can clearly see that there is data (I can see the data from the browser). </p>

<p>[{""data"": [[1286323200000, 70, ""Initial release\n""], [1286409600000, 65, null], [1286496000000, 89, null], [1286582400000, 106, null], [1286668800000, 143, null], [1286755200000, 172, null], [1286841600000, 106, null], [1286928000000, 116, null], [1287014400000, 78, null], [1287100800000, 102, null],  ...... [1338768000000, 2, null]], ""label"": ""Photo and Video""}]</p>

<p>So I tried this one.</p>

<pre><code>fromJSON(paste(readLines('http://www.appannie.com/app/ios/instagram/ranking/history/chart_data/.json?s=2010-10-06&amp;e=2012-06-04&amp;c=143441&amp;f=ranks&amp;d=iphone'), collapse="""")) 
</code></pre>

<p>But this one gives me an error of ""unexpected character '&lt;'"".</p>

<p>Can somebody give me a guidance why this is happening and how to solve this?    </p>
"
66,Publicly Available Spam Filter Training Set,"<p>I'm new to machine learning, and for my first project I'd like to write a naive Bayes spam filter. I was wondering if there are any publicly available training sets of labeled spam/not spam emails, preferably in plain text and not a dump of a relational database (unless they pretty-print those?). </p>

<p>I know such a publicly available database exists for other kinds of text classification, specifically news article text. I just haven't been able to find the same sort of thing for emails.</p>
"
67,Configuration error when installing R on Linux,"<p>Steps to install:</p>

<ol>
<li>./configure --enable-R-shlib</li>
</ol>

<p>I get this error:</p>

<pre><code> configure: error: --with-x=yes (default) and X11 headers/libs are not available
</code></pre>

<p>in config.log file
I see this entry:</p>

<pre><code> #define X_DISPLAY_MISSING 1
</code></pre>

<p>any ideas?</p>
"
68,R - Making loops faster,"<p>This little code snippet is supposed to loop through a sorted data frame.  It keeps a count of how many successive rows have the same information in columns aIndex and cIndex and also bIndex and dIndex.  If these are the same, it deposits the count and increments it for the next time around, and if they differ, it deposits the count and resets it to 1 for the next time around.</p>

<pre><code>for (i in 1:nrow(myFrame)) {
  if (myFrame[i, aIndex] == myFrame[i, cIndex] &amp;
    myFrame[i, bIndex] == myFrame[i, dIndex]) {
      myFrame[i, eIndex] &lt;- count
      count &lt;- (count + 1)
  } else {
      myFrame[i, eIndex] &lt;- count
      count &lt;- 1
  }
}
</code></pre>

<p>It's been running for a long time now.  I understand that I'm supposed to vectorize whenever possible, but I'm not really seeing it here.  What am I supposed to do to make this faster?</p>

<p>Here's what an example few rows should look like after running:</p>

<pre><code>aIndex bIndex cIndex dIndex eIndex
     1      2      1      2      1
     1      2      1      2      2
     1      2      4      8      3
     4      8      1      4      1
     1      4      1      4      1
</code></pre>
"
69,Centroid of convex polyhedron,"<p>I have a closed convex polyhedron which is defined by an array of convex polygons (faces) which are defined by arrays of vertices in 3D space. I'm trying to find the centroid of the polyhedron, assuming uniform density. At the moment I calculate it with the algorithm in this pseudo-code.</p>

<pre><code>public Vector3 getCentroid() {
    Vector3 centroid = (0, 0, 0);
    for (face in faces) {
        Vector3 point = face.centroid;
        point.multiply(face.area());
        centroid.add(point);
    }
    centroid.divide(faces.size());
    return centroid;
}
</code></pre>

<p>This essentially takes the weighted average of the centroids of the faces. I'm not 100% sure this is correct as I haven't been able to find a correct algorithm online. If someone could either confirm my algorithm or refer me to a correct one I would appreciate it.</p>

<p>Thanks.</p>

<hr>

<p>[EDIT]</p>

<p>So here is the actual Java code I am using to find the centroid. It breaks the polyhedron into pyramids converging on an arbitrary point inside the polyhedron. The weighted average for the pyramid centroids is based on the following formula.</p>

<p>C<sub>all</sub> = SUM<sub>all pyramids</sub>(C<sub>pyramid</sub> * volume<sub>pyramid</sub>) / volume<sub>all</sub></p>

<p>Here is the (<em>heavily</em> commented code):
</p>

<pre><code>    // Compute the average of the facial centroids.
    // This gives an arbitrary point inside the polyhedron.
    Vector3 avgPoint = new Vector3(0, 0, 0);
    for (int i = 0; i &lt; faces.size(); i++) {
        avgPoint.add(faces.get(i).centroid);
    }
    avgPoint.divide(faces.size());

    // Initialise the centroid and the volume.
    centroid = new Vector3(0, 0, 0);
    volume = 0;

    // Loop through each face.
    for (int i = 0; i &lt; faces.size(); i++) {
        Face face = faces.get(i);

        // Find a vector from avgPoint to the centroid of the face.
        Vector3 avgToCentroid = face.centroid.clone();
        avgToCentroid.sub(avgPoint);

        // Gives the unsigned minimum distance between the face and a parallel plane on avgPoint.
        float distance = avgToCentroid.scalarProjection(face.getNormal());

        // Finds the volume of the pyramid using V = 1/3 * B * h
        // where:   B = area of the pyramid base.
        //          h = pyramid height.
        float pyramidVolume = face.getArea() * distance / 3;

        // Centroid of a pyramid is 1/4 of the height up from the base.
        // Using 3/4 here because vector is travelling 'down' the pyramid.
        avgToCentroid.multiply(0.75f);
        avgToCentroid.add(avgPoint);
        // avgToCentroid is now the centroid of the pyramid.

        // Weight it by the volume of the pyramid.
        avgToCentroid.multiply(pyramidVolume);

        volume += pyramidVolume;
    }

    // Average the weighted sum of pyramid centroids.
    centroid.divide(volume);
</code></pre>

<p>Please feel free to ask me any questions you may have about it or point out any errors you see.</p>
"
70,Windows/Cygwin links in R,"<p>[originally posted at <a href=""http://support.rstudio.org/help/discussions/problems/1055-following-windows-symlinks"" rel=""nofollow"">RStudio's support site</a>, but it looks like it's a core R issue, not RStudio.]</p>

<p>I'm trying to figure out how to create a filesystem link that will be traversed within R.  I've tried both Cygwin symlinks &amp; Windows links to no avail.  I'm doing this because I've got a big directory full of large data files that I'd like to avoid copying to my workspace.</p>

<p>To create the symlink, I did <code>ln -s ../otherdir/data data</code> in Cygwin. If I then do <code>ls data/</code>, I can see the data files through the link.</p>

<p>To create the Windows link, I did a ""copy"" in Windows Explorer on the <code>otherdir/data/</code> directory, then did ""paste shortcut"" in my workspace and changed the name to <code>data.lnk</code>. If I double click that link, I'm taken correctly through the link.</p>

<p>So both links are correctly targeted.</p>

<p>Now in RStudio, I get the following output, indicating that neither link can be traversed:</p>

<pre><code>&gt; dir()
[1] ""data"" ""data.lnk"" ""docs"" ""src"" ""tmp""
&gt; dir('data')
character(0)
&gt; dir('data.lnk')
character(0)
&gt; dir('data/')
character(0)
&gt; dir('data.lnk/')
character(0)
</code></pre>

<p>Is there some variation on this that will work?  I'm using Windows 7 and R 2.13.1.</p>
"
71,Simple operator+ problem eluding me,"<p>I'm trying to add two lists of <code>uint8_t</code> as though the list were individual integers, and I'm getting some weird values:</p>

<p><code>0x1000 + 0x100 + 0x10 -&gt; 0x1210 ?????</code></p>

<p>Code is as follows:</p>

<pre><code>// values 0x123456 stored as: {12, 34, 56}
integer operator+(integer rhs){
    // internal list called 'value'
    std::list &lt;uint8_t&gt; top = value, bottom = rhs.value;
    if (value.size() &lt; rhs.value.size())
        top.swap(bottom);
    top.push_front(0);                           // extra byte for carrying over
    while (bottom.size() + 1 &lt; top.size())       // match up the byte sizes, other than the carry over
        bottom.push_front(0);
    bool carry = false, next_carry = false;
    for(std::list &lt;uint8_t&gt;::reverse_iterator i = top.rbegin(), j = bottom.rbegin(); j != bottom.rend(); i++, j++){
        next_carry = (((uint8_t) (*i + *j + carry)) &lt;= std::min(*i, *j));
        *i += *j + carry;
        carry = next_carry;
    }
    if (carry)
        *top.begin() = 1;
return integer(top);
}
</code></pre>

<p>Can someone tell me what I'm doing wrong?</p>
"
72,How do I distribute 5 points evenly onto an irregular shape?,"<p>Here is a problem I am trying to solve:</p>

<p>I have an irregular shape. How would I go about evenly distributing 5 points on this shape so that the distance between each point is equal to each other?</p>
"
73,Draw hyperplane in R?,"<p>How does one go about drawing an hyperplane (given the equation) in 3D in R ?
(i.e. 3d equivalent to ""abline"")</p>

<p>Thanks in advance, </p>
"
74,R equivalent of SELECT DISTINCT on two or more fields/variables,"<p>Say I have a dataframe df with two or more columns, is there an easy way to use unique() or other R function to create a subset of unique combinations of two or more columns?</p>

<p>I know I can use sqldf() and write an easy ""SELECT DISTINCT var1, var2, ... varN"" query, but I am looking for an R way of doing this.</p>

<p>It occurred to me to try ftable coerced to a dataframe and use the field names, but I also get the cross tabulations of combinations that don't exist in the dataset:</p>

<blockquote>
  <p>uniques &lt;-
  as.data.frame(ftable(df$var1,
  df$var2))</p>
</blockquote>
"
75,Generalize a query that involves multiple instances of three tables SQL,"<p>I am doing a formula to make some statistics in SQL, but I do not know how to improve a query I am working on...</p>

<p>I made a <a href=""http://sqlfiddle.com/#!2/d2ff6/9"" rel=""nofollow"">sqlfiddle so you can understand me better</a></p>

<p>So I have 3 tables and I need to solve  a formula varying the indexes, i,j... </p>

<pre><code>i j
---
1 1
1 2
1 3
2 1
2 2
2 3
3 1
3 2
3 3
</code></pre>

<p>and then do some sqrt and pow. I want the result in a table but I do not know how to generalize all those long queries into one...</p>
"
76,how to determine base of a number?,"<p>Given a integer number and its reresentation in some arbitrary number system. The purpose is to find the base of the number system. For example, number is 10 and representation is 000010, then the base should be 10. Another example: number 21 representation is 0010101 then base is 2. One more example is: number is 6 and representation os 10100 then base is sqrt(2). Does anyone have any idea how to solve such problem?</p>
"
77,Calculation of scatterplot Standard Deviation in R,"<p>I have created a scatter plot of two vectors, using R, combined with a line (using <em>abline</em>) which represents the x=y diagonal. I wish to calculate the standard deviation of the dots from the diagonal, and color the area which is found between the first and third quantiles.<br>
I have no idea how to do this, and would appreciate all help!!!
Thanks in advance.
Haj. </p>
"
78,What is an elegant way to find all prime numbers in a specified range in R?,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stackoverflow.com/questions/3789968/generate-a-list-of-primes-in-r-up-to-a-certain-number"">Generate a list of primes in R up to a certain number</a>  </p>
</blockquote>



<p>What are elegant ways to find all prime numbers in a specified range in R language?</p>
"
79,Using sapply with switch,"<p>Lets say I have the following case:</p>

<p>I am trying to apply a switch statement to each term in test where <code>test = c(""AA"",""bb"")</code> and <code>mapping = c(""AA""=5,""bb""=7)</code></p>

<p>If I do </p>

<pre><code>sapply(test, switch, mapping )
</code></pre>

<p>I get    </p>

<pre><code>    AA bb
AA  5  5
bb  7  7
</code></pre>

<p>instead of <code>c(5,7)</code> like I want. Is there any way to modify <code>sapply(test,switch,...)</code> such that the first 2 arguments are still test and switch and I am able to pass in a vector for the mapping?</p>
"
80,Long time scale correlation,"<p>I have some EXTREMELY noisy data (standard deviation a is greater than the mean), but plotting it with a 15 data point running average does well to get a visual indication of the trending.  I want to see if any of my data sets are correlated, but standard correlation gives me very poor results due to the noisiness of the data (and the fact that I care about trends over scales on the order of hundreds of data points).  I'm thinking of maybe just cranking the running average up to a few hundred data points and then running the correlation on those, but something makes me think that there might be a better way.</p>
"
81,What is the standard approach to doing handwriting recongition?,"<p>I want to try my hand at some handwriting recognition.  I was wondering what the standard approach to doing this is.  I was thinking a hidden markov model might be a good model to use, and am not sure what dataset would be useful to train on.</p>

<p>Any guidance is greatly appreciated.  Thanks</p>
"
82,How to cleanly index numpy arrays with arrays (or anything else that supports addition so that it can be offset),"<p>The easiest way to explain my question may be with an example, so let me define some arrays:</p>

<pre><code>&gt;&gt;&gt; test = arange(25).reshape((5,5))
&gt;&gt;&gt; test
array([[ 0,  1,  2,  3,  4],
      [ 5,  6,  7,  8,  9],
      [10, 11, 12, 13, 14],
      [15, 16, 17, 18, 19],
      [20, 21, 22, 23, 24]])
&gt;&gt;&gt; Xinds = array([1,2,3])
&gt;&gt;&gt; Yinds = array([1,2,3])
</code></pre>

<p>Now, if I wanted the elements in rows 1, 2, and 3 and in column 0, I could go:</p>

<pre><code>&gt;&gt;&gt; test[Yinds,0]
array([ 5, 10, 15])
</code></pre>

<p>If I wanted the items in rows 1, 2, and 3 and all columns, I could go:</p>

<pre><code>&gt;&gt;&gt; test[Yinds, :]
array([[ 5,  6,  7,  8,  9],
       [10, 11, 12, 13, 14],
       [15, 16, 17, 18, 19]])
</code></pre>

<p>However, if I try to extend this to get the elements in rows 1, 2, and 3 and columns 1, 2, and 3, -- surprise! -- I instead get the elements in (1,1), (2,2), and (3,3)</p>

<pre><code>&gt;&gt;&gt; test[Yinds, Xinds]
array([ 6, 12, 18])
</code></pre>

<p>Instead of what I want:</p>

<pre><code>&gt;&gt;&gt; test[Yinds, :][:, Xinds]
array([[ 6,  7,  8],
       [11, 12, 13],
       [16, 17, 18]])
&gt;&gt;&gt; test[1:4,1:4]
array([[ 6,  7,  8],
       [11, 12, 13],
       [16, 17, 18]])
</code></pre>

<p>I realize I could define a slice, but I want to be able to add an offset to the indices (e.g. Yinds+offset), and that can't be done with slices.</p>

<p>I could do something like</p>

<pre><code>&gt;&gt;&gt; xStart = 1
&gt;&gt;&gt; xEnd   = 4
&gt;&gt;&gt; yStart = 1
&gt;&gt;&gt; yEnd   = 4

&gt;&gt;&gt; offset = 1
&gt;&gt;&gt; test[yStart+offset:yEnd+offset, xStart+offset:xEnd+offset]
...
</code></pre>

<p>or</p>

<pre><code>&gt;&gt;&gt; Xinds = array([1,4])
&gt;&gt;&gt; Yinds = array([1,4])

&gt;&gt;&gt; offset = 1
&gt;&gt;&gt; test[slice(*(Yinds+offset)), slice(*(Xinds+offset))]
...
</code></pre>

<p>But neither is particular clean.</p>

<p>Monkey patching the addition operator in to slice doesn't seem to be an option, and inheriting from slice to add the operator doesn't appear to work either; I get the error, ""type 'slice' is not an acceptable base type"". (*Grumble* This wouldn't be a problem in Ruby *Grumble*)</p>

<p>So, my question is, what's the cleanest way to access a (more than 1 dimensional) sub-array with something that can be stored and offset?</p>

<p>Options so far:</p>

<ul>
<li>test[Yinds+offset, :][:, Xinds+offset]</li>
<li>test[yStart+offset:yEnd+offset, xStart+offset:xEnd+offset]</li>
<li>test[slice(*(Yinds+offset)), slice(*(Xinds+offset))]</li>
</ul>
"
83,How do I find the minimum row number for each factor in R?,"<p>I have a dataframe, Call it A, that looks something like this:</p>

<pre><code>GroupID  Dist1   Dist2 ...
1        4       4 
1        5       4 
1        3       16 
2        0       4 
2        7       2 
2        8       0 
2        6       4 
2        7       4 
2        8       2 
3        7       4 
3        5       6
...
</code></pre>

<p>GroupID is a factor, Dist1, Dist2 are integers.</p>

<p>I have a derived dataframe, SummaryA</p>

<pre><code>GroupID  AveD1  AveD2 ...
1        4       8 
2        6       2
3        6       5
...
</code></pre>

<p>For each groupID, I need to find the ROW NUMBER that has the minimum, to do further manipulation, and to extract data to my summary set. For instance, I need:</p>

<pre><code>GroupID  MinRowD1  
1        1 
2        4 
3        11 
</code></pre>

<p>On matches, it doesn't matter which I choose, but I'm stuck as to how I get this. I can't use which(), because it doesn't operate over factors nicely, I can't use ave(Fun=min), because I need the location, not the minimum value.
If I do something with matching to the minimum for each group, I can have multiple matches, which screws stuff up.</p>

<p>Any suggestions for how to do this?</p>
"
84,combining two data frames of different lengths,"<p>I have two data frames</p>

<p>first is of only one column and 10 rows</p>

<p>second is of 3 columns and 50 rows</p>

<p>When I try to combine this by using cbind it gives this error ""Error in data.frame(..., check.names = FALSE) : ""</p>

<p>Can anyone suggest if there is any other function to do this?
P.S I have tried this using lists too but it gives the same error</p>

<p><em>Poster provided extra information in an answer below:</em></p>

<p>Hi all sorry for the confusion...the dataframe consisting of 3 columns should be first 3 columns in a csv file, where as the data frame with one column should be the fourth column in a csv file when i write write.table function. The first 3 columns has 50 rows and the fourth column should occupy first 10 rows</p>
"
85,How do I perfect the following R plot?,"<p>I have the following plot in R:</p>

<pre><code>x = c(0, 1e-10, 1e-08, 1e-06, 1e-05, 0.001, 0.1);
y = c(22000, 21490, 17252, 9204, 6118, 5092, 4998);
z = c(85.59, 85.59, 85.58, 85.49, 85.29, 85.29, 85.29);

x1 = c(0, 1e-10, 1e-08, 1e-06, 1e-05, 0.001, 0.1);
y2 = c(22000, 20039, 15185, 7705, 5436, 5223, 4933);
z2 = c(85.59, 85.59, 85.58, 85.53, 85.46, 85.49, 85.49);

y = y / 60;

y2 = y2 / 60;


x = log(x);

par(""mar"")
par(mar = c(par(""mar"")[1:3], 5.1))

plot(x,y, type=""n"", lwd=4, ylab="""", xlab=""threshold"", xaxt=""n"",yaxt=""n"")

axis(1,lwd=4)
axis(2,lwd=4)

points(x, y, type = ""l"", lwd=4)

points(x, y, col=""red"", cex=2, pch=19)

points(x, y2, lwd=4, type=""o"", lty=2, col=""black"");

points(x, y2, col=""red"", cex=2, pch=19);

par(""usr"")
par(usr = c(par(""usr"")[1:2], 84,86))

axis(4, lwd=4)
points(x, z, type=""l"", lwd=4)

points(x, z, col=""blue"", cex=2, pch=15)

points(x, z2, type=""l"", col=""black"", lty=2, lwd=4)

points(x, z2, col=""blue"", cex=2, pch=15)


mtext(""Measure1"", side = 4, col = ""blue"",line=3)
mtext(""Measure2"", side = 2, col = ""red"",line=3)
</code></pre>

<p>It is almost perfect for me, but I just want to tweak it according to the following things:</p>

<ol>
<li><p>The left y axis and the x axis - they are bolder, but for some reason, the bold part is not stretching <em>all</em> over the axes.</p></li>
<li><p>I want to make the font of the numbers and the labels a bit bigger, and maybe in bold font face.</p></li>
<li><p>I may even want to change the font to Times, but it sounds to me like a lot of hassle, from what I have seen online.</p></li>
</ol>

<p>Thanks!</p>
"
86,run function on each row which returns multiple rows R,"<p>After much experimenting and googling... and subsequent experimenting again, I am finally resulting to asking my first question on StackOverflow :)</p>

<p>I have a data.frame, and want to apply a custom function, <code>expandBases</code>, to each row of the data.frame. <code>expandBases</code> returns a data.frame consisting of 1 or more rows (and this will vary depending on the data supplied to it). <code>expandBases</code> actually returns more columns than in the toy example below -- but for the sake of illustration:</p>

<pre><code>structure(list(id = structure(1:3, .Label = c(""a"", ""b"", ""c""), class = ""factor""),
startpos = c(1, 2, 3), len = c(1, 2, 3)), .Names = c(""id"",
""startpos"", ""len""), row.names = c(NA, -3L), class = ""data.frame"")


expandBases &lt;- function(startpos, len)
{
    return(data.frame(cy &lt;- startpos + 0:(len - 1)))
}
</code></pre>

<p>I would like the <code>id</code> factor to be replicated for each row of the returned data.frame. I've been told to use lapply + do.call(rbind). I was wondering if there is a plyr-based solution to this?</p>

<p>Thanks in advance.</p>
"
87,Statistical Data Reconstruction,"<p>As I read an introductory statistics book, a question struck me which am curious to find a good answer for.</p>

<p><strong>Background</strong></p>

<p>The author teaches that statistical classification (understand this to mean organizing raw data into say classes -e.g by grouping the data into equal-sized categories, and assigning them frequencies, like what happens in preparation of a histogram) is a form of data compression. He further teaches that much as this compression approach aids in extraction and learning of the important information embedded in the data, he warns that certain approaches (e.g the use of the mode as a measure of central tendency) risk leaking away important information.</p>

<p>Of significance, he states that the mean (a measure of central tendency) and variance (a measure of variation / dispersion) are most important.</p>

<p><strong>Problem</strong></p>

<p>Now, I wonder, assuming one was to follow this line of thinking, and given a data sample, proceeded to <em>statistically-compress</em> it (in the sense indicated above). </p>

<ol>
<li>What statistical compression mechanism is sufficient to be able to recover without any loss whatsoever, the original data?</li>
<li>And assuming such a lossless recovery is not possible, how can one reconstruct the most statistically-close (a <em>probably approximate</em>) approximation to the original data?</li>
</ol>

<p><strong>Side-Note</strong></p>

<p>What compression information is sufficient to best reconstruct (or approximate) the original dataset:</p>

<ol>
<li>Only the mean and variance?</li>
<li>Knowledge of the probability distribution of the mean and that of the variance?</li>
<li>Or a smaller, but more representative sample of the original population?</li>
<li>Or just the probability distribution of the data?</li>
<li>Any other better mechanism?</li>
</ol>

<p><strong>Illustration / Application / Clarification</strong>:</p>

<p>Asume am given the following datasets (each line is a different dataset):</p>

<p>66, 66, 66, 67, 67, 67, 68, 69 </p>

<p>52, 53, 61, 67, 71, 72, 78, 82</p>

<p>43, 44, 50, 54, 67, 90, 91, 97</p>

<p>Note that the mean for all three of the above samples is 67. Now, assume there was some statistical operation(s),$f$, such that applying it to any of the above samples would yield a (smaller) set of information,$s$, from which the original dataset could be reconstructed (<em>preferably</em> without any loss -- but a <em>probably approximate</em> reconstruction is equally good for me). The problem in this case would be to find such a function $f$ that can yield the small set of information $s$, from which (via some reverse-transformation, $f'$), one can obtain the original data:</p>

<p>$$f(Data) = s$$  such that  $$f'(s) = Data$$</p>

<p>And $f$ is an application of statistical methods.</p>
"
88,How to use LibSVM in Java?,"<p>I am trying to implement an indoor location tracking system using bluetooth dongles. The idea is to walk around with an android device and calculate your location in a room based on the signal strengths of bluetooth dongles placed around the room. 
In order to do this I have decided to use machine learning to approximate, as closely as possible, the RSSI as a distance, meters for example. 
I have been told by a lecturer in my college that LibSVM is what I'm looking for so I've been doing some reading. 
I had a look at this <a href=""http://lekshmideepu.blogspot.ie/2012/02/libsvm-tutorial.html"" rel=""nofollow"">tutorial</a> and can't seem to get my head around the data that's needed to train the system.
The data that I will have is:</p>

<ul>
<li>the locations of each dongle (along with a mac address) saved in a database, x and y coordinates</li>
<li>the Received Signal Strength Indicator (RSSI) of the dongles nearest to my android device</li>
<li>the mac addresses will be used to query the database for certain dongles</li>
</ul>

<p>I understand the data has to be in SVM format but I'm a bit unsure of what it should be in terms of input data and output data. The example below, taken from the tutorial I've mentioned, shows that a man is a class and a woman is a class. So in my case would I have just one class ""dongle""? And should all the values dongle reflect the values I have stored in my database?</p>

<blockquote>
  <p>man voice:low figure:big income:good </p>
  
  <p>woman voice:high figure:slim
  income:fare</p>
  
  <ol>
  <li>Convert the feature values to its numeric representation.    Let's say, that best salary would be 5 and worst salary 1 (or no salary =
  0), the same with other enumarated variables.</li>
  <li>We have 2 classes, man and women . convert the classes to numeric values: man = 1, woman = -1</li>
  <li>Save it in libsvm data format:</li>
  </ol>
  
  <p>[class/target] 1:[firstFeatureValue] 2:[secondFeatureValue] etc. ex.:
  a women with great salary, low voice and small figure would be encoded
  like:
  -1 1:5 2:1.5 3:1.8</p>
  
  <p>In general the input file format of SVM is</p>
  
  <p>[label] [index1]:[value1] [index2]:[value2] ... [label]
  [index1]:[value1] [index2]:[value2] ...</p>
</blockquote>

<p>Could someone give me an example of what I should be aiming for?</p>

<p>This is all brand new to me so any helpful hints or tips to get me going would be great.
Thanks in advance</p>
"
89,R automatically writing charts/graphs to files,"<p>In R, I have a script which generates 2134 graphical plots. In the R environment a graphics windows pops open and flips through all 2134 graphical plots while the script is running. Specifically, these are ACF plots and I would like to be able to write all of these graphical plots to a file.</p>

<p>In the past if I only had a chart or two, I would just right click, select copy as bitmap and save as a word document. The key is automating this process, as it is not feasible for a human to righ click and copy/paste for 2134 plots.</p>

<ol>
<li>How do I write these graphical plots to a file?</li>
<li>What is the best file format to do this?</li>
<li>Is there already a package that will do this for me?</li>
</ol>

<p>All help is greatly appreciated!</p>
"
90,Mapping cut factor variable to numeric in R,"<p>I have a factor variable represented by the histogram bins with values: '660-664' , ... , '740-744' , 745-749' ..</p>

<p>How can I map the factor variable to its mean value, e.g mapping '660-664' to 662?</p>

<p>Basically, what I'm looking for is the inverse of the ""cut"" function.</p>
"
91,xtable: Removing rownames but keeping &,"<p>In xtable, is there any way to print a latex table without rownames, but while keeping <code>&amp;</code> on the left hand side?</p>

<p>I also don't want to set the rownames to NA in my data matrix to achieve this feat. </p>

<p>Example:</p>

<pre><code>require(xtable)
o &lt;- do.call(cbind,lapply(1:10,function(i) matrix(rnorm(10)) ))
print(xtable(o))
</code></pre>

<p>We can see on the LHS of this output, there's <code>1,2,3,4,...,10</code>. This is bad. I don't want this.</p>

<p>However</p>

<p><code>print(xtable(o),include.rownames=FALSE)</code></p>

<p>doesn't give me what I want because it deletes the <code>&amp;</code> at the LHS of the matrix.</p>
"
92,R: Scatterplot with too many points,"<p>I am trying to plot two variables where N=700K. The problem is that there is too much overlap, so that the plot becomes mostly a solid block of black. Is there any way of having a grayscale ""cloud"" where the darkness of the plot is a function of the number of points in an region? In other words, instead of showing individual points, I want the plot to be a ""cloud"", with the more the number of points in a region, the darker that region.</p>
"
93,C.D.F. of a maximum depending on two random variables and an absolute value,"<blockquote>
  <p>If $X$ and $Y$ are exponential and independent random variables with parameters $\lambda$ and $\mu$ respectively, find the C.D.F. $F_U(u)$ of $U = max(0, X-Y)$, determine if U is continuous, and find the C.D.F. of $F_V(v)$ of $V = |X-Y|$.</p>
</blockquote>
"
94,Is aggregate the right function to use here?,"<p>consider the following data frame:</p>

<pre><code>d &lt;- data.frame(c1=c(rep(""a"",6),rep(""b"",6)), 
                c2=c(""v1"",""v1"",""v2"",""v3"",""v3"",""v1"", ""v2"",""v3"",""v1"",""v2"",""v3"",""v2""), 
                c3=c(1.4,-1.2,1.5,1.6,-1.7,1.2, -1.1,-1.2,1.3,1.5,1.1,-1.9))
</code></pre>

<p>I want to add a 4th column c4 that counts how many positive and negative numbers are there for ""a"" and ""b"" in column c1. However, only those values in c3 should be considered where c2 equals ""v1"". Furthermore, if there are only positive or negative values an empty string should be printed</p>

<p>So for my example the 4th column should look like:</p>

<pre><code>&gt; d
   c1 c2   c3 c4
1   a v1  1.4 2/1
2   a v1 -1.2 2/1
3   a v2  1.5 2/1
4   a v3  1.6 2/1
5   a v3 -1.7 2/1
6   a v1  1.2 2/1
7   b v2 -1.1 "" ""
8   b v3 -1.2 "" ""
9   b v1  1.3 "" ""
10  b v2  1.5 "" ""
11  b v3  1.1 "" ""
12  b v2 -1.9 "" ""
</code></pre>

<p>for a the value of 2/1 is used as there are two positive numbers and one negative number where c2=""v1""</p>

<p>At the moment I came closest using the aggregate function but I still don't really get it right. Not sure if there is a better function to use for that kind of issue?</p>
"
95,Equations For Quadratic Regression,"<p>Does anyone know the specific equations for the three parameters in a least-squares quadratic regression? I'm looking for something like $\beta_1=,\beta_2=,\beta_3=$ for each of $y=\beta_1+\beta_2x+\beta_3x^2$. To be clear, the right side of each of these equations should be evaluateable, using the data, to find the parameter. I was able to find the equations for linear regression on line, but google hasn't turned anything up for this.</p>

<p>Thanks in advance</p>
"
96,How to get a BATCH file executed from a web page?,"<p>I have a one line batch file that I want to call from a web page via a button what is the best way to achieve this?</p>

<p>The BATCH File is as follows:</p>

<pre><code>c:\R\bin\Rscript.exe ""c:\Users\user\Desktop\Shares.R"" 
</code></pre>

<p>Or is it possible just to call the R script straight from the web page and skip the BATCH file altogether, Can this be done? It is all being done on a local host so I don't think there should be a problem with permissions etc...</p>

<p>Any Help would be greatly appreciated.</p>

<p>Regards, </p>

<p>Anthony.</p>
"
97,Median of independent variables,"<blockquote>
  <p>Each of the variables $X_1$, $X_2$, $X_3$ independently takes values from the set ${-1,0,1}$ with probabilities 0.2, 0.6, 0.2 respectively. The variable $Y$ is defined to be the median of $X_1$, $X_2$, $X_3$. Show that $P(Y=-1)=0.104$ and deduce $P(Y=1)$ and $P(Y=0)$. Calculate E(Y) and Var(Y) and determine whether the variance of Y exceeds that of the mean of $X_1$, $X_2$, $X_3$.</p>
</blockquote>

<p>I am not very good with medians because I've never come across a formula for it. Also Y doesn't seem to be a function of a random variable (is it?) which I am more familiar with. </p>
"
98,Expanding the square in the variance,"<p>$\newcommand{\var}{\operatorname{var}}$</p>

<p>In the Pattern Recognition book of Bishop, I'm reading the following statement, that I don't fully understand.</p>

<blockquote>
  <p>The variance of $f(x)$ is defined by <br>
  $$\var[f] = E[(f(x)-E[f(x)])^2]$$ <br>
  Expanding out the square, we see that the variance can also be written
  in terms of the expectations of $f(x)$ and $f(x)^2$: <br>
  $$\var[f] = E[f(x)^2]-E[f(x)]^2$$</p>
</blockquote>

<p>I can derive this the following way, but I'm not sure if this is correct, and if it is, why?</p>

<p>$$
\begin{align}
\var[f] &amp; = E[(f(x)-E[f(x)])^2]\\[6pt]
&amp; = E[f(x)^2-2f(x)E[f(x)]+E[f(x)]^2]\\[6pt]
&amp; = E[f(x)^2]-E[2f(x)E[f(x)]]+E[E[f(x)]^2] \quad\text{since }E[X+Y] = E[X] + E[Y] \\[6pt]
&amp; = E[f(x)^2]-2E[f(x)]E[f(x)]+E[E[f(x)]^2]\\[6pt]
&amp; {}\qquad\text{since }E[cX] = cE[X],
\text{ but is $E[f(x)]$ const. here?} \\[6pt]
&amp; = E[f(x)^2]-2E[f(x)]E[f(x)]+E[f(x)]^2 \\[6pt]
&amp; {}\quad\quad\text{ I guess $E[E[X]] = E[X]$, or something like that?} \\[6pt]
&amp; = E[f(x)^2]-E[f(x)]^2
\end{align}
$$</p>

<p>Is this correct? Are my assumptions I marked correct?</p>
"
99,How do I index n sets of 4 columns to plot multiple plots using matplotlib?,"<p>I want to know how I should index / access some data programmatically in python. 
I have columnar data: depth, temperature, gradient, gamma, for a set of boreholes. There are n boreholes. I have a header, which lists the borehole name and numeric ID. Example:</p>

<pre><code>Bore_name,Bore_ID,,,Bore_name,Bore_ID,,,, ... 
&lt;a row of headers&gt;
depth,temp,gradient,gamma,depth,temp,gradient,gamma ...
</code></pre>

<p>I don't know how to index the data, apart from rude iteration:</p>

<pre><code>with open(filename,'rU') as f:
    bores = f.readline().rstrip().split(',')   
    headers = f.readline().rstrip().split(',')


# load from CSV file, missing values are empty 'cells'
tdata = numpy.genfromtxt(filename, skip_header=2, delimiter=',', missing_values='', filling_values=numpy.nan)

for column in range(0,numpy.shape(tdata)[1],4):  
    # plots temperature on x, depth on y
    pl.plot(tdata[:,column+1],tdata[:,column], label=bores[column])
    # get index at max depth
    depth = numpy.nanargmin(tdata[:,column])
    # plot text label at max depth (y) and temp at that depth (x)
    pl.text(tdata[depth,column+1],tdata[depth,column],bores[column])
</code></pre>

<p>It seems easy enough this way, but I've been using R recently and have got a bit used to their way of referencing data objects via classes and subclasses interpreted from headers. </p>
"
100,"Position of the sun given time of day, and lat/long","<p>Anyone want to share some nice code that gives the position of the sun (elevation and azimuth) given latitude and longitude and time of day ?</p>

<p>And date, of course. </p>
"
101,Reinforcement learning of a policy for multiple actors in large state spaces,"<p>I have a real-time domain where I need to assign an action to N actors involving moving one of O objects to one of L locations. At each time step, I'm given a reward R, indicating the overall success of all actors.</p>

<p>I have 10 actors, 50 unique objects, and 1000 locations, so for <em>each actor</em> I have to select from 500000 possible actions. Additionally, there are 50 environmental factors I may take into account, such as how close each object is to a wall, or how close it is to an actor. This results in 25000000 potential actions <em>per actor</em>.</p>

<p>Nearly all reinforcement learning algorithms don't seem to be suitable for this domain.</p>

<p>First, they nearly all involve evaluating the expected utility of each action in a given state. My state space is huge, so it would take forever to converge a policy using something as primitive as Q-learning, even if I used function approximation. Even if I could, it would take too long to find the best action out of a million actions in each time step.</p>

<p>Secondly, most algorithms assume a single reward per actor, whereas the reward I'm given might be polluted by the mistakes of one or more actors.</p>

<p>How should I approach this problem? I've found no code for domains like this, and the few academic papers I've found on multi-actor reinforcement learning algorithms don't provide nearly enough detail to reproduce the proposed algorithm.</p>
"
102,How can I check if a point is below a line or not?,"<p>How can I check if a point is below a line or not ?</p>

<p>I've the following data:</p>

<pre><code>Line [ {x1,y1}, {x2,y2} ]
Points {xA,yA}, {xB,yB} ...
</code></pre>

<p>I need to write a small algorithm in python to detect points on one side and the other side of the line.</p>

<p>thanks</p>
"
103,POD low dimensional vector in boost,"<p>I'm looking for POD low dimension vectors (2,3 and 4D let say) with all the necessary arithmetic niceties (operator +, - and so on). POD low dimension matrices would be great as well.</p>

<p>boost::ublas vectors are not POD, there's a pointer indirection somewhere (vector are resizeable).</p>

<p>Can I find that anywhere in boost? Using boost::array along with boost.operator lib is an options but maybe I'm missing something easier elsewhere?</p>

<p>Apart boost, does anybody know any good library around?</p>

<p>PS: POD &lt;=> plain old data</p>

<p><strong>EDIT:</strong></p>

<p>Otherwise, here are some other links I gathered from another thread:</p>

<ul>
<li><a href=""http://www.cgal.org/"" rel=""nofollow"">http://www.cgal.org/</a></li>
<li><a href=""http://geometrylibrary.geodan.nl"" rel=""nofollow"">http://geometrylibrary.geodan.nl</a></li>
<li><a href=""http://www.cmldev.net"" rel=""nofollow"">http://www.cmldev.net</a></li>
<li><a href=""http://www.openexr.com/index.html"" rel=""nofollow"">http://www.openexr.com/index.html</a></li>
<li><a href=""http://project-mathlibs.web.cern.ch/project-mathlibs/sw/html/SMatrix.html"" rel=""nofollow"">http://project-mathlibs.web.cern.ch/project-mathlibs/sw/html/SMatrix.html</a></li>
</ul>
"
104,Math formula editor that can be plugged into any website,"<p>I am writing a Q and A web application. I'm wondering if there's any library of web services that can allow user to create math equations easily in the web editor?</p>

<p>Thanks</p>
"
105,what is trend correction?,"<p>I am doing some practice questions and I don't know what I have to do next for ""trend correction."" I am suppose to do trend correction to the data columns I have but what is trend correction?</p>

<p><img src=""http://i.stack.imgur.com/gY41R.png"" alt=""enter image description here""></p>
"
106,Weighted linear least squares linear fit. How to force through first point on line?,"<p>I'm using alglib to calculate slope and intercept values using a least squares linear fit equation, as follows:-</p>

<pre><code>int info;
double[] c;
alglib.lsfitreport rep;
var fmatrix = new double[,] { { 1, 0 }, { 1, 0.05 }, { 1, 0.1 }, { 1, 0.2 } };
var y = new double[] { 32046, 32260, 32296, 32678 };
var w = new double[] { 1, 1, 1, 1 };

alglib.lsfitlinearw(y, w, fmatrix, out info, out c, out rep);

var intercept = c[0];
var slope = c[1];
</code></pre>

<p>""fmatrix"" contains my x-axis values (the second of each pair:- 0, 0.05, 0.1, 0.2); ""y"" obviously holds my y-axis values; ""w"" are the weightings - in this case all 1's, so no weightings are applied in this example.</p>

<p>Now, I need to apply a couple of different ""forcings"". I've managed to find how to ""force through origin"" by changing the first value of each fmatrix pair from ""1"" to ""0"", i.e.:</p>

<pre><code>var fmatrix = new double[,] { { 0, 0 }, { 0, 0.05 }, { 0, 0.1 }, { 0, 0.2 } };
</code></pre>

<p>However I also need to force through the first point (x=0, y=32046). Any ideas how to achieve this? Math isn't my strong point and I don't really understand the function's documentation:  <a href=""http://www.alglib.net/translator/man/manual.csharp.html#sub_lsfitlinearw"" rel=""nofollow"">http://www.alglib.net/translator/man/manual.csharp.html#sub_lsfitlinearw</a></p>
"
107,R: Character Variables must be Duplicated in .C/.Fortran,"<p>I am trying to use the <code>leaps</code> package. This data frame <code>df</code> has 9 columns. Columns 2 through 8 are the explanatory variables and column 9 is the reponse variable. <code>df</code> also has (column) names.</p>

<p>When I try to use the leaps package, I get a cryptic error.</p>

<pre><code>x &lt;- df[,2:8]
y &lt;- df[,9]
leaps &lt;- regsubsets(x, y)
Error in leaps.setup(x, y, wt = weights, nbest = nbest, nvmax = nvmax,  : 
  character variables must be duplicated in .C/.Fortran
</code></pre>

<p>What does this error mean, and how can I prevent this?</p>

<p>Here is a snippet of the data.frame:</p>

<pre><code>&gt; dput(df[1:2,])
structure(list(Var1 = c(2396, 2396), Var2 = c(NA_character_, 
NA_character_), Var3 = c(NA_character_, NA_character_), Var4 = c(501, 
511), Var5 = c(5, 5), Var6 = c(13, 8), Var7 = c(NA_real_, NA_real_
), Var8 = c(NA_real_, NA_real_), Var9 = c(0.0047, 0.0371)), .Names = c(""Var1"", 
""Var2"", ""Var3"", ""Var4"", ""Var5"", ""Var6"", ""Var7"", ""Var8"", ""Var9""
), row.names = 1:2, class = ""data.frame"")

&gt; str(df)
'data.frame':   10000 obs. of  9 variables:
 $ Var1: num  2396 2396 2396 2396 2396 ...
 $ Var2: chr  NA NA NA NA ...
 $ Var3: chr  NA NA NA NA ...
 $ Var4: num  501 511 523 757 770 803 803 803 807 506 ...
 $ Var5: num  5 5 3 5 1 1 5 5 5 5 ...
 $ Var6: num  13 8 13 11 13 8 13 8 11 11 ...
 $ Var7: num  NA NA NA NA NA NA NA NA NA NA ...
 $ Var8: num  NA NA NA NA NA NA NA NA NA NA ...
 $ Var9: num  0.0047 0.0371 0.042 0.0488 0.0048 ...
</code></pre>

<p>I tried replacing the missing values with 0 just to see if it would work, and that didn't help.</p>
"
108,How to calculate percentile? Is it possible to get 100 percentile?,"<p>How do we calculate percentile? I think it should be calculated as:-<br>
P:- Total number of candidates<br>
L:- Number of candidates whose marks are below yours</p>

<p>Percentile = (L/P)*100<br>
That means if you are the topper and there are 200,000 candidates and if your marks are same as 7 others then;  </p>

<p>your percentile = (199992/200000)*100 = 99.996</p>

<p>That means in any circumstances you can't have perfect 100 percentile.<br>
Then how come this is possible, see links:-<br>
<a href=""http://www.zeenews.com/news680105.html"" rel=""nofollow"">http://www.zeenews.com/news680105.html</a><br>
<a href=""http://economictimes.indiatimes.com/cat/two-cat-aspirants-get-100-percentile/articleshow/2685824.cms"" rel=""nofollow"">http://economictimes.indiatimes.com/cat/two-cat-aspirants-get-100-percentile/articleshow/2685824.cms</a>  </p>
"
109,Define range of value,"<p>I need to set an int variable in each iteration +-1, the range should be between 0-10. When i add 1 to 10 -> 0 when i add -1 to 0 -> 10. I know i need to go with modulo here but cannot find any solution.  </p>
"
110,Function: calculating seconds between data points,"<p>I have the following column in my data frame:</p>

<pre><code>              DateTime
1  2011-10-03 08:00:04
2  2011-10-03 08:00:05
3  2011-10-03 08:00:06
4  2011-10-03 08:00:09
5  2011-10-03 08:00:15
6  2011-10-03 08:00:24
7  2011-10-03 08:00:30
8  2011-10-03 08:00:42
9  2011-10-03 08:01:01
10 2011-10-03 08:01:24
11 2011-10-03 08:01:58
12 2011-10-03 08:02:34
13 2011-10-03 08:03:25
14 2011-10-03 08:04:26
15 2011-10-03 08:06:00
</code></pre>

<p>With <code>dput</code>:</p>

<pre><code>&gt; dput(smallDF)
structure(list(DateTime = structure(c(1317621604, 1317621605, 
1317621606, 1317621609, 1317621615, 1317621624, 1317621630, 1317621642, 
1317621661, 1317621684, 1317621718, 1317621754, 1317621805, 1317621866, 
1317621960, 1317622103, 1317622197, 1317622356, 1317622387, 1317622463, 
1317622681, 1317622851, 1317623061, 1317623285, 1317623404, 1317623498, 
1317623612, 1317623849, 1317623916, 1317623994, 1317624174, 1317624414, 
1317624484, 1317624607, 1317624848, 1317625023, 1317625103, 1317625179, 
1317625200, 1317625209, 1317625229, 1317625238, 1317625249, 1317625264, 
1317625282, 1317625300, 1317625315, 1317625339, 1317625353, 1317625365, 
1317625371, 1317625381, 1317625395, 1317625415, 1317625423, 1317625438, 
1317625458, 1317625469, 1317625487, 1317625500, 1317625513, 1317625533, 
1317625548, 1317625565, 1317625581, 1317625598, 1317625613, 1317625640, 
1317625661, 1317625674, 1317625702, 1317625715, 1317625737, 1317625758, 
1317625784, 1317625811, 1317625826, 1317625841, 1317625862, 1317625895, 
1317625909, 1317625935, 1317625956, 1317625973, 1317626001, 1317626043, 
1317626062, 1317626100, 1317626113, 1317626132, 1317626153, 1317626179, 
1317626212, 1317626239, 1317626271, 1317626296, 1317626323, 1317626361, 
1317626384, 1317626407), class = c(""POSIXct"", ""POSIXt""), tzone = """")), .Names = ""DateTime"", row.names = c(NA, 
-100L), class = ""data.frame"")
</code></pre>

<p><strong>My goal:</strong> I want to calculate the time difference, in seconds, between each measurement. </p>

<p><em><strong>Edit</strong>:</em>
I'm looking to get the following result, where the time difference (in seconds) between each data point is calculated, <em>except</em> for the first value of the day (line 3), when the time is calculate relative to 8 am:</p>

<pre><code>              DateTime      Seconds
1  2011-09-30 21:59:02      6
2  2011-09-30 21:59:04      2
3  2011-10-03 08:00:04      4
4  2011-10-03 08:00:05      1
5  2011-10-03 08:00:06      1
6  2011-10-03 08:00:09      3
7  2011-10-03 08:00:15      5
8  2011-10-03 08:00:24      9
9  2011-10-03 08:00:30      6
10 2011-10-03 08:00:42      12
11 2011-10-03 08:01:01      19
12 2011-10-03 08:01:24      23
13 2011-10-03 08:01:58      34
14 2011-10-03 08:02:34      36
15 2011-10-03 08:03:25      51
16 2011-10-03 08:04:26      61
17 2011-10-03 08:06:00      94
</code></pre>

<p>However, the measurements start at 8:00 am, so if the value is the first of the day, the number of seconds relative to 8:00 am need to be calculated. In the example above, the first measurement ends at 8:00:04 so using the <code>$sec</code> attribute of <code>POSIX</code> could work here, but on other days the first value may happen a few minutes after 8:00 o'clock.</p>

<p>I've tried to achieve that goal with the following function:</p>

<pre><code>SecondsInBar &lt;- function(x, startTime){
    # First data point or first of day
    if (x == 1 || x &gt; 1 &amp;&amp; x$wkday != x[-1]$wkday){
        seconds &lt;- as.numeric(difftime(x, 
            as.POSIXlt(startTime, format = ""%H:%M:%S""), 
            units = ""secs""))
    # else calculate time difference
    } else {
        seconds &lt;- as.numeric(difftime(x, x[-1], units = ""secs""))
    }
    return (seconds)
}
</code></pre>

<p>Which then could be called with <code>SecondsInBar(smallDF$DateTime, ""08:00:00"")</code>.</p>

<p>There are at least two problems with this function, but I don't know how to solve these:</p>

<ul>
<li>The code segment <code>x$wkday != x[-1]$wkday</code> returns a <code>$ operator is
invalid for atomic vectors</code> error, </li>
<li>And the <code>as.POSIXlt(startTime,    format = ""%H:%M:%S"")</code> uses the
current date, which makes the    <code>difftime</code> calculation erroneous.</li>
</ul>

<p><strong>My question:</strong>
Where am I going wrong with this function? 
And: is this approach a viable way or should I approach it from a different angle?</p>
"
111,select even numbered days from data frame,"<p>I have a data.frame of a time series of data, I would like to thin the data by only keeping the entries that are measured on every even day number. For example:</p>

<pre><code>set.seed(1)
RandData &lt;- rnorm(100,sd=20)
Locations &lt;- rep(c('England','Wales'),each=50)
today &lt;- Sys.Date()
dseq &lt;- (seq(today, by = ""1 days"", length = 100))

Date &lt;- as.POSIXct(dseq, format = ""%Y-%m-%d"")

Final &lt;- data.frame(Loc = Locations,
                    Doy = as.numeric(format(Date,format = ""%j"")),
                    Temp = RandData)
</code></pre>

<p>So, how would I reduce this data frame to only contain every entry that is measured on even numbered days such as Lloc, day, and temp on day 172, day 174 and so on...</p>
"
112,How to use R to build bubble charts with gradient fills,"<p>I use R code below to build bubble chart.</p>

<pre><code>pdf(file='myfigure.pdf',height=10,width=13)
y&lt;-c(123,92,104,23,17,89,13)
x&lt;-c(11,45,24,50,18,7,2)
size&lt;-c(1236,1067,1176,610,539,864,1026)
radius&lt;-sqrt(size/pi)
col&lt;-c(2,2,3,4,5,5,6)
name&lt;-c(""Acura"", ""Alfa Romeo"",""AM General"",""Aston Martin Lagonda"",""Audi"",""BMW"",""Bugatti"")

symbols(x,y,  circles=radius,fg=""white"",bg=col,ylim=c(-20,140))
text(x, y, name, cex=1.5,font=4)
dev.off()
</code></pre>

<p><img src=""http://i.stack.imgur.com/x9OLf.png"" alt=""enter image description here"">
But I want the bubbles with 3d surface, say gradient fills and shadow. like the chart below.
<img src=""http://i.stack.imgur.com/Gs9YB.png"" alt=""enter image description here""></p>

<p>Anyone knows how to use R to release it? Thanks!</p>

<p>Thanks for all the suggestions.While finally I tried a silly way by drawing multiple circles from dark to light to make it gradient filled.  Any suggestions to make it better? Thanks!
<img src=""http://i.stack.imgur.com/qRPjK.png"" alt=""enter image description here""></p>

<pre><code>makeTransparent&lt;-function(someColor, alpha)
{
 newColor&lt;-col2rgb(someColor)
 apply(newColor, 2, function(curcoldata){rgb(red=curcoldata[1], green=curcoldata[2],                                          blue=curcoldata[3],alpha=alpha,maxColorValue=255)})
}

pdf(file='myfigure.pdf',height=10,width=13)
y&lt;-c(123,92,104,23,17,89,13)
x&lt;-c(11,45,24,50,18,7,2)
size&lt;-c(1236,1067,1176,610,539,864,1026)
radius&lt;-sqrt(size/pi)
col&lt;-c(2,2,3,4,5,5,6)
name&lt;-c(""Acura"", ""Alfa Romeo"",""AM General"",""Aston Martin Lagonda"",""Audi"",""BMW"",""Bugatti"")


x2&lt;-c()
y2&lt;-c()
circles&lt;-c()
bg&lt;-c()
fg&lt;-c()

num&lt;-30
radius_min&lt;-0.3
alpha_min&lt;-40
alpha_max&lt;-180

for (i in 1:num){

x2&lt;-c(x2,x)
y2&lt;-c(y2,y)
circles&lt;-c(circles,radius*(radius_min+(i-1)*(1-radius_min)/num))
bg&lt;-c(bg,makeTransparent(col,alpha=alpha_max-(i-1)*(alpha_max-alpha_min)/num))
if(i!=num){fg&lt;-c(fg,makeTransparent(col,alpha=alpha_max-(i-1)*(alpha_max-alpha_min)/num))}else{fg&lt;-c(fg,rep('white',length(x)))}


}




symbols(x2,y2,circles=circles,fg=fg,bg=bg)
text(x, y, name, cex=1.5,font=4)
dev.off()
</code></pre>
"
113,Refraction vector not normalized,"<p>How can I get the not-normalized output refracted vector, with an also not-normalized incident vector? </p>

<p>I'm following that formulas, work with normalized input, but if I pass not-normalized doesn't. Tried to divide the dot product by input vector length but also nothing.</p>

<p><a href=""http://en.wikipedia.org/wiki/Snell%27s_law#Vector_form"" rel=""nofollow"">Wikipedia Snell's Law Vector form</a></p>
"
114, Loop to perform calculations across rows on specific columns matching a pattern (in data frame)?,"<p>I have a dataframe with some boolean values (1/0) as follows (sorry I couldn't work out how to make this into a smart table)</p>

<pre><code>       Flag1.Sam Flag2.Sam Flag3.Sam Flag1.Ted Flag2.Ted Flag3.Ted
probe1         0         1         0         1         0         0
probe2         0         0         0         0         0         0
probe3         1         0         0         0         0         0
probe4         0         0         0         0         0         0
probe5         1         1         0         1         0         0
</code></pre>

<p>I have 64 samples (Sam/Ted....etc) which are in a list called files i.e;</p>

<pre><code>files &lt;- c(""Sam"", ""Ted"", ""Ann"", ....) 
</code></pre>

<p>And I would like to create a a column summing the flag values for each sample to create the following: </p>

<pre><code>               Sam Ted 
probe1.flagsum   1   1
probe2.flagsum   0   0 
probe3.flagsum   1   0 
probe4.flagsum   0   0
probe5.flagsum   2   1
</code></pre>

<p>I am fairly new to R, trying to learn on a need to know basis but I have tried the following: </p>

<pre><code>for(i in files) {
    FLAGS$i &lt;- cbind(sapply(i, function(y) { 
        #greping columns to filter for one sample
        filter1 &lt;- grep(names(filters), pattern=y)
        #print out the summed values for those columns  
        FLAGS$y &lt;-rowSums(filters[,(filter1)])
    }
}
</code></pre>

<p>The above code does not work and I am bit lost as how to move forward.</p>

<p>Can anyone help me untangle this problem or point me in the right direction of the commands/tools to use.</p>

<p>Thank you.</p>
"
115,How to measure the streakedness of numerical data?,"<p>Would anyone know how to use C/C++ to calculate the streakedness of data? The definition of streakedness is how many deviations away from the mean(i.e running average a numerical data streak. Thank you for your help.</p>

<p>[EDIT] From our company's chief software architect, here is the requirement for a statistical measure. Could someone please define a statistical formula based onour architect's definition of data streakedness? -- February 19th 2013 8:45 AM</p>

<p>Equal numbers are a streak. $1,2,3,3,3,4,5$ has a streak of $7$.</p>

<p>Case A: $1,2,3,4,5,6,7,8,9,10,11,12,13$ has a longest streak of $13$.</p>

<p>Case B: $1,2,3,4,5,6,7,3,8,9,10,11,12$ has a longest streak of $7$, a second smaller streak of $6$.</p>

<p>Case C: $1,2,3,4,5,6,7,1,2,3,4,5,6$ has a longest streak of $7$, and a second smaller streak of $6$.</p>

<p>Case D: $1,2,3,4,5,6,7,1,2,3,1,2,1$ has a longest streak of $7$, a second smaller streak of $3$, and a third smallest streak of $2$</p>

<p>Case E: $1,2,3,4,5,6,7,6,5,4,1,2,3$ has a longest streak of $7$, and a second smaller streak of $3$.</p>

<p>Case F: $1,2,3,4,5,6,7,6,5,4,3,2,1$ has a longest streak of $7$, and no smaller streaks.</p>

<p>The cases A – F are ordered in ‘most sorted to least sorted’, but all have the same length longest streak. Using the averages of streak length is not appropriate:</p>

<p>A: $\text{Average} = 13/1 = 13$</p>

<p>B: $\text{Average} = (7+6)/2 = 6.5$</p>

<p>C: $\text{Average} = (7+6)/2 = 6.5$</p>

<p>D: $\text{Average} = (7+3+2)/3 = 4$</p>

<p>E: $\text{Average} = (7+3)/2 = 5$</p>

<p>F: $\text{Average} = 7/1 = 7$</p>

<p>Factoring in non-streaks (counting them as 1’s):</p>

<p>A: $\text{Average} = 13/1 = 13$</p>

<p>B: $\text{Average} = (7+6)/3 = 4.3$</p>

<p>C: $\text{Average} = (7+6)/2 = 6.5$</p>

<p>D: $\text{Average} = (7+3+2+1)/4 = 3.25$</p>

<p>E: $\text{Average} = (7+1+1+1+3)/5 = 2.6$</p>

<p>F: $\text{Average} = (7+1+1+1+1+1+1)/7 = 1.85$</p>

<p>A variable $R$ can be used to indicate how many deviations away from the mean a particular streak is. The level of a streak can be defined not just in ($\text{integer} \times \text{deviation}$) distances from the mean but also as ($\text{integer} \times \text{fraction_of_deviation}$) distances. To accomplish this, a variable R-factor can be used. The R-factor indicates the separation between two successive R-levels in terms of a fraction of the deviation. By varying the R-factor, streaks can be ranked as required. However, the ""credibility"" of the streak should also be considered, and included in a ranking mechanism. The deviation within the streak is an obvious measure of how staggered the data is within the streak. A good streak should be less staggered, or in other words, have less deviation. For this reason, a very high level streak is considered to be good, even if its deviation is more than what would normally be desired. Thus, while the level $R$ influences the ranking positively, the deviation within the streak influences it negatively</p>
"
116,Converting String to unique integer in R,"<p>I have a data frame of this type</p>

<pre><code>string1,string2,value1
string3,string1,value2
string3,string5,value3
...
...
</code></pre>

<p>I would convert srings in unique integers:</p>

<pre><code>1,2,value1
3,1,value2
3,5,value3
...
...
</code></pre>

<p>I am trying with c() operator, that convert the string in a unique integer. The problem is how to manage the two columns of the data frame. How can I do this?</p>
"
117,How to convert data.frame to (flat) matrix?,"<p>How can I convert the data.frame below to a matrix as given? the first two columns of the data.frame contain the row variables, all combinations of the other columns (except the one containing the values) determine the columns. Ideally, I'm looking for a solution that does not require further packages (so <em>no</em> <code>reshape2</code> solution). Also, no <code>ftable</code> solution.</p>

<pre><code>(df &lt;- data.frame(c1=rep(c(1, 2), each=8), c2=rep(c(1, 2, 1, 2), each=4),
                  gr=rep(c(1, 2), 8), subgr=rep(c(1,2), 4, each=2), val=1:16) )

c1 c2 gr1.subgr1 gr1.subgr2 gr2.subgr1 gr2.subgr2
1  1   1          3          2          4
1  2   5          7          6          8
2  1   9         11         10         12
2  2  13         15         14         16
</code></pre>
"
118,Likelihood Cramér-Rao Bound.,"<p>How can I show the following necessary and sufficient condition?</p>

<blockquote>
  <p>An unbiased estimator $ \hat{\theta} $ of $ \theta $ achieves the Cramér-Rao Lower Bound if and only if
  $$
\frac{\partial \log(L(\theta))}{\partial \theta} = I(\theta) \cdot (\theta - \hat{\theta}),
$$
  where $ I(\theta) $ and $ L(\theta) $ denote respectively the information and likelihood
  functions of a sample $ (X_{1},X_{2},\ldots,X_{n}) $ of i.i.d. random variables having a smooth pdf.</p>
</blockquote>

<p>The ‘$ \Longrightarrow $’ implication is clear, but I don’t know how to prove the ‘$ \Longleftarrow $’ implication.</p>

<p>Thanks a lot!</p>
"
119,Algorithm for finding similar images,"<p>I need an algorithm that can determine whether two images are 'similar' and recognizes similar patterns of color, brightness, shape etc.. I might need some pointers as to what parameters the human brain uses to 'categorize' images. .. </p>

<p>I have looked at hausdorff based matching but that seems mainly for matching transformed objects and patterns of shape.</p>

<p><strong>edit</strong></p>

<p>thanks a lot guys, the input should help me greatly!</p>
"
120,How to wrap a consecutive range of integers?,"<p>What is the shortest possible calculation <code>f(i, n, len, offset)</code> that wraps a range of integers starting from <code>n</code> (>=0) with length <code>len</code>, given a certain <code>offset</code>?</p>

<pre><code>i   offset 0  offset 1  offset 2  offset 15
10  -&gt; 10     -&gt; 15     -&gt; 14     -&gt; 13
11  -&gt; 11     -&gt; 10     -&gt; 15     -&gt; 14
12  -&gt; 12     -&gt; 11     -&gt; 10     -&gt; 11
13  -&gt; 13     -&gt; 12     -&gt; 11     -&gt; 10
14  -&gt; 14     -&gt; 13     -&gt; 12     -&gt; 11
15  -&gt; 15     -&gt; 14     -&gt; 13     -&gt; 12
</code></pre>

<p>So <code>f(10, 10, 5, 1) = 15</code>, <code>f(15, 10, 5, 1) = 14</code> and <code>f(10, 10, 5, 2) = 14</code>.</p>

<p>Bonus karma for negative numbers or negative offsets or ranges that cross 0.</p>
"
121,subsetting nonconsecituve observations in R,"<p>I have a table with >2M rows. I am only interested in percentiles of one variable vs. percentiles of number of observations (e.g. Lorentz curve).</p>

<ul>
<li><p>How do I create a smaller dataframe that contains e.g. observations number 1,101,201,301,...,last , or observations that corresponds to e.g. the 1,2,3,...,100 percentile of total number of observations?</p></li>
<li><p>Is there a quick way to get the lorenz curve of (index, variable) with axes on a percentage basis? Right now I was thinking of adding variables for percentiles of index and variables and then plot them against each other.</p></li>
</ul>

<p>Thanks,</p>

<p>Roberto</p>
"
122,A method for generating random math problems based on performance,"<p>I've been attempting to find out more research and resources on generating random math problems based on performance and calculated difficulty. There's a few places that I'm just now stumbling upon, but I figured I'd ask the community what they recommend reading up on.</p>

<p><a href=""http://www.faqs.org/patents/app/20090017427"" rel=""nofollow"">http://www.faqs.org/patents/app/20090017427</a></p>

<p>There's an article here on an application based on research at MIT
<a href=""http://www.mathscore.com/math/mathProblems.html"" rel=""nofollow"">http://www.mathscore.com/math/mathProblems.html</a>
I'm wondering where specifically that research resides or what I can learn from it and others.</p>

<p>My goal is to first understand how probability, priority queues and anything else plays a role (if at all) in generating varying math problems.</p>

<p>Any resources and recommended reading would be most useful if you can provide that.</p>

<p>Thanks :)</p>
"
123,Does a line profiler for code require a parse tree and is that sufficient?,"<p>I am trying to determine what is necessary to write a line profiler for a language, like those available for Python and Matlab.</p>

<p>A naive way to interpret ""line profiler"" is to assume that one can insert time logging around every line, but the definition of a line is dependent on how a parser handles whitespace, which is only the first problem.  It seems that one needs to use the parse tree and insert timings around individual nodes.</p>

<p>Is this conclusion correct?  Does a line profiler require the parse tree, and is that all that is needed (beyond time logging)?</p>

<hr>

<p>Update 1: Offering a bounty on this because the question is still unresolved.</p>

<p>Update 2: Here is a link for a well known <a href=""http://packages.python.org/line_profiler/"" rel=""nofollow"">Python line profiler</a> in case it is helpful for answering this question.  I've not yet been able to make heads or tails of it's behavior relative to parsing.  I'm afraid that the code for the Matlab profiler is not accessible.</p>

<p>Also note that one could say that manually decorating the input code would eliminate a need for a parse tree, but that's not an automatic profiler.</p>

<p>Update 3: Although this question is language agnostic, this arose because I am thinking of creating such a tool for R (unless it exists and I haven't found it).  </p>

<p>Update 4: Regarding use of a line profiler versus a call stack profiler - <a href=""http://stackoverflow.com/questions/7252602/digging-into-r-profiling-information/7297445#7297445"">this post</a> relating to using a call stack profiler (<code>Rprof()</code> in this case) exemplifies why it can be painful to work with the call stack rather than directly analyze things via a line profiler.</p>
"
124,Calling .NET/C# from R,"<p>I'd like to use an API from R that is only available in .NET.  Is there a standard method that can be used to call .NET C# code from R?  If so, how can I do so?</p>
"
125,How to scale down blocks without causing pixel overlapping,"<p>I have a bunch of blocks that needs to be drawn into a grid. Now displaying them unscaled everything is fine but when I try to scale them down to fit withing a window I get ""scale-artifacts"" because I use a normal scale-ratio formula and floats.</p>

<p>Is there a way to avoid these problems ?</p>

<p>Common example data:</p>

<pre><code>Original length: 200000
</code></pre>

<p>Scaled down to a <code>25x25 pixel grid</code> (it's this small for development and debugging)
The scaled down max length: 625 (25 * 25)</p>

<pre><code>Scale-ratio: (625 / 200000) = 0,003125
</code></pre>

<p><strong>Example data 1 - overlapping, scaled blocks overwrite each other</strong>
Start of block => end of block: [start, end)</p>

<pre><code>1: 2100 =&gt; 2800
2: 2800 =&gt; 3600
3: 3600 =&gt; 4500
4: 4500 =&gt; 5500
</code></pre>

<p>Jumping over showing the output of this example because I think example 2 and 3 will get the point across. Left it in for completeness.</p>

<p><strong>Example data 2 - incorrect space between 2 and 3</strong>
Start of block => end of block: [start, end)</p>

<pre><code>1: 960 =&gt; 1440
2: 1440 =&gt; 1920
3: 1920 =&gt; 2400

1: 960 =&gt; 1440, length: 480, scaled length: 1.5:
2: 1440 =&gt; 1920, length: 480, scaled length: 1.5:
3: 1920 =&gt; 2400, length: 480, scaled length: 1.5:
</code></pre>

<p>pixel start, end, length</p>

<pre><code>1: 3, 0, 1
2: 4, 0, 1
3: 6, 0, 1
</code></pre>

<p>Displayed grid:</p>

<pre><code>[ 0, 0, 0, 1, 2, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
...
</code></pre>

<p><strong>Example data 3 - 1 moved a step back incorrectly</strong>
Start of block => end of block: [start, end)</p>

<pre><code>1: 896 =&gt; 1344
2: 1344 =&gt; 1792
3: 1792 =&gt; 2240

1: 896 =&gt; 1344, length: 448, scaled length: 1.4:
2: 1344 =&gt; 1792, length: 448, scaled length: 1.4:
3: 1792 =&gt; 2240, length: 448, scaled length: 1.4:
</code></pre>

<p>pixel start, end, length</p>

<pre><code>1: 2, 0, 1
2: 4, 0, 1
3: 5, 0, 1
</code></pre>

<p>Displayed grid:</p>

<pre><code>[ 0, 0, 1, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
...
</code></pre>

<p>What example data 2 and 3 should have looked like:</p>

<pre><code>[ 0, 0, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
...
</code></pre>

<p>Remember the block values are [start, end)</p>

<p><strong>Preemptive strike (down-voters / trollers) Remember: I'm not psychic or a mind-reader. If you want to give negative do it in a constrictive way or it is useless (i will not learn anything) and will just pollute the thread.</strong></p>

<p><strong>Update</strong>  </p>

<pre><code>#include &lt;iostream&gt;
#include &lt;math.h&gt;
#include &lt;limits.h&gt;
#include &lt;assert.h&gt;
#include &lt;vector&gt;
#include &lt;array&gt;
#include &lt;utility&gt; // pair
#include &lt;algorithm&gt; // for_each

using namespace std;

const size_t width_size = 25; // 25 pixels
const size_t height_size = 25; // 25 pixels
const size_t grid_length = width_size * height_size; // width * height
array&lt;size_t, grid_length&gt; grid;

const size_t original_length = 200000;

typedef pair&lt;unsigned long, unsigned long&gt; block;
vector&lt;block&gt; test_values;

void show_grid()
{
    for (size_t y = 0; y &lt; height_size; ++y) {
        const size_t start_pos_for_current_heigth = y * width_size;
        const size_t end_pos_for_current_heigth = start_pos_for_current_heigth + width_size;

        cout &lt;&lt; ""[ "";
        for (size_t i = start_pos_for_current_heigth; i &lt; end_pos_for_current_heigth; ++i) {
            if (i + 1 &lt; end_pos_for_current_heigth)
                cout &lt;&lt; grid[i] &lt;&lt; "", "";
            else
                cout &lt;&lt; grid[i];
        };
        cout &lt;&lt; "" ]"" &lt;&lt; endl;
    }
}

void scale_and_add(const float scale)
{
    size_t test_value_id = 1;

    for_each(test_values.cbegin(), test_values.cend(), [&amp;](const block &amp;p) {
        const float s_f = p.first * scale;
        const unsigned long s = round(s_f);
        const float e_f = p.second * scale;
        const unsigned long e = round(e_f);
        const unsigned long block_length = p.second - p.first;
        const float block_length_scaled = block_length * scale;
        assert(s &lt;= grid_length);
        assert(e &lt;= grid_length);

        cout &lt;&lt; test_value_id &lt;&lt; "":"" &lt;&lt; endl;
        cout &lt;&lt; "" "" &lt;&lt; p.first &lt;&lt; "" =&gt; "" &lt;&lt; p.second &lt;&lt; "" length: "" &lt;&lt; block_length &lt;&lt; endl;
        cout &lt;&lt; "" "" &lt;&lt; s &lt;&lt; "" ("" &lt;&lt; s_f &lt;&lt; "") =&gt; "" &lt;&lt; e &lt;&lt; "" ("" &lt;&lt; e_f &lt;&lt; "") length: "" &lt;&lt; (e - s) &lt;&lt; "" ("" &lt;&lt; block_length_scaled &lt;&lt; "")"" &lt;&lt; "" (scaled)"" &lt;&lt; endl;

        for (size_t i = s; i &lt; e; ++i) {
            if (grid[i] != 0) {
                cout &lt;&lt; ""overlapp detected !"" &lt;&lt; endl;
            }
            grid[i] = test_value_id;
        }

        ++test_value_id;
    });
}

void reset_common()
{
    grid.fill(0);
    test_values.clear();
}

int main()
{
    const float scale = ((float)grid_length / (float)original_length);
    cout &lt;&lt; ""scale: "" &lt;&lt; scale &lt;&lt; "" length per pixel: "" &lt;&lt; ((float)original_length / (float)grid_length) &lt;&lt; endl;

    // Example data 1
/*    cout &lt;&lt; ""Example data 1"" &lt;&lt; endl;

    test_values.push_back(make_pair(2100, 2800));
    test_values.push_back(make_pair(2800, 3600));
    test_values.push_back(make_pair(3600, 4500));
    test_values.push_back(make_pair(4500, 5500));

    scale_and_add(scale);
    show_grid();

    reset_common();

    // Example data 2
    cout &lt;&lt; ""Example data 2"" &lt;&lt; endl;

    test_values.push_back(make_pair(960, 1440));
    test_values.push_back(make_pair(1440, 1920));
    test_values.push_back(make_pair(1920, 2400));

    scale_and_add(scale);
    show_grid();

    reset_common();

    // Example data 3
    cout &lt;&lt; endl &lt;&lt; ""Example data 3"" &lt;&lt; endl;

    test_values.push_back(make_pair(896, 1344));
    test_values.push_back(make_pair(1344, 1792));
    test_values.push_back(make_pair(1792, 2240));

    scale_and_add(scale);
    show_grid();

    reset_common();*/

    // Generated data - to quickly find the problem
    cout &lt;&lt; ""Generated data"" &lt;&lt; endl;

    auto to_op = [&amp;](const size_t v) {
        return v * (original_length / grid_length) * 1.3; // 1.4 and 1.5 are also good values to show the problem
    };
    size_t pos = 0;
    size_t psize = 1; // Note this value (length) and check it with the displayed one, you'll be surprised !
    for (size_t g = 0; g &lt; 10; ++g) {
        test_values.push_back(make_pair(to_op(pos), to_op(pos + psize)));
        pos += psize;
    }

    scale_and_add(scale);
    show_grid();

    return 0;
}
</code></pre>

<p>Output:  </p>

<pre><code>scale: 0.003125 length per pixel: 320
    Generated data
1:
 0 =&gt; 416 length: 416
 0 (0) =&gt; 1 (1.3) length: 1 (1.3) (scaled)
2:
 416 =&gt; 832 length: 416
 1 (1.3) =&gt; 3 (2.6) length: 2 (1.3) (scaled)
3:
 832 =&gt; 1248 length: 416
 3 (2.6) =&gt; 4 (3.9) length: 1 (1.3) (scaled)
4:
 1248 =&gt; 1664 length: 416
 4 (3.9) =&gt; 5 (5.2) length: 1 (1.3) (scaled)
5:
 1664 =&gt; 2080 length: 416
 5 (5.2) =&gt; 7 (6.5) length: 2 (1.3) (scaled)
6:
 2080 =&gt; 2496 length: 416
 7 (6.5) =&gt; 8 (7.8) length: 1 (1.3) (scaled)
7:
 2496 =&gt; 2912 length: 416
 8 (7.8) =&gt; 9 (9.1) length: 1 (1.3) (scaled)
8:
 2912 =&gt; 3328 length: 416
 9 (9.1) =&gt; 10 (10.4) length: 1 (1.3) (scaled)
9:
 3328 =&gt; 3744 length: 416
 10 (10.4) =&gt; 12 (11.7) length: 2 (1.3) (scaled)
10:
 3744 =&gt; 4160 length: 416
 12 (11.7) =&gt; 13 (13) length: 1 (1.3) (scaled)
[ 1, 2, 2, 3, 4, 5, 5, 6, 7, 8, 9, 9, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
</code></pre>

<p>This code example demonstrates my problem more clearly.</p>

<p>Interesting fact: mingw-g++, which i used to write this example, shows slightly different values. I usually use visual studio 2010 but couldn't this time because I'm not at home.</p>
"
126,How can I find the smallest value in a matrix that is greater than -Inf with R?,"<p>I have a matrix of values some of which are -Inf. How can I find the smallest value that is greater than -Inf?</p>
"
127,NA/NaN/Inf from rscript,"<p>what goes wrong?</p>

<pre><code>$&gt; priv/summary.r -i tests/current
Loading required package: reshape
Loading required package: plyr

Attaching package: 'reshape'

The following object(s) are masked from 'package:plyr':

    round_any

Loading required package: proto
Error: NA/NaN/Inf in foreign function call (arg 1)
Execution halted
make: *** [results] Error 1
</code></pre>

<p>It is Mac OS X 10.6</p>

<p>EDIT</p>

<p>summary.r:</p>

<pre><code>#!/usr/bin/env Rscript --vanilla

# Parse the --file= argument out of command line args and
# determine where base directory is so that we can source
# our common sub-routines
arg0 &lt;- sub(""--file=(.*)"", ""\\1"", grep(""--file="", commandArgs(), value = TRUE))
dir0 &lt;- dirname(arg0)
source(file.path(dir0, ""common.r""))

# Setup parameters for the script
params = matrix(c(
  'help',    'h', 0, ""logical"",
  'width',   'x', 2, ""integer"",
  'height',  'y', 2, ""integer"",
  'outfile', 'o', 2, ""character"",
  'indir',   'i', 2, ""character""
  ), ncol=4, byrow=TRUE)

# Parse the parameters
opt = getopt(params)

if (!is.null(opt$help))
  {
    cat(paste(getopt(params, command = basename(arg0), usage = TRUE)))
    q(status=1)
  }

# Initialize defaults for opt
if (is.null(opt$width))   { opt$width   = 1024 }
if (is.null(opt$height))  { opt$height  = 768 }
if (is.null(opt$indir))   { opt$indir  = ""current""}
if (is.null(opt$outfile)) { opt$outfile = file.path(opt$indir, ""summary.png"") }

# Load the benchmark data
b = load_benchmark(opt$indir)

# If there is no actual data available, bail
if (nrow(b$latencies) == 0)
{
  stop(""No latency information available to analyze in "", opt$indir)
}

png(file = opt$outfile, width = opt$width, height = opt$height)

# First plot req/sec from summary
plot1 &lt;- qplot(elapsed, successful / window, data = b$summary,
                geom = c(""smooth"", ""point""),
                xlab = ""Elapsed Secs"", ylab = ""Op/sec"",
                main = ""Throughput"") +
                geom_smooth(aes(y = failed / window, colour = ""Errors"")) +
                scale_colour_manual(name = """", values = c(""red""))

# Setup common elements of the latency plots
latency_plot &lt;- ggplot(b$latencies, aes(x = elapsed)) +
                   facet_grid(. ~ op) +
                   labs(x = ""Elapsed Secs"", y = ""Latency (ms)"")

# Plot 99 and 99.9th percentiles
plot2 &lt;- latency_plot +
            geom_smooth(aes(y = X99th, color = ""X99th"")) +
            geom_smooth(aes(y = X99_9th, color = ""X99_9th"")) +
            scale_color_hue(""Percentile"",
                            breaks = c(""X99th"", ""X99_9th""),
                            labels = c(""99th"", ""99.9th""))


# Plot median, mean and 95th percentiles
plot3 &lt;- latency_plot +
            geom_smooth(aes(y = median, color = ""median"")) +
            geom_smooth(aes(y = mean, color = ""mean"")) +
            geom_smooth(aes(y = X95th, color = ""X95th"")) +
            scale_color_hue(""Percentile"",
                            breaks = c(""median"", ""mean"", ""X95th""),
                            labels = c(""Median"", ""Mean"", ""95th""))

grid.newpage()

pushViewport(viewport(layout = grid.layout(3, 1)))

vplayout &lt;- function(x,y) viewport(layout.pos.row = x, layout.pos.col = y)

print(plot1, vp = vplayout(1,1))
print(plot2, vp = vplayout(2,1))
print(plot3, vp = vplayout(3,1))

dev.off()
</code></pre>

<p>EDIT 2:</p>

<p>This code is not mine, it is well tested (I guess) and works for thousands of people. But not for me. I thought that it is some well known issue with instalation or smth.. I have retested on ubuntu lucid with r from apt/cran repo with no luck</p>

<p>EDIT 3:</p>

<p>This is what I spot after reinstall. 
Additional packages are downloaded and build and then:</p>

<pre><code>The downloaded packages are in
    ‘/tmp/RtmpTeqAM7/downloaded_packages’
Warning messages:
1: In library(Name, character.only = TRUE, logical.return = TRUE) :
  there is no package called 'getopt'
2: In library(Name, character.only = TRUE, logical.return = TRUE) :
  there is no package called 'ggplot2'
Error: could not find function ""getopt""
Execution halted
</code></pre>

<p>Every next execution raises NA/NaN/Inf instead.</p>

<p>EDIT 4: </p>

<p>getopt --version
getopt (enhanced) 1.1.4</p>

<p>EDIT 5: </p>

<p>It looks for r's package getopt. I've added it - no change :/</p>

<p>ubuntu error:</p>

<pre><code>sudo Rscript ./priv/summary.r -i tests/current
Loading required package: reshape
Loading required package: plyr

Attaching package: 'reshape'

The following object(s) are masked from 'package:plyr':

    round_any

Loading required package: proto
null device 
          1 
</code></pre>

<p>EDIT 6:
common.r</p>

<pre><code># Load a library, or attempt to install it if it's not available
load_library &lt;- function(Name)
  {
    if (!library(Name, character.only = TRUE, logical.return = TRUE))
      {
        install.packages(Name, repos = ""http://lib.stat.cmu.edu/R/CRAN"")
      }
  }

# Load a latency file and ensure that it is appropriately tagged
load_latency_frame &lt;- function(File)
  {
    op &lt;- strsplit(basename(File), ""_"")[[1]][1]
    frame &lt;- read.csv(File)
    frame$op = rep(op, nrow(frame))
    return (frame)
  }

# Load summary and latency information for a given directory
load_benchmark &lt;- function(Dir)
  {
    ## Load up summary data
    summary &lt;- read.csv(sprintf(""%s/%s"", Dir, ""summary.csv""))

    ## Get a list of latency files
    latencies &lt;- lapply(list.files(path = Dir, pattern = ""_latencies.csv"",
                                   full.names = TRUE),
                        load_latency_frame)
    latencies &lt;- do.call('rbind', latencies)

    ## Convert timing information in latencies from usecs -&gt; msecs
    latencies[4:10] &lt;- latencies[4:10] / 1000

    return (list(summary = summary, latencies = latencies))
  }

load_library(""getopt"")
load_library(""grid"")
load_library(""ggplot2"")
</code></pre>
"
128,R: using data.table := operations to calculate new columns,"<p>Let's take the following data:</p>

<pre><code>dt &lt;- data.table(TICKER=c(rep(""ABC"",10),""DEF""),
        PERIOD=c(rep(as.Date(""2010-12-31""),10),as.Date(""2011-12-31"")),
        DATE=as.Date(c(""2010-01-05"",""2010-01-07"",""2010-01-08"",""2010-01-09"",""2010-01-10"",""2010-01-11"",""2010-01-13"",""2010-04-01"",""2010-04-02"",""2010-08-03"",""2011-02-05"")),
        ID=c(1,2,1,3,1,2,1,1,2,2,1),VALUE=c(1.5,1.3,1.4,1.6,1.4,1.2,1.5,1.7,1.8,1.7,2.3))
setkey(dt,TICKER,PERIOD,ID,DATE)
</code></pre>

<p>Now for each ticker/period combination, I need the following in a new column:</p>

<ul>
<li><code>PRIORAVG</code>: The mean of the latest VALUE of each ID, excluding the current ID, providing it is no more than 180 days old.</li>
<li><code>PREV</code>: The previous value from the same ID.</li>
</ul>

<p>The result should look like this:</p>

<pre><code>      TICKER     PERIOD       DATE ID VALUE PRIORAVG PREV
 [1,]    ABC 2010-12-31 2010-01-05  1   1.5       NA   NA
 [2,]    ABC 2010-12-31 2010-01-08  1   1.4     1.30  1.5
 [3,]    ABC 2010-12-31 2010-01-10  1   1.4     1.45  1.4
 [4,]    ABC 2010-12-31 2010-01-13  1   1.5     1.40  1.4
 [5,]    ABC 2010-12-31 2010-04-01  1   1.7     1.40  1.5
 [6,]    ABC 2010-12-31 2010-01-07  2   1.3     1.50   NA
 [7,]    ABC 2010-12-31 2010-01-11  2   1.2     1.50  1.3
 [8,]    ABC 2010-12-31 2010-04-02  2   1.8     1.65  1.2
 [9,]    ABC 2010-12-31 2010-08-03  2   1.7     1.70  1.8
[10,]    ABC 2010-12-31 2010-01-09  3   1.6     1.35   NA
[11,]    DEF 2011-12-31 2011-02-05  1   2.3       NA   NA
</code></pre>

<p>Note the <code>PRIORAVG</code> on row 9 is equal to 1.7 (which is equal to the <code>VALUE</code> on row 5, which is the only prior observation in the past 180 days by another <code>ID</code>)</p>

<p>I have discovered the <code>data.table</code> package, but I can't seem to fully understand the <code>:=</code> function. When I keep it simple, it seems to work. To obtain the previous value for each ID (I based this on the solution to <a href=""http://stackoverflow.com/questions/9843774/loop-through-data-table-to-calculate-conditional-averages"">this question</a>):</p>

<pre><code>dt[,PREV:=dt[J(TICKER,PERIOD,ID,DATE-1),roll=TRUE,mult=""last""][,VALUE]]
</code></pre>

<p>This works great, and it only takes 0.13 seconds to perform this operation over my dataset with ~250k rows; my vector scan function gets identical results but is about 30,000 times slower.</p>

<p>Ok, so I've got my first requirement. Let's get to the second, more complex requirement. Right now the fasted method so far for me is using a couple of vector scans and throwing the function through the <code>plyr</code> function <code>adply</code> to get the result for each row.</p>

<pre><code>calc &lt;- function(df,ticker,period,id,date) {
  df &lt;- df[df$TICKER == ticker &amp; df$PERIOD == period 
        &amp; df$ID != id &amp; df$DATE &lt; date &amp; df$DATE &gt; date-180, ]
  df &lt;- df[order(df$DATE),]
  mean(df[!duplicated(df$ID, fromLast = TRUE),""VALUE""])
}

df &lt;- data.frame(dt)
adply(df,1,function(x) calc(df,x$TICKER,x$PERIOD,x$ID,x$DATE))
</code></pre>

<p>I wrote the function for a <code>data.frame</code> and it does not seem to work with a <code>data.table</code>. For a subset of 5000 rows this takes about 44 seconds but my data consists of > 1 million rows. I wonder  if this can be made more efficient through the usage of <code>:=</code>.</p>

<pre><code>dt[J(""ABC""),last(VALUE),by=ID][,mean(V1)]
</code></pre>

<p>This works to select the average of the latest VALUEs for each ID for ABC.</p>

<pre><code>dt[,PRIORAVG:=dt[J(TICKER,PERIOD),last(VALUE),by=ID][,mean(V1)]]
</code></pre>

<p>This, however, does not work as expected, as it takes the average of all last VALUEs for all ticker/periods instead of only for the current ticker/period. So it ends up with all rows getting the same mean value. Am I doing something wrong or is this a limitation of <code>:=</code> ?</p>
"
129,Loading depending packages using .onLoad,"<p>My package requires ggplot2 package, but I am having trouble to fix the following NOTES that I get when I run R CMD check.</p>

<pre><code>no visible global function definition for qplot
'library' or 'require' call not declared from: ggplot2
</code></pre>

<p>I also have a .onLoad function,</p>

<pre><code>.onLoad &lt;- function(libname, pkgname){
.libPaths(""~/RLibrary"")
require(ggplot2)
}
</code></pre>

<p>Any suggestions on how to solve the errors? Where should I place the onLoad function?</p>

<p>Thank you<br>
San</p>
"
130,Issues with subspace lda algorithm,"<p>I have this issue with subspace lda algorithm implementation. I am not sure why but it is coming to be worse than PCA. I am testing it in the ORL database. Here is my code</p>

<pre><code>load person_orl

trainsize = 8;
%% start eigenface computation
%append the images in columns of a matrix A
images = []; 
SUBJECTS 
for i=1:SUBJECTS
    for j=1:trainsize
        face = cell2mat(person(i).faces(j));
        facecol = reshape(face, [], 1);
        images = [images facecol];
        totaltrain = totaltrain + 1;
    end
end
%
images = double(images);

% find mean
meanimage = mean(images, 2); %mean in column DIM
%subtract mean image
A = zeros(size(images)); % A = mean subtracted image
for i=1:size(images,2)
    A(:, i) = images(:,i) - meanimage;
end
%covariance matrix
%X = A * A';
% PCA of this covariance is computationally expensive, so workaround
%Y = 1/(SUBJECTS * trainsize) * A' * A;
Y = A' * A;
whos images meanimage subtractedmean X Y
% after finding eigen vector of Y, eigen vector of X = subtractedmean * V
% eigen values are the same

[V, lambda] = eig(Y);

images = double(images);
eigenfaces = A * V; %eigenvectors

for i=1:size(lambda,1)
    lambda_values(i) = lambda(i,i);
end
%sort the vectors according to eigenvalues
[tr index] = sort(lambda_values, 2, 'descend');
eigenfaces = eigenfaces(:, index);
lambda_values = lambda_values(index);
%vectorsize = ceil( FRACTION * length(lambda_values) );
eigenfacestouse = eigenfaces(:,1: (totaltrain - SUBJECTS));

% plot eigenfaces
%{
figure
colormap('gray');

for i=1:10
    subplot(2,5,i)
    %vector normalized, reshape to image
    imagesc(reshape(eigenfaces(:,i) ./ norm(eigenfaces(:,i)), row, col));
    axis off
    %imshow(reshape(eigenfaces(:,i) ./ (max(eigenfaces(:,i))) , row, col));
end
suptitle('Eigenfaces');
pause();
%}

%% project in eigen space (coordinates for the images in new space)
images_eigen_space = eigenfacestouse' * A;

%A_pca has the images projected in new space (in each column)
%find overall mean
mean_eigen_space = mean(images_eigen_space, 2);
%find class specific mean images
class_mean_eigen_space = zeros(size(mean_eigen_space, 1), SUBJECTS);
for i=1:SUBJECTS
    start = (i-1) * trainsize + 1;
    col_span = start : (start+trainsize-1) ;
    class_mean_eigen_space(:, i) = mean(images_eigen_space(:, col_span), 2);
end
%class_mean_eigen_space(20, :)

%% compute total, between and within scatter matrices (S, Sb, Sw)
A_eigen_space = images_eigen_space - repmat(mean_eigen_space, 1, totaltrain);
S = A_eigen_space * A_eigen_space';
Sb = zeros(size(S));
Sw = zeros(size(S));

%between
for i=1:SUBJECTS
    a = (class_mean_eigen_space(:, i) - mean_eigen_space);
    Sb = Sb +  a * a';
end
Sb = Sb/SUBJECTS;

%within
img_index = 0;

for i=1:SUBJECTS
    class_sigma = zeros(size(S));
    for j=1:trainsize
        img_index = img_index + 1;
        a = (images_eigen_space(:, img_index) - class_mean_eigen_space(:, i));
        class_sigma = class_sigma +  a * a';
    end
    class_sigma = (1/trainsize) * class_sigma;
    Sw = Sw + class_sigma;
    %disp(['completed ' num2str(i)]);
end
Sw = (1/SUBJECTS) * Sw;

%% optimization
%[eig_vec, eig_val] = eig(inv(Sw) * Sb); % Cost function J = inv(Sw) * Sb
[eig_vec, eig_val] = eig(Sb, Sw); % Cost function J = inv(Sw) * Sb
eig_vec = fliplr(eig_vec);

%only take SUBJECTS - 1 eigen vectors
fisher_faces = eig_vec(:, 1: (SUBJECTS-1) );

fisher_training_projected  = fisher_faces' * images_eigen_space;

%% Testing
%required: 
%mean in original space
%eigenfaces and fisherfaces (eigen vector and fisher vectors)
%projected images in the fisher face space (for comparison)

figure;
testimagenumber = 0;
error = 0;
for i=1:SUBJECTS
    for j=trainsize+1:length(person(i).faces)
        testimagenumber = testimagenumber + 1;
        testim = cell2mat(person(i).faces(j));
        %imshow(im); pause();

        %subtract mean image
        testimdouble = double(testim) - reshape(meanimage, row, col);

        %project mean subtracted image into the eigen space, 
        %calculate weight i.e. coordinate acc to new basis vectors
        testimvector = reshape(testimdouble, [], 1);

        fisher_im_projected = fisher_faces' * eigenfacestouse' * testimvector; % Test image feature vector

        %size(fisher_training_projected)
        %size(im_fisher_projected)


        distanceToTrain = zeros(totaltrain, 1);
        %find distance between this test image and training images
        for k=1:totaltrain
            distanceToTrain(k) = norm(fisher_training_projected(:,k) - fisher_im_projected, 2);
        end

        [tr index] = sort(distanceToTrain);
        disp(['matched image' num2str(index(1))]);
        idPersonPredicted = floor((index(1) - 1) / trainsize) + 1;
        disp(['matched image id ' num2str(idPersonPredicted)]);

        if( i ~= idPersonPredicted)
            disp('error');
            error = error + 1;
        end
        %display
        subplot(1,4,1);
        imshow(testim)
        title('test image');
        firstmatch = uint8(reshape(images(:,index(1)), row, col));
        secondmatch = uint8(reshape(images(:,index(2)), row, col));
        thirdmatch = uint8(reshape(images(:,index(3)), row, col));
        subplot(1,4,2);
        imshow(firstmatch);
        title('first match');
        subplot(1,4,3);
        imshow(secondmatch);
        title('second match');
        subplot(1,4,4);
        imshow(thirdmatch);
        title('third match');
        pause();
    end
end
disp(['error : ' num2str(error/testimagenumber * 100)]);
close all
</code></pre>

<p>I am not sure what I have done wrong. Can anyone tell me where I am wrong.</p>
"
131,Replace for loop with formula,"<p>I have this loop that runs in O(end - start) and I would like to replace it with something O(1).<br>
If ""width"" wouldn't be decreasing, it would be pretty simple.</p>

<pre><code>for (int i = start; i &lt;= end; i++, width--)
    if (i % 3 &gt; 0) // 1 or 2, but not 0
        z += width;
</code></pre>

<p>start, end and width have positive values</p>
"
132,Efficiently Row Standardize a Matrix,"<p>I need an efficient way to row standardize a sparse matrix.</p>

<p>Given</p>

<pre><code>W = matrix([[0, 1, 0, 1, 0, 0, 0, 0, 0],
            [1, 0, 1, 0, 1, 0, 0, 0, 0],
            [0, 1, 0, 0, 0, 1, 0, 0, 0],
            [1, 0, 0, 0, 1, 0, 1, 0, 0],
            [0, 1, 0, 1, 0, 1, 0, 1, 0],
            [0, 0, 1, 0, 1, 0, 0, 0, 1],
            [0, 0, 0, 1, 0, 0, 0, 1, 0],
            [0, 0, 0, 0, 1, 0, 1, 0, 1],
            [0, 0, 0, 0, 0, 1, 0, 1, 0]])
row_sums = W.sum(1)
</code></pre>

<p>I need to produce...</p>

<pre><code>W2 = matrix([[0.  , 0.5 , 0.  , 0.5 , 0.  , 0.  , 0.  , 0.  , 0.  ],
             [0.33, 0.  , 0.33, 0.  , 0.33, 0.  , 0.  , 0.  , 0.  ],
             [0.  , 0.5 , 0.  , 0.  , 0.  , 0.5 , 0.  , 0.  , 0.  ],
             [0.33, 0.  , 0.  , 0.  , 0.33, 0.  , 0.33, 0.  , 0.  ],
             [0.  , 0.25, 0.  , 0.25, 0.  , 0.25, 0.  , 0.25, 0.  ],
             [0.  , 0.  , 0.33, 0.  , 0.33, 0.  , 0.  , 0.  , 0.33],
             [0.  , 0.  , 0.  , 0.5 , 0.  , 0.  , 0.  , 0.5 , 0.  ],
             [0.  , 0.  , 0.  , 0.  , 0.33, 0.  , 0.33, 0.  , 0.33],
             [0.  , 0.  , 0.  , 0.  , 0.  , 0.5 , 0.  , 0.5 , 0.  ]]) 
</code></pre>

<p>Where,</p>

<pre><code>for i in range(9):
    W2[i] = W[i]/row_sums[i]
</code></pre>

<p>I'd like to find a way to do this without loops (i.e. Vectorized) and using Scipy.sparse matrices. W could be as large at 10mil x 10mil.</p>
"
133,How to scale a value down?,"<p>I have a variable <code>x</code> that accepts a range of values from 0 - 500. </p>

<p>I want to represdnt this variable's value in a new variable <code>xScaled</code> that accepts a range of 0 - 1. </p>

<p><strong>Example:</strong> 
<em>Given <code>x = 292</code> what is the relative value of <code>xScaled</code> and how can this be calculated?</em> </p>

<p>Thanks </p>
"
134,making MacVim work with R - tutorial for beginners,"<p>Please,</p>

<p>does anyone know a tutorial for how to execute R codes from MacVim? I found some plugins but I was not even able to install any of them... the instructions usually don't make much sense for beginners.</p>

<p>Thanks!</p>
"
135,Calculation - numpy python bug ,"<p>I am using NumPy to do some calculation on finding the Y intercept, through an Aperture between a big box and a small box. I have over 100.000 particles in the big box, and around 1000 in the small one. And it's taking a lot of time to do so. All the self.YD, self.XD are very large arrays that i'm multiplying. </p>

<p>PS: The ind are indexes of the values that need to be multiplied. I had a nonzero condition before that line in my code. </p>

<p>Any ideas how I would do this calculation in a simpler way?</p>

<pre><code>YD_zero = self.oldYD[ind] - ((self.oldYD[ind]-self.YD[ind]) * self.oldXD[ind])/(self.oldXD[ind]-self.XD[ind])
</code></pre>

<p>Thanks!</p>

<p><strong>UPDATE</strong></p>

<p>Would using multiply, divide, subtract and all that stuff of Numpy. make it faster?
Or if maybe if i split the calculation. for example.</p>

<p>to do this first:</p>

<pre><code>    YD_zero = self.oldYD[ind] - ((self.oldYD[ind]-self.YD[ind])*self.oldXD[ind])
</code></pre>

<p>and then the next line would be:</p>

<pre><code>    YD_zero /= (self.oldXD[ind]-self.XD[ind])
</code></pre>

<p>Any suggestions?!</p>

<p><strong>UPDATE 2</strong></p>

<p>I have been trying to figure this out, in a while now, but not much progress. My concern is that the denominator :</p>

<pre><code>    self.oldXL[ind]-self.XL[ind] == 0
</code></pre>

<p>and I am getting some weird results.</p>

<p>The other thing is the nonzero function. I have been testing it for a while now. Could anybody tell me that it is almost the same as find in Matlab</p>
"
136,Equal frequency discretization in R,"<p>I'm having trouble finding a function in R that performs equal-frequency discretization.  I stumbled on the 'infotheo' package, but after some testing I found that the algorithm is broken.  'dprep' seems to no longer be supported on CRAN.</p>

<p>EDIT :</p>

<p>For clarity, I do not need to seperate the values between the bins. I really want equal frequency, it doesn't matter if one value ends up in two bins. Eg :</p>

<pre><code>c(1,3,2,1,2,2) 
</code></pre>

<p>should give a bin <code>c(1,1,2)</code> and one <code>c(2,2,3)</code></p>
"
137,multiprocessing.Process - Variable as function,"<p>I am using the code below to parallelize the processing of a numpy array.  The target function in this case performs a simple linear stretch on the input data.  The array is segmented and then fed to the pool in chunks.  This is working quite well thanks to the numerous parallel processing with python posts.  </p>

<pre><code>        pool = [multiprocessing.Process(target=linear_stretch, args= (shared_arr,slice(i, i+step), 35, 200, 2.0)) for i in range (0, y, step)]
</code></pre>

<p>My question is, is it possible to do something like the following:</p>

<pre><code>stretch = Linear.linear_stretch()
</code></pre>

<p>Where I create an object of the function (please correct my vocab!) and then call it in the multiprocessing.Process.  </p>

<p>The module that the function resides in currently looks like:</p>

<pre><code>Linear.py

import numpy

def linear_stretch(args):
    #Do some stuff
</code></pre>
"
138,charts.PerformanceSummary: how to set sequential colors,"<pre><code>require(quantmod)
require(PerformanceAnalytics)

getSymbols(c('SP500', 'WAAA', 'WBAA'), src = 'FRED', from = '1950-01-01')
X &lt;- na.omit(merge(to.weekly(SP500), WAAA, WBAA))

dWAAA &lt;- diff(WAAA / 100, 1)
dWBAA &lt;- diff(WBAA / 100, 1)

D &lt;- 20

dP.WAAA &lt;- - D * dWAAA
dP.WBAA &lt;- - D * dWBAA

charts.PerformanceSummary(p = .99, R = dP.WAAA['1990-01-01/2012-08-17'],
                          methods = 'ModifiedES', width = 48)
charts.PerformanceSummary(p = .99, R = dP.WBAA['1990-01-01/2012-08-17'],
                          methods = 'ModifiedES', width = 48)
</code></pre>

<p>May you tell me any way to set colors' smoothing-transitions-sequential palette in order to replace default black color with something which looks nicer?</p>

<p>I would like something which is blue-based and changes blue variety starting from the 1st plot ending to the 3rd one.</p>

<p>Thanks,</p>
"
139,How can I assign different symbols to different groups in a lattice plot?,"<p>Say I have 3 groups with 3 dots each on the plot and I need a black &amp; white version where the 3 dots of the 3 groups are all displayed with different symbols.
How should I specify the panel.superpose function?</p>

<p><a href=""http://www.r-bloggers.com/working-with-themes-in-lattice-graphics/"" rel=""nofollow"">http://www.r-bloggers.com/working-with-themes-in-lattice-graphics/</a>
http://stat.ethz.ch/R-manual/R-devel/library/lattice/html/panel.superpose.html</p>
"
140,How to map a latitude/longitude to a distorted map?,"<p>I have a bunch of latitude/longitude pairs that map to known x/y coordinates on a (geographically distorted) map.</p>

<p>Then I have one more latitude/longitude pair. I want to plot it on the map as best is possible. How do I go about doing this?</p>

<p>At first I decided to create a system of linear equations for the three nearest lat/long points and compute a transformation from these, but this doesn't work well at all. Since that's a linear system, I can't use more nearby points either.</p>

<p>You can't assume North is up: all you have is the existing lat/long->x/y mappings.</p>

<p>EDIT: it's not a Mercator projection, or anything like that. It's arbitrarily distorted for readability (think subway map). I want to use only the nearest 5 to 10 mappings so that distortion on other parts of the map doesn't affect the mapping I'm trying to compute.</p>

<p>Further, the entire map is in a very small geographical area so there's no need to worry about the globe--flat-earth assumptions are good enough.</p>"
141,Rearanging labels of ggplot scatterplot with the direct labels library in R,"<p>I am trying to organize the labels of my ggplot scatterplot so that the labels don't overlap with one another. For this purpose, I am trying to use the direct labels library but I cannot get it to work. When I tried the code:</p>

<pre><code>mytable &lt;- read.csv('http://www.fileden.com/files/2012/12/10/3375236/My%20Documents/CF1_deNovoAssembly.csv', sep="","",  header=TRUE)

mytable$Consensus.length &lt;- log(mytable$Consensus.length)

mytable$Average.coverage &lt;-log(mytable$Average.coverage)

mytable$Name &lt;- do.call(rbind,strsplit(as.character(mytable$Name), "" "", '['))[,3]

ggplot(mytable, aes(x=Consensus.length, y=Average.coverage, label=Name)) + geom_point() + ylab(""Contig Average Coverage (log)"") + xlab(""Contig Consensus Length (log)"") + opts(title=""Contig Coverage vs Length"") + geom_text(hjust=0, vjust=-0.2, size=4)
direct.label(p, ""first.qp"")
</code></pre>

<p>I got this error:</p>

<pre><code>Error in direct.label.ggplot(p, ""first.qp"") : 
  Need colour aesthetic to infer default direct labels.
</code></pre>

<p>So I changed the plotting script by adding aes to the geom_point()</p>

<pre><code>ggplot(mytable, aes(x=Consensus.length, y=Average.coverage, label=Name)) + geom_point(aes(colour=Average.coverage)) + ylab(""Contig Average Coverage (log)"") + xlab(""Contig Consensus Length (log)"") + opts(title=""Contig Coverage vs Length"") + geom_text(hjust=0, vjust=-0.2, size=4)
</code></pre>

<p>And now I get the following error</p>

<pre><code>Error in order.labels(d) : labels are not aligned
</code></pre>

<p>I found <a href=""http://stackoverflow.com/questions/7611169/intelligent-point-label-placement-in-r"">this thread</a> in which they suggest either placing the labels manually if only a few data points or not at all if too many data points. I agree with this but I will be generating this graph with many different data sets and I do need the data labels. So far this is how the graph looks
<img src=""http://i.stack.imgur.com/9u75l.png"" alt=""enter image description here""></p>
"
142,Change interlinear space in ggplot title?,"<p>If I use <code>\n</code> with <code>labs(title=""whatever \n comes after this"")</code> I end up with quite a big space between the lines. Is there a way to influence it? (I mean except font-size of the title itself).</p>
"
143,Why is one's compement representation better than others,"<p>It looks like one's complement representation of signed numbers is the most popular now (and probably the only representation used in the modern hardware). Why is it exactly better than others?</p>
"
144,as.formula does not like equivalence '=' (object not found),"<p>consider the following example</p>

<pre><code>df1 &lt;- data.frame(a=c(1,2,3),b=c(2,4,6));
transform(df1,c=a+b)
    a b c
  1 1 2 3
  2 2 4 6
  3 3 6 9
</code></pre>

<p>So far, so good. Now I would like to code this dynamically, using as.formula:</p>

<pre><code>transform(df1,as.formula(""c=a+b""))
</code></pre>

<p>However, R says</p>

<pre><code>Error in eval(expr, envir, enclos) : object 'b' not found
</code></pre>

<p>This error does not occur using ""~"" as separator of left hand and right hand side. Can I somehow delay the evaluation of the formula? Is it possible at all to use as.formula on an assignment? I have tried fiddling around with 'with' but to no avail.</p>
"
145,R table column order when including the empty string,"<p>I have a series of value that include the empty string</p>

<blockquote>
  <p>levels(mydata$phone_partner_products)
    """"                     ""dont_know""            ""maybe_interesting""<br>
    ""not_interesting""      ""very_interesting""     ""very_not_interesting""</p>
</blockquote>

<p>If I make a frequencies table I get this
table(mydata$phone_partner_products)</p>

<pre><code>                            dont_know    maybe_interesting 
            3752                  226                 2907 
 not_interesting     very_interesting very_not_interesting 
            1404                 1653                 1065
</code></pre>

<p>How can I reorder the columns in a more meaningful way?
How can I rename the empty string """" level?</p>

<p>Thank you a lot in advance.</p>
"
146,Save extracted features opencv,"<p>I have extracted the SIFT feature descriptors in OpenCV.. Next step to me is to train an SVM but before that step, I think I need to save the extracted features in a file, in order to train these features... </p>

<p>So my questions are: 1- how to save these matrices?
2- I would like to know what to do about the features, I extracted features for one image, but I need to extract features for many images in different pose for one object (e.g cola) and save them in one file...</p>

<p>Do you have any idea, how to do that in opencv? </p>

<p>Thank you... </p>
"
147,Common mistakes with javascript arithmetic,"<p>I've ran into several oddities using javascripts floating point arithmetic, but I can never recall them off the top of my head!</p>

<p><strong>What are some common mistakes when using JavaScript to do math?</strong></p>
"
148,"How to calculate probabilities from confusion matrices? need denominator, chars matrices","<p><a href=""http://acl.ldc.upenn.edu/C/C90/C90-2036.pdf"" rel=""nofollow"">This paper</a> contains confusion matrices for spelling errors in a noisy channel. It describes how to correct the errors based on conditional properties.</p>

<p>The conditional probability computation is on page 2, left column. In footnote 4, page 2, left column, the authors say: ""The chars matrices  can  be   easily  replicated, and are therefore omitted from the appendix."" I cannot figure out how can they be replicated!</p>

<p><strong>How to replicate them? Do I need the original corpus? or, did the authors mean they could be recomputed from the material in the paper itself?</strong></p>
"
149,Algorithm need assistance,"<p>I'm having some trouble thinking of an algorithm for my problem. I'll try to explain the best I can. This is just an example.</p>

<p>So say 3% of 1000 people clicked on a link.</p>

<p>So I need an algorithm that would spread a click randomly over 1000 views. </p>

<p>It's easy to spread it evenly eg: 1000 * .03 = 30, so every 30 views someone would click a link.</p>

<p>EG:</p>

<pre><code>For (i = 0; i&lt;1000; i++) {
if(i % 30 == 0);
click()
}
</code></pre>
"
150,How to apply a library command on each row and create a new variable,"<p>I'm having a little trouble creating a new variable using a command within the ineq() library, which calculates a Gini coefficient.  The vector I give the ineq command is a list of the columns I'm interested in.  I want to run this command for each individual row and then append the new variable.</p>

<p>When I attempt this as a loop or using a ddply (which I'm just learning now), the output is the same Gini score (for the entire data set) for each row.  </p>

<p><strong>How can I run this command for each individual row?</strong>  Thanks in advance!</p>

<pre><code>library(ineq)
df &lt;- data.frame( user = 1:5, v1 = c(2,4,6,8,10), v2 = c(1,5,11,5,1), v3 = c(3,1,2,7,9))

for (i in nrow(df)) {
  df$gini &lt;- ineq(c(df$v1, df$v2, df$v3))
}

myGini &lt;- ddply(df, .(user), gini=ineq(c(v1, v2, v3)))
</code></pre>
"
151,"In R draw two lines, with slopes double and half the value of the best fit line","<p>I have data with a best fit line draw.  I need to draw two other lines.  One needs to have double the slope and the other need to have half the slope.  Later I will use the region to differentially color points outside it as per:
<a href=""http://stackoverflow.com/questions/2687212/conditionally-colour-data-points-outside-of-confidence-bands-in-r"">http://stackoverflow.com/questions/2687212/conditionally-colour-data-points-outside-of-confidence-bands-in-r</a></p>

<h1>Example dataset:</h1>

<pre><code>## Dataset from http://www.apsnet.org/education/advancedplantpath/topics/RModules/doc1/04_Linear_regression.html

## Disease severity as a function of temperature

# Response variable, disease severity
diseasesev&lt;-c(1.9,3.1,3.3,4.8,5.3,6.1,6.4,7.6,9.8,12.4)

# Predictor variable, (Centigrade)
temperature&lt;-c(2,1,5,5,20,20,23,10,30,25)

## For convenience, the data may be formatted into a dataframe
severity &lt;- as.data.frame(cbind(diseasesev,temperature))

## Fit a linear model for the data and summarize the output from function lm()
severity.lm &lt;- lm(diseasesev~temperature,data=severity)

# Take a look at the data
plot(
  diseasesev~temperature,
  data=severity,
  xlab=""Temperature"",
  ylab=""% Disease Severity"",
  pch=16,
  pty=""s"",
  xlim=c(0,30),
  ylim=c(0,30)
)
title(main=""Graph of % Disease Severity vs Temperature"")
par(new=TRUE) # don't start a new plot
abline(severity.lm, col=""blue"")
</code></pre>
"
152,Repeat elements of vector in R,"<p>I'm trying to repeat the elements of vector a, b number of times. That is, a=""abc"" should be ""aabbcc"" if y = 2.</p>

<p>Why doesn't either of the following code examples work?</p>

<pre><code>sapply(a, function (x) rep(x,b))
</code></pre>

<p>and from the plyr package,</p>

<pre><code>aaply(a, function (x) rep(x,b))
</code></pre>

<p>I know I'm missing something very obvious ...</p>
"
153,How to make a checkerboard in numpy?,"<p>I'm using numpy to initialize a pixel array to a gray checkerboard (the classic representation for ""no pixels"", or transparent).  It seems like there ought to be a whizzy way to do it with numpy's amazing array assignment/slicing/dicing operations, but this is the best I've come up with:</p>

<pre><code>w, h = 600, 800
sq = 15    # width of each checker-square
self.pix = numpy.zeros((w, h, 3), dtype=numpy.uint8)
# Make a checkerboard
row = [[(0x99,0x99,0x99),(0xAA,0xAA,0xAA)][(i//sq)%2] for i in range(w)]
self.pix[[i for i in range(h) if (i//sq)%2 == 0]] = row
row = [[(0xAA,0xAA,0xAA),(0x99,0x99,0x99)][(i//sq)%2] for i in range(w)]
self.pix[[i for i in range(h) if (i//sq)%2 == 1]] = row
</code></pre>

<p>It works, but I was hoping for something simpler.</p>
"
154,How to compute error rate from a decision tree?,"<p>Does anyone know how to calculate the error rate for a decision tree with R?
I am using the <code>rpart()</code> function.</p>
"
155,Apply function to every column in every table in list R,"<p>I have a list of tables in R. The number of rows differs from table to table but the number of columns is uniform (22). I need to calculate the min,max,and median of each column in each table and put that into a vector. There needs to be a separate vector per table.</p>
"
156,c++ pow function- invalid result?,"<p>Why is the output of the dResult invalid ?<br>
Env: Visual Studio 2008   </p>

<pre><code>int _tmain(int argc, _TCHAR* argv[])  
{  
   double dN = - 0.091023604111478473 ;  
   double dD = 0.127777777777777;  
   double dResult =  pow( dN,dD );   
   //dResult = -1.#IND000000000000  
   return 0;  
}   
</code></pre>

<p>Thanks in advance..</p>
"
157,Extract part of string,"<p>I've been trying to extract something from a string (actually a $call) in R, and it's driving me nuts. If you have:</p>

<pre><code>library(vars)
data &lt;- as.data.frame(matrix(c(runif(40)), ncol=2))
z &lt;- matrix(c(runif(40)), ncol=2)
var.modell &lt;- VAR(data, p = 2, exogen=z, type = ""trend"")
</code></pre>

<p>How do you extract the z? I've tried googling and searching stack overflow. I found this: <a href=""http://stackoverflow.com/questions/12443278/r-extract-a-part-of-a-string-in-r"">R extract a part of a string in R</a></p>

<p>which made me try:</p>

<pre><code>sub("".*?exogen=(.*?)"", ""\\1"", var.modell$call, perl = TRUE)
</code></pre>

<p>But it returns:</p>

<pre><code>[1] ""VAR""   ""data""  ""2""     ""trend"" ""z""    
</code></pre>

<p>What am I doing wrong?</p>
"
158,Finding the LCM of a range of numbers,"<p>I read an interesting DailyWTF post today, <a href=""http://thedailywtf.com/Articles/Out-of-All-the-Possible-Answers.aspx"">""Out of All The Possible Answers...""</a> and it interested me enough to dig up the original <a href=""http://forums.thedailywtf.com/forums/t/10030.aspx"">forum post</a> where it was submitted. This got me thinking how I would solve this interesting problem - the original question is posed on <a href=""http://projecteuler.net/index.php?section=problems&amp;id=5"">Project Euler</a> as: </p>

<blockquote>
  <p>2520 is the smallest number that can be divided by each of the 
  numbers from 1 to 10 without any remainder.</p>
  
  <p>What is the smallest number that is evenly divisible by all of 
  the numbers from 1 to 20?</p>
</blockquote>

<p>To reform this as a programming question, <strong>how would you create a function that can find the Least Common Multiple for an arbitrary list of numbers?</strong></p>

<p>I'm incredibly bad with pure math, despite my interest in programming, but I was able to solve this after a little Googling and some experimenting. I'm curious what other approaches SO users might take. If you're so inclined, post some code below, hopefully along with an explanation. Note that while I'm sure libraries exist to compute the GCD and LCM in various languages, I'm more interested in something that displays the logic more directly than calling a library function :-) </p>

<p>I'm most familiar with Python, C,  C++, and Perl, but any language you prefer is welcome. Bonus points for explaining the logic for other mathematically-challenged folks out there like myself.</p>

<p><strong>EDIT</strong>: After submitting I did find this similar question <a href=""http://stackoverflow.com/questions/147515"">Least common multiple for 3 or more numbers</a> but it was answered with the same basic code I already figured out and there's no real explanation, so I felt this was different enough to leave open.</p>
"
159,array arithmetic,"<p>I have one array as this:</p>

<pre><code>  ""4"": ""40000"",
  ""5"": ""3000"",
  ""6"": ""200"",
  ""7"": ""10"",
  ""8"": ""1""
</code></pre>

<p>I want to have an another array which adds all the bigger ones to smaller ones. I hope it makes sense. How can I achieve this?</p>

<pre><code>  ""4"": ""43211"",
  ""5"": ""3211"",
  ""6"": ""211"",
  ""7"": ""11"",
  ""8"": ""1""
</code></pre>

<p>Just to find an efficient method so language doesn't matter but if necessary I use php or javascript. It is an associative array so it is not sorted. And another trick is that array might or might not contain some elements. So for example ""6"" or ""8"" might be missing.</p>
"
160,"How to delete """" from csv values in R and how to change column Names when writing to a CSV with R","<p>I`m trying to create csv files out of an R table. But i cant understand why all the values get
checked with """" when i use the write.csv() function. my data looks like this:</p>

<pre><code>        Sample  Sample_Name Sample_Group    Pool_ID Sample_Plate    NorTum  Sentrix_ID    Sentrix_Position  HybNR
    1   00_11242    00_24200N2  MUTYH   GS0005703-OPA   GS0010004-DNA   N     1280307   R007_C005   1
    2   00_11242    00_24200N2  MUTYH   GS0005704-OPA   GS0010004-DNA   N   1280307 R007_C011   1
    3   00_11242    00_24200N2  MUTYH   GS0005702-OPA   GS0010004-DNA   N   1416198 R007_C011   2
    4   00_11242    00_24200N2  MUTYH   GS0005701-OPA   GS0010004-DNA   N   1416198 R007_C005   2
    5   00_7    00_7T   MUTYH   GS0005701-OPA   GS0010004-DNA   T   1416198 R006_C005   2
    6   00_7    00_7T   MUTYH   GS0005702-OPA   GS0010004-DNA   T   1416198 R006_C011   2
    7   00_7    00_7T   MUTYH   GS0005704-OPA   GS0010004-DNA   T   1280307 R006_C011   1
    8   00_7    00_7T   MUTYH   GS0005703-OPA   GS0010004-DNA   T   1280307 R006_C005   1
    9   01_677  01_677N HNPCC_UV    GS0005701-OPA   GS0010004-DNA   N   1416198 R002_C006   2
    10  01_677  01_677N HNPCC_UV    GS0005704-OPA   GS0010004-DNA   N   1280307 
</code></pre>

<p>And in the output file which is a CSV every value looks like this:</p>

<pre><code>""100"" ""R05_80611"" ""R05_80611N"" ""NA_FAM"" ""GS0005701-OPA"" ""GS0010004-DNA"" ""N"" 1416198 ""R003_C006"" 2
</code></pre>

<p>Why is the function putting quotes around my values and is there a way to quickly do this.
I<code>m also wondering how i can change my column names when writing to CSV files but since i</code>m a starter i`ve no clue on how to do this.</p>

<p>Thanks in advance!</p>

<p>Grts Sander</p>
"
161,UberCart statistics on products added,"<p>I want statistics on the products added to carts, but not checked out. I.e. if a user adds a product to his cart, but doesnt actual pay out, how can I see these products that were added? Or maybe even get notifications everytime a product is added?</p>
"
162,"Get point IDs after clustering, using python","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stackoverflow.com/questions/1545606/python-k-means-algorithm"">Python k-means algorithm</a>  </p>
</blockquote>



<p>I want to <strong>cluster</strong> 10000 indexed points based on their feature vectors and <strong>get their ids</strong> after clustering i.e. cluster1:[p1, p3, p100, ...], cluster2:[...] ...</p>

<p>Is there any way to do this in Python? Thx~</p>

<p>P.s. The indexed points are stored in a 10000*10 matrix, where each row represents a feature vector.</p>
"
163,R randomForest's rfcv method,"<p>I would like to use rfcv to cull the unimportant variables from a data set before creating a final random forest with more trees (please correct and inform me if that's not the way to use this function).  For example,</p>

<pre><code>&gt;     data(fgl, package=""MASS"")
&gt;     tst &lt;- rfcv(trainx = fgl[,-10], trainy = fgl[,10], scale = ""log"", step=0.7)
&gt;     tst$error.cv
        9         6         4         3         2         1 
0.2289720 0.2149533 0.2523364 0.2570093 0.3411215 0.5093458
</code></pre>

<p>In this case, if I understand the result correctly, it seems that we can remove three variables without negative side effects.  However,</p>

<pre><code>&gt;     attributes(tst)
$names
[1] ""n.var""     ""error.cv""  ""predicted""
</code></pre>

<p>None of these slots tells me what those first three variables that can be harmlessly removed from the dataset actually were.</p>
"
164,How do I make R use HTML help on Debian?,"<p>I am use R 2.14.0 on Debian Squeeze. I am running R in the terminal. Currently when I type 
 <code>?myfunction</code> , the help page is displayed in the terminal. I would like for the help page to be displayed in my web browser instead, just like on Windows (or Fedora). How do I do this?
I have tried typing <code>help.start()</code> but it did not do anything.</p>

<p>UPDATE: I got it working once, then it stopped working. I added the following lines to <code>$R_HOME/etc/.Rprofile</code></p>

<pre><code>options(""help_type""=""html"")

options(""browser""=""chromium-browser"")
</code></pre>

<p>It worked once. Then stopped. </p>

<p><code>help.start()</code> now launches the browser. However, <code>?someFunction</code> only launched the browser once and has now reverted to the in-terminal help.</p>

<p>Thanks.</p>
"
165,Special characters in R language,"<p>I have a table, which looks like this:</p>

<pre><code>1β              2β     
1.0199e-01        2.2545e-01       
2.5303e-01        6.5301e-01
1.2151e+00        1.1490e+00
</code></pre>

<p>and so on...</p>

<p>I want to make a boxplot of this data. The commands I am using is this:</p>

<pre><code>pdf('rtest.pdf')
 w1&lt;-read.table(""data_CMR"",header=T)
 w2&lt;-read.table(""data_C"",header=T)
boxplot(w1[,], w2[,], w3[,],outline=FALSE,names=c(colnames(w1),colnames(w2),colnames(w3)))
dev.off()
</code></pre>

<p>The problem is instead of symbol beta (β), I get two dots (..) in the output.</p>

<p>Any suggestions, to solve this problem.</p>

<p>Thank you in advance.</p>
"
166,Understanding the probabilistic interpretation of logistic regression,"<p>I am having problem developing intuition about the probabilistic interpretation of logistic regression. Specifically, why is it valid to consider the output of logistic regression function as a probability?</p>

<p>Thanks!</p>
"
167,R Iterating over indexes of a multi-dimentional array,"<p>I have a multi-dimensional array, and the number of its dimensions are unknown until the runtime, for example:</p>

<pre><code>dims &lt;- rep(3, dim_count)
arr &lt;- array(0, dims)
</code></pre>

<p>Now, having the <code>dims</code> vector I would like to iterate by all the indexes of the array, for example, having</p>

<pre><code>dims &lt;- c(2,3)
</code></pre>

<p>I would like to be able to get a series of vectors:</p>

<pre><code>c(1,1)
c(1,2)
c(1,3)
c(2,1)
c(2,2)
c(2,3)
</code></pre>

<p>Or just a function generating the next one from the previous one. </p>

<p>The only three ways I could think of were:</p>

<ol>
<li><p>iterate over <code>1:length(arr)</code> and translate those numbers to the
index vectors - in the above example I would be looking for a (preferably built-in) function doing <code>indexesOf(arr, 4) -&gt; c(2,1)</code>.</p></li>
<li><p>Get the last generated index vector, increment the element on its last position, and make sure it is in the bounds given by <code>dims</code>.</p></li>
<li><p>Generate a matrix, the columns of which would contain all the indexes I need. </p></li>
</ol>

<p>But, sadly, neither of the first two approaches is fast or elegant. The third one looks like a decent idea, I can do it on paper, but i can't code it in R using <code>rbind</code> and so on.</p>

<p>Is there a good way of doing this, preferably without nested loops?</p>

<p>Just for comparison, my ugly looped implementation of #3:</p>

<pre><code>getAllIndexes = function(dims) {
  dimCount &lt;- length(dims)
  ret &lt;- array(1:dims[1], c(1,dims[1]))
  for(i in 2:length(dims)){
    curdims &lt;- dims[i]

    a &lt;- array(rep(ret, curdims), c(nrow(ret), curdims * ncol(ret) ))
    b &lt;- rep(1:curdims, each=ncol(ret))
    ret &lt;- rbind(a, b, deparse.level=0)
  }
  ret
}
</code></pre>
"
168,Fourier Series from Discrete Fourier Transform,"<p>I'm trying to recreate a function from a discrete fourier transform. In Matlab it would be done like this:</p>

<pre><code>function [y] = Fourier(dft,x)
n = length(dft);
y = cos(pi*(x+1)'*(0:n-1))*real(dft)+sin(pi*(x+1)'*(0:n-1))*imag(dft)
end
</code></pre>

<p>My attempt in Python is falling flat because I don't know how to add up all the coefficients correctly</p>

<pre><code>def reconstruct(dft, x):
n = len(dft)
y = ([(coeff.real)*np.cos(np.pi*x*nn) + (coeff.imag)*np.cos(np.pi*x*nn) for coeff in dft for nn in range(0,n)])
</code></pre>

<p>But this isn't correct because I need to sum over n and add those sums together. Where am I off?</p>

<p>The equation I am trying to recreate is below:</p>

<p><img src=""http://i.stack.imgur.com/CopGv.png"" alt=""Fourier Series""></p>
"
169,How to deal with data matching specific criteria in a large dataset in R,"<p>I have a large data set in R (1.2M records). Those are some readings for different protocols. Now, I would like to classify this data (which I can do with rpart/RWeka). However, I first need to process the data, and this question is exactly about that.</p>

<p>The data set consists of a pair of outputs (throughput,response time) per set of control parameters, for 4 different protocols. Now, I would like to ""bin"" these values, and for each set of control parameters choose only those protocols which are in 10% of the maximum throughput (for that set of input params), and in 10% of minimim response time. </p>

<p>I know I can use aggregate to find max throughput, min response time in another data.frame, and then join it with original data.frame. Then, I can use ifelse to find those protocol names matching criteria. However, that seems to me as inefficient, and I don't know how would I encode multiple matches (per set of input values) in a single column.</p>

<p>Any suggestions?</p>

<p>Example (REQS and REPS are input parameters):</p>

<pre><code>PROTO  REQS  REPS  THR  RT
A      8     8     10   1
B      8     8     9.5  2
C      8     8     7    1.1
A      16    8     10   4
B      16    8     5    1
C      16    8     1    0.5
A      8     16    8    1
B      8     16    10   1.09
C      8     16    9.5  1
</code></pre>

<p>Should produce something like:</p>

<pre><code>REQS REPS THRGOOD RTGOOD BOTHGOOD
8    8    A,B     A,C    A
16   8    A       C      empty
8    16   B,C     A,B,C  B,C
</code></pre>
"
170,Postgres math expression calculcated for each row in table,"<p>Using PostgreSQL, supposing a table like the following:</p>

<pre><code> 12184 |               4 |          83
 12183 |               3 |         171
 12176 |               6 |          95
</code></pre>

<p>How can I compute a math expression for each row in the table? </p>

<p>For example, to divide column 2 by column 3, such that the output would be:</p>

<pre><code> 12184 |   0.04819277108
 12183 |   0.01754385965
 12176 |   0.06315789474
</code></pre>

<p>My instinct was to try:</p>

<pre><code>SELECT col1, col2 / col3 FROM table_name;
</code></pre>

<p>But that return the ceiling (ie. rounded-down) integer part, I need the floating point value.</p>
"
171,Getting column values based on defined order in R,"<p>I have a matrix with several columns and I want to get
on special column extracted (as vector), but ordered according to
another column in that matrix.</p>

<p>So far I am doing it with a for-loop approach which I find not
very pretty. I guess that there is a much better way to do such things.</p>

<p>Here a working example:</p>

<pre><code># creating a example matrix with two columns
df &lt;- data.frame(colA=c(""A"",""B"",""C"",""D"",""E""),colB=(1:5))
mat &lt;- as.matrix(df)

# my desired order
my_order &lt;- c(""C"",""D"",""A"",""B"")

# For loop and collecting results in a vector
out &lt;- NULL
for(i in my_order) out &lt;- c(out,mat[mat[,1]==i,2])

print(out)
</code></pre>

<p>Any suggestions, examples of a more elegant and computationally faster way?</p>
"
172,Model measurement and error in NumPy,"<p>I'd like to try the SciPy suite instead of Octave for doing the statistics in my lab experiments. Most of my questions were answered <a href=""http://stackoverflow.com/q/12343271/653152"">here</a>, there is just another thing left:</p>

<p>I usually have an error attached to the measurements, in Octave I just did the following:</p>

<pre><code>R.val = 10;
R.err = 0.1;

U.val = 4;
U.err = 0.1;
</code></pre>

<p>And then I would calculate <code>I</code> with it like so:</p>

<pre><code>I.val = U.val / R.val;
I.err = sqrt(
    (1 / R.val * U.err)^2
    + (U.val / R.val^2 * R.err)^2
);
</code></pre>

<p>When I had a bunch of measurements, I usually used a structure array, like this:</p>

<pre><code>R(0).val = 1;
R(0).err = 0.1;
…
R(15).val = 100;
R(15).err = 9;
</code></pre>

<p>Then I could do <code>R(0).val</code> or directly access all of them using <code>R.val</code> and I had a column vector with all the values, for <code>mean(R.val)</code> for instance.</p>

<p>How could I represent this using SciPy/NumPy/Python?</p>
"
173,Java: How do I perform integer division that rounds towards -Infinity rather than 0?,"<p>(<strong>note</strong>: not the same as <a href=""http://stackoverflow.com/questions/1783519/how-to-round-down-integers-in-java"">this other question</a> since the OP never explicitly specified rounding towards 0 or -Infinity)</p>

<p><a href=""http://docs.oracle.com/javase/specs/jls/se7/html/jls-15.html#jls-15.17.2"" rel=""nofollow"">JLS 15.17.2</a> says that integer division rounds towards zero. If I want <code>floor()</code>-like behavior for positive divisors (I don't care about the behavior for negative divisors), what's the simplest way to achieve this that is numerically correct for all inputs?</p>

<pre><code>int ifloor(int n, int d)
{
    /* returns q such that n = d*q + r where 0 &lt;= r &lt; d
     * for all integer n, d where d &gt; 0
     *
     * d = 0 should have the same behavior as `n/d`
     *
     * nice-to-have behaviors for d &lt; 0:
     *   option (a). same as above: 
     *     returns q such that n = d*q + r where 0 &lt;= r &lt; -d
     *   option (b). rounds towards +infinity:
     *     returns q such that n = d*q + r where d &lt; r &lt;= 0
     */
}

long lfloor(long n, long d)
{
    /* same behavior as ifloor, except for long integers */
}
</code></pre>

<p>(update: I want to have a solution both for <code>int</code> and <code>long</code> arithmetic.)</p>
"
174,ggplot2-line plotting with TIME series and multi-spline,"<p>This question's theme is simple but drives me crazy: 
1. how to use <code>melt()</code>
2. how to deal with multi-lines in single one image?</p>

<p>Here is my raw data:</p>

<pre><code>a   4.17125 41.33875    29.674375   8.551875    5.5
b   4.101875    29.49875    50.191875   13.780625   4.90375
c   3.1575  29.621875   78.411875   25.174375   7.8012
</code></pre>

<p>Q1:
I've learn from this post <a href=""http://stackoverflow.com/questions/3777174/plotting-two-variables-as-lines-using-ggplot2"">Plotting two variables as lines using ggplot2</a> to know how to draw the multi-lines for multi-variables, just like this: 
<img src=""http://i.stack.imgur.com/NNzFB.png"" alt=""enter image description here""></p>

<p>The following codes can get the above plot. However, the x-axis is indeed time-series.</p>

<pre><code>df &lt;- read.delim(""~/Desktop/df.b"", header=F)
colnames(df)&lt;-c(""sample"",0,15,30,60,120)
df2&lt;-melt(df,id=""sample"")
ggplot(data = df2, aes(x=variable, y= value, group = sample, colour=sample)) + geom_line() + geom_point()
</code></pre>

<p>I wish it could treat <strong>0 15 30 60 120 as real number</strong> to show the time series, rather than name_characteristics. Even having tried this, I failed. </p>

<pre><code>row.names(df)&lt;-df$sample
df&lt;-df[,-1]
df&lt;-as.matrix(df)
df2 &lt;- data.frame(sample = factor(rep(row.names(df),each=5)), Time = factor(rep(c(0,15,30,60,120),3)),Values = c(df[1,],df[2,],df[3,]))
ggplot(data = df2, aes(x=Time, y= Values, group = sample, colour=sample)) 
        + geom_line() 
        + geom_point()
</code></pre>

<p>Loooooooooking forward to your help. </p>

<p>Q2:
I've learnt that the following script can add the spline() function for single one line, what about I wish to apply spline() for <strong>all the three lines</strong> in single one image? </p>

<pre><code>n &lt;-10
d &lt;- data.frame(x =1:n, y = rnorm(n))
ggplot(d,aes(x,y))+ geom_point()+geom_line(data=data.frame(spline(d, n=n*10)))
</code></pre>
"
175,Recommended Books on Math for Game Development,"<p>It's been a few years since my computational geometry class in college, and I unfortunately sold my textbook (<a href=""http://rads.stackoverflow.com/amzn/click/0521649765"" rel=""nofollow"">Computational Geometry in C</a>), so now that I am writing a 2D video game, I am looking for books to give me a hand.   What books are recommended that cover the geometry, math, and physics of 2D video game development?</p>
"
176,"UMVUE $a^{-\theta}$ in $Beta(\theta+1,1)$","<p>for finding UMVUE $a^{-\theta}$ in $Beta(\theta+1,1)$, how can calculate $a^{-1}\mathbb P_\theta(X_1\lt a\mid U)$.$(0&lt;a&lt;1)$ please explain more.
thanks</p>
"
177,Parallelizing a for-loop depending on previous outcomes,"<p>I'm just taking my first steps with the package <code>parallel</code> and the <code>foreach()</code> function. therefore this question will be rather stupid. so here's my task i want to fullfil:</p>

<ol>
<li>apply function to an object <code>t</code></li>
<li>use the outcome in the next iteration </li>
</ol>

<p>a simple example would be:</p>

<pre><code>newFunc&lt;-function(){
    test[i+1] &lt;&lt;- sqrt(test[i])
}

test &lt;- c(1,rep(NA, 10))

foreach(i=1:11, .combine='rbind', .export='test')%do% newFunc()
</code></pre>

<p>this yields me a vector of ones as a for-loop would of course as well. however, if i try parallelise this this yields a different outcome:</p>

<pre><code>test &lt;- c(1,rep(NA, 10))

library(doParallel)
library(foreach)
cl &lt;- makeCluster(4)
registerDoParallel(cl)

foreach(i=1:11, .combine='rbind', .export='test')%dopar% newFunc()

stopCluster(cl)
</code></pre>

<p>this leaves me with the output <code>c(1, NA, NA, NA, NA, ..., NA)</code>. I <em>guess</em> this is because the slaves don't know the  result of other functions? I hope I supplied the necessary information. My actual function is of course more complex, yet this example seemed the easiest way do demonstrate my problem.</p>

<p><strong>edit:</strong> I guess the first question is: can such a problem be parallelised at all?</p>
"
178,A way to always dodge a histogram?,"<p>Using ggplot2 I'm creating a histogram with a factor on the horizontal axis and another factor for the fill color, using a dodged position. My problem is that the fill factor sometimes takes only one value for a value of the horizontal factor, and with nothing to dodge the bar takes up the full width. Is there a way to make it dodge nothing so that all bar widths are the same? Or equivalently to plot the 0's?</p>

<p>For example</p>

<pre><code>ggplot(data = mtcars, aes(x = factor(carb), fill = factor(gear))) +
geom_histogram(position = ""dodge"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/ezqc7.png"" alt=""enter image description here""></p>

<p><a href=""http://stackoverflow.com/a/9101710/903061"">This answer</a> has a couple ideas. It was also asked before the new version was released, so maybe something changed? Using facets (also shown <a href=""http://stackoverflow.com/a/7104847/903061"">here</a>) I don't like for my situation, though I suppose editing the data and using <code>geom_bar</code> could work, but it feels inelegant. Moreover, when I tried facetting anyway</p>

<pre><code>ggplot(mtcars, aes(x = factor(carb), fill = factor(gear))) +
    geom_bar() + facet_grid(~factor(carb))
</code></pre>

<p>I get the error ""Error in layout_base(data, cols, drop = drop): 
  At least one layer must contain all variables used for facetting""</p>

<p>I suppose I could generate a data frame of counts and then use <code>geom_bar</code>,</p>

<pre><code>mtcounts &lt;- ddply(subset(mtcars, select = c(""carb"", ""gear"")),
    .fun = count, .variables = c(""carb"", ""gear""))
</code></pre>

<p>filling out the levels that aren't present with 0's. Does anyone know if that would work or if there's a better way?</p>
"
179,Efficient distance calculation between N points and a reference in numpy/scipy,"<p>I just started using scipy/numpy. I have an 100000*3 array, each row is a coordinate, and a 1*3 center point. I want to calculate the distance for each row in the array to the center and store them in another array. What is the most efficient way to do it?</p>
"
180,ROC Curves Turorial,"<p>Guys I am looking for a third party software that applies classifiers (like SVM, multilayer perceptrons, I head libsvm is quite good) and also more important any tutorial about ROC Curves as I want to learn how to do them. The ones I found In google are not really helpful so if you know anything else that would be nice. Could somebody provide me something similar?</p>
"
181,How would I go about measuring the impact an article has on the internet?,"<p>For an application of mine, I analyze the sentiment of articles, using NLTK, to display sentiment trends. But right now all articles weigh the same amount. This does not show a very accurate picture because some articles have a higher impact on the internet than others. For example, a blog post from some unknown blog should not weigh the same amount as an article from the New York Times. </p>

<p>How can I determine their impact?</p>
"
182,How to walk up a tree in R?,"<p>I'm teaching myself R, well, more like playing around in it; the <a href=""http://www.oga-lab.net/RGM2/func.php?rd_id=party%3aBinaryTree-class"" rel=""nofollow"">Binary Tree</a> package to be specific. After I got the example on that page working, I thought I'd try to walk UP the tree from the terminal nodes to the parent. This particular package doesn't seem to have a getParent() function/method (that I can discern) that will do what I want.</p>

<p>So I thought I should convert the tree to some other object: a graph, an array of text strings, whatever, but I don't seem to have good enough google-fu to find what I'm looking for. I'm still trying to find my way around the documentation as well.</p>

<p>So how would how would someone who knows what (s)he's doing walk up the tree in the example given on the web page?</p>
"
183,R suppress startupMessages from dependency,"<p>One of my R package's dependencies displays startup messages when loaded.  I would like to suppress these startup messages.</p>

<p>The only fix I found so far was removing the offending package from the Depends: line in my DESCRIPTION file.  Then calling <code>suppressPackageStartupMessages(require(""offendingPackage""))</code>    in <code>.onLoad</code> of my package.</p>

<p>I would rather keep the offending package as part of my Depends, but it seems that anything specified in depends is automatically loaded and therefore can't be supressed.</p>

<p>Thanks,
  Nick</p>
"
184,sum over values in python dict except one,"<p>Is there a way to sum over all values in a python dict except one by using a selector in </p>

<pre><code>&gt;&gt;&gt; x = dict(a=1, b=2, c=3)
&gt;&gt;&gt; np.sum(x.values())
6
</code></pre>

<p>?
My current solution is a loop based one:</p>

<pre><code>&gt;&gt;&gt; x = dict(a=1, b=2, c=3)
&gt;&gt;&gt; y = 0
&gt;&gt;&gt; for i in x:
...     if 'a' != i:
...             y += x[i]
... 
&gt;&gt;&gt; y
5
</code></pre>

<p>EDIT: </p>

<pre><code>import numpy as np
from scipy.sparse import *
x = dict(a=csr_matrix(np.array([1,0,0,0,0,0,0,0,0]).reshape(3,3)),      b=csr_matrix(np.array([0,0,0,0,0,0,0,0,1]).reshape(3,3)), c=csr_matrix(np.array([0,0,0,0,0,0,0,0,1]).reshape(3,3)))
y = csr_matrix((3,3))
for i in x: 
    if 'a' != i:
        y = y + x[i]
print y
</code></pre>

<p>returns <code>(2, 2)   2.0</code></p>

<p>and </p>

<pre><code>print np.sum(value for key, value in x.iteritems() if key != 'a')
</code></pre>

<p>raises</p>

<pre><code>File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-    packages/numpy/core/fromnumeric.py"", line 1446, in sum
    res = _sum_(a)
File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/sparse/compressed.py"", line 187, in __radd__
    return self.__add__(other)
File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/scipy/sparse/compressed.py"", line 173, in __add__
    raise NotImplementedError('adding a scalar to a CSC or CSR '
NotImplementedError: adding a scalar to a CSC or CSR matrix is not supported
</code></pre>
"
185,Finding full QR decomposition from reduced QR,"<p>What's the best way to find additional orthonormal columns of Q? I have computed the reduced QR decomposition already, but need the full QR decomposition.</p>

<p>I assume there is a standard approach to this, but I've been having trouble finding it.</p>

<p>You might wonder why I need the full Q matrix. I'm using it to apply a constraint matrix for ""natural"" splines to a truncated power series basis expansion. I'm doing this in Java, but am looking for a language-independent answer.</p>
"
186,How to detect the right encoding for read.csv?,"<p>I have this file (http://b7hq6v.alterupload.com/en/) that I want to read in R with <code>read.csv</code>. But I am not able to detect the correct encoding. It seems to be a kind of UTF-8. I am using R 2.12.1 on an WindowsXP Machine. 
Any Help?</p>
"
187,C#: Conversion (Recognition) a MATH formula from a TextBox,"<p>I have a problem. In my application I want to use a function ( let say <code>f(x)</code>, where <code>x</code> is the variable ) which is typed in a textbox, for example <code>sin(x) + x*x - 5</code>.</p>

<p>What I want is C# to recognize this function as <code>Math.Sin(x) + x*x - 5</code>, in other words to know that <code>x</code> is the variable.</p>

<p>I have for x some 2000 datas, let say <code>xi</code>, where <code>i = 2000</code>, so all I want is <code>f(xi)</code> values, for all the <code>x</code> datas, where <code>f</code> is the function from the textbox.</p>

<p>I WOULD APPRECIATE A LOT IF ANY COULD HELP ME</p>
"
188,What is most efficient way to plot a domain of convergence?,"<p>Say, you have a Newton Method algorithms with 2 parameters of interest(a,b).
And I would like to plot their domain of convergence with x-axis = a, y-axis = b. Is there a really fast and simple to do this??? Any suggestions?</p>

<p>My algorithm will basically converge for some values of a &amp; b. If I input (a,b), it will return (the number of iterations , value of a that it converge to, value of b that it converge to). Right now, I am thinking of setting up a  for loop within another for loop, which run through all possible values of b first holding a fixed, and all possible values that a will converge holding b fixed. </p>

<p>However, my trouble is: how to identify whether a &amp; b is converging or not. And is there a better way than using nested for loops????</p>
"
189,What's the best C# recommendation engine or framework?,"<p>Is there anyway to use the examples for the ""My Media"" Microsoft research project? My Media is a ""dynamic personalization and recommendation software framework toolkit"" ( <a href=""http://www.mymediaproject.org"" rel=""nofollow"">http://www.mymediaproject.org</a> ), but out of the box it doesn't provide a sample database (only a LINQ-to-SQL .dbml schema), I don't believe it will be easy to re-create by hand. </p>

<p>I was hoping to understand recommendation engines and machine learning with this C#/.Net as a testbed, but without a simple quick start or db it seems impractical. Any suggestions?</p>

<p>(I guess it's time to switch to Java with Apache's Mahout, Weka or something similar?)</p>
"
190,How do you reorder the categorical values in a bar graph? [R],"<p>Specifically, it is possible to reproduce the problem in 2 steps.</p>

<p>(1) load <a href=""http://www.liaad.up.pt/~ltorgo/DataMiningWithR/DataSets/Analysis.txt"" rel=""nofollow"">this txt file</a> into R <a href=""http://www.liaad.up.pt/~ltorgo/DataMiningWithR/code2.html"" rel=""nofollow"">with</a> </p>

<pre><code>algae &lt;- read.table('http://www.liaad.up.pt/~ltorgo/DataMiningWithR/DataSets/Analysis.txt',
         header=F,
         dec='.',
         col.names=c('season','size','speed','mxPH','mnO2','Cl','NO3','NH4','oPO4','PO4','Chla','a1','a2','a3','a4','a5','a6','a7'),
         na.strings=c('XXXXXXX'))
</code></pre>

<p>(2) plot with <code>plot(algae$speed)</code>.</p>

<p>The categorical values will be ordered as ""high"", ""low"", ""medium"". However, I'd like them to be ""high"", ""medium"", ""low"". Is this possible?</p>
"
191,Can you easily plot rugs/axes on the top/right in ggplot2?,"<p>The following example has no inherent meaning... it's just meant to demonstrate particular placement of labels, rugs, etc. and is representative of [edited] <i>(a) a significantly larger project I'm working on that I can't discuss in detail, (b) which requires the use of ggplot, and (c) needs visual features of graphics similar to those reflected in the plot given, below.</i></p>

<p>Is it possible to recreate the following using ggplot2 either directly or with some fiddling with grid?</p>

<pre><code>x &lt;- rnorm(20)
y &lt;- rnorm(20)

plot(x, y, axes=F, xlab="""", ylab="""")

axis(side = 1, at = round(mean(x), 2))
axis(side = 2, at = round(mean(y), 2))

axis(side = 3, at = round( range(x), 2 ))
axis(side = 4, at = round( range(y), 2 ))

rug(x, side=3)
rug(y, side=4)
</code></pre>

<hr>

<h1>Please see the solutions (Chase's, modified, and one based on Hadley's Geom code) posted below</h1>
"
192,R knn large dataset,"<p>I'm trying to use knn in R (used several packages(<code>knnflex</code>, <code>class</code>)) to predict the probability of default based on 8 variables.  The dataset is about 100k lines of 8 columns, but my machine seems to be having difficulty with a sample of 10k lines.  Any suggestions for doing knn on a dataset > 50 lines (ie <code>iris</code>)?</p>

<p>EDIT:</p>

<p>To clarify there are a couple issues.</p>

<p>1) The examples in the <code>class</code> and <code>knnflex</code> packages are a bit unclear and I was curious if there was some implementation similar to the randomForest package where you give it the variable you want to predict and the data you want to use to train the model:</p>

<pre><code>RF &lt;- randomForest(x, y, ntree, type,...) 
</code></pre>

<p>then turn around and use the model to predict data using the test data set:</p>

<pre><code>pred &lt;- predict(RF, testData)
</code></pre>

<p>2) I'm not really understanding why <code>knn</code> wants training AND test data for building the model.  From what I can tell, the package creates a matrix ~ to <code>nrows(trainingData)^2</code> which also seems to be an upper limit on the size of the predicted data.  I created a model using 5000 rows (above that # I got memory allocation errors) and was unable to predict test sets > 5000 rows.  Thus I would need either:</p>

<p>a) find a way to use > 5000 lines in a training set </p>

<p>or</p>

<p>b) find a way to use the model on the full 100k lines.</p>
"
193,How do you unify probability (Naive Bayes)?,"<p>In the blog found here:</p>

<p><a href=""http://alexdavies.net/2011/10/word-lists-for-sentiment-analysis-of-twitter/"" rel=""nofollow"">http://alexdavies.net/2011/10/word-lists-for-sentiment-analysis-of-twitter/</a></p>

<p>the author discusses sentiment analysis with Naive Bayes. He uses Naive Bayes to predict if a tweet is happy or sad, but instead of the prediction being a binary value he produces the probability of happiness. He combines the probability of happy and the probability of sad into one probability. </p>

<p>His formula is such:</p>

<p>w = set of words</p>

<p>s = set of categories (only happy and sad)</p>

<p>$$p(s \mid \bar{w}) = \left( \sum_{s'} \exp \left( \sum_{w \in \bar{w}} \log p(w, s') - \log p(w,s) - \log p(s') + \log p(s)\right) \right)^{-1}$$</p>

<p>$$p(\mathrm{happy} \mid \bar{w}) = \left( \exp \left( \sum_{w \in \bar{w}} \log p(w, \mathrm{sad}) - \log p(w, \mathrm{happy})\right) + 1\right)^{-1}$$</p>

<p>Why does the math transform the two log probabilities to p(happy | w)? Doesn't subtracting the logs give you the ratio of sad to happy?</p>
"
194,In what situations would I specify operation as unchecked?,"<p>For example:</p>

<pre><code>int value = Int32.MaxValue;

unchecked
{
    value += 1;
}
</code></pre>

<p>In what ways would this be useful? can you think of any?</p>
"
195,R 2.14 byte compile - not possible with install.packages?,"<p>It seems that R 2.14 can byte-compile packages using a switch on <code>R CMD INSTALL</code> but I couldn't find such an option in the install.packages documentation.  Am I missing something?</p>
"
196,convolution in R,"<p>I tried to do convolution in R directly and using FFTs then taking inverse. But it seems from simple observation it is not correct. Look at this example:</p>

<pre><code># DIRECTLY
&gt; x2$xt
[1] 24.610 24.605 24.610 24.605 24.610
&gt; h2$xt
[1] 0.003891051 0.003875910 0.003860829 0.003845806 0.003830842
&gt; convolve(h2$xt,x2$xt)
[1] 0.4750436 0.4750438 0.4750435 0.4750437 0.4750435

# USING INVERSE FOURIER TRANSFORM
&gt; f=fft(fft(h2$xt)*fft(x2$xt), inv=TRUE)
&gt; Re(f)/length(f) 
[1] 0.4750438 0.4750435 0.4750437 0.4750435 0.4750436
&gt;
</code></pre>

<p>Lets take the index 0. At 0, the convolution should simply be the last value of x2$xt (24.610) multiplied by first value of h2$xt (0.003891051) which should give convolution at index 0 = 24.610*0.003891051 = 0.09575877 which is way off from 0.4750436.</p>

<p>Am I doing something wrong? Why is the values so different from expected?</p>
"
197,Fastest method to define whether a number is a triangular number,"<p>A triangular number is the sum of the n natural numbers from 1 to n. What is the fastest method to find whether a given positive integer number is a triangular one? </p>


"
198,Print from specific positions in NumPy array,"<p>I am new to NumPy and I have created the following array:</p>

<pre><code>import numpy as np

a = np.array([[1,2,3],[4,5,6],[7,8,9]])
</code></pre>

<p>and I am wondering if there is a way to print a number from a specific position in the array.</p>

<p>Let's say I wanted to print number 7, and ONLY number 7. Would that be possible?</p>
"
199,Implementing Root-Calculation Function,"<p>Implementing math functions for various things is simple enough. <code>int mul(int,int);</code>, <code>int  pow(int,int);</code>, even <code>double div(float,float);</code> are easy to do and can be implemented with loops or recursion. (These are the same methods used to perform these functions by hand or in the head.) To multiply, just repeatedly add the number. To divide, repeatedly subtract it. To get the power, repeatedly multiply. And so on.</p>

<p>One mathematical function that I’ve always wondered about however is roots. For example, how would you write a function to calculate the square (or cube, etc.) root of a number (ie, <code>double root(float num, float root);</code>)? I tried looking around and could not find an algorithm or method of doing this.</p>

<p>When I try to calculate a root by hand, I usually use the guess method (start with an approximate number, add a fraction, multiply, see how far off it is, add a smaller fraction, multiply, check again, and repeat until satisfied). I suppose that could work, but surely there is a better—and faster—method (regardless of how much faster a computer can do it than by hand).</p>

<p>Obviously LUTs are not relevant since it would have to be generic enough to take any operands (unless you are writing a game with a finite set of data). The <a href=""http://en.wikipedia.org/wiki/Methods_of_computing_square_roots"" rel=""nofollow"">Wikipedia article</a> mentions the guess method and lists some ancient ones (from long before computers were invented) as well as some pure math and even calculus methods (including some that have “infinity” as a component). The only ones that seem to have anything to do with electronics use tricks or logirithms. (And that’s just for <em>square-roots</em>, let alone cube-roots, and such.)</p>

<p>Is there no easy root calculation method? How do calculators do it? How do computers do it? (No, simply doing <code>double pow(a,0.5);</code> won’t work because then how would <code>double pow(float,float)</code> be implemented?)</p>

<p>Am I just incorrectly grouping root functions with simpler functions? Are they more complex than they seem?</p>
"
200,Most common regex elements,"<p>I have been thinking about a regex tool for beginners. However, regex is a big language with lots of dialects and little tips'n'tricks. Arguably, many of these are unnecessary for a beginner. </p>

<p>I thought it would be interesting to see whether it is possible to determine a subset of all regex elements that suffice for creating, say, 90% of all regexes. </p>

<p>Now, we have a large body of open source code available online that could help answer this question. The challenge then, is </p>

<ol>
<li>to search through the body of open source code online and to recognize a regex expression in at least most of the major languages used in open source code, and then </li>
<li>parse those regex expressions and catalogue the elements used in order to see which ones are used the most</li>
</ol>

<p>By element here I mean anything that constitutes the regex language. Character classes, (), quantifiers, the whole spectrum of backslash elements, and so on.</p>

<p>The tricky part is #1. I suspect there is an API on say, ohloh or sourceforge or equivalent for this, but haven't really found anything. Any pointers on how to get started? Thanks!</p>

<p>Edit based on comment: The obvious answer would be to create a spider from scratch and then, say, create regular expressions by hand to recognize regular expressions in different languages. To me that also seems a rather obviously foolish solution: it is overly generic and seems like reinventing the wheel. I am looking for a more specialized tool for the job. </p>
"
201,Finding number of digits in integer when being passed an int = 0,"<p>Using Java btw. But Usually people seem to do this sort of thing with. </p>

<pre><code>int numDigits = (int)(log10(num)+1); //can explicitly floor, or casting to int will do that
</code></pre>

<p>but log10(0) = -INF, which means my length is being set to the largest negative integer value. </p>

<p>I suppose I could make a condition</p>

<pre><code>if (numDigits is negative)
    numDigits = 1 //not 0 because I'm counting 0 as 1 digit.
</code></pre>

<p>This is being used to implement a natural number constructor, just so people have context to my problem.</p>
"
202,Weight distribution,"<p>I am currently working with multi-objective problem with 3 objectives and I am using weighted sum approach, where weights on all objectives together should sum up to 1, I am also using 0.1 as a step, for example if I have two objectives my weights would be:</p>

<pre><code> 0.9 0.1
 0.8 0.2
 0.7 0.3
</code></pre>

<p>But currently I'm stuck with 3 objectives and trying to figure out an algorithm that could make a similar distribution of weights across 3 objectives like this:</p>

<pre><code> 1obj       2obj               3obj
 1.0        0.0                0.0
 0.9     (0 ; 0.1)           (0.1 ; 0)
 0.8   (0 ; 0.1 ; 0.2)    (0.2 ; 0.1 ; 0)
</code></pre>

<p>Can you pls suggest an algorithm that will get all possible combinations</p>
"
203,Installing NumPy on Snow Leopard (10.6) with XCode 4 installed,"<p>I can't seem to install NumPy to System Python now that I've upgraded to Xcode 4.</p>

<p>When I try to do:</p>

<pre><code>easy_install -U numpy
</code></pre>

<p>I get:</p>

<pre><code>File ""/private/tmp/easy_install-xoqh9U/numpy-1.6.1/numpy/core/setup.py"", line 696, in get_mathlib_info
RuntimeError: Broken toolchain: cannot link a simple C program
</code></pre>

<p>I'd like to use System Python (I have many other modules compiled/installed, and don't want to use EPD for this purpose).</p>
"
204,dynamic mean: measurement of randomly distributed events,"<p>Aim is to estimate an error on a stochastic event rate. I read out the event counter second-wise, every black $1$ is a counted event (new events over time, see the plot below).</p>

<p>During the measurement I am estimating the event rate, so as more statistics is accumulated, mean event rate (red) should asymptotically become more accurate. </p>

<p><img src=""http://i.stack.imgur.com/lPFLq.png"" alt=""dyn_mean1""> </p>

<p>as one can see, the mean value oscillates around true value of 0.5</p>

<p><img src=""http://i.stack.imgur.com/0VJKY.png"" alt=""dyn_mean2""></p>

<p>even after one order of magnitude more events collected.</p>

<p>Practical question: How can one calculate the number of events needed to estimate the mean value to a maximum error ($0.5\pm \sigma$)? - error should fall by $\sqrt{N}$ </p>

<p>Theoretically: Can this oscillation be described analytically? Can you suggest further reading?</p>

<p>The events are radiation counts, so they are uncorrelated, may by Poisson-distribution applied?</p>

<p><strong>addendum:</strong>
idealized first approximation - every 10th event is non-zero:
<img src=""http://i.stack.imgur.com/iRD6Y.png"" alt=""dyn_mean_reg1""> </p>

<p>may be this curve is superimposed with the realer-life example above, is here any techniques of partitioning in arbitrary functions applicable? </p>
"
205,Relational Algebra and R,"<p>I'm new to R and I was wondering if you can do easily (or with a package) relational Algebra (or SQL like function) ? I might not been using the right tool, what I want is mainly analyze date and generate graphs from different 'group by ' perspective.</p>
"
206,"Odds of guessing suit from a deck of cards, with perfect memory","<p>While teaching my daughter why drawing to an inside straight is almost always a bad idea, we stumbled upon what I think is a far more difficult problem:</p>

<p>You have a standard 52-card deck with 4 suits and I ask you to guess the suit of the top card. The odds of guessing the correct suit are obviously 1 in 4. You then guess again, but the first card <em>is not returned</em> to the deck. You guess a suit other than the first drawn and the odds are 13/51, somewhat better than 1 in 4.</p>

<p>Continuing through the deck your odds continually change (never worse than 1 in 4, definitely 100% for the last card) what are your overall odds for any given draw over the course of 52 picks?</p>

<p>Can this be calculated? Or do you need to devise a strategy and write a computer program to determine the answer? Do these type of problems have a name?</p>

<p>Dad and to a much less extent daughter, await your thoughts!</p>
"
207,Math Calculation to retrieve angle between two points?,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stackoverflow.com/questions/7586063/how-to-calculate-the-angle-between-two-points-relative-to-the-horizontal-axis"">How to calculate the angle between two points relative to the horizontal axis?</a>  </p>
</blockquote>



<p>I've been looking for this for ages and it's just really annoying me so I've decided to just ask...</p>

<p>Provided I have two points (namely x1, y1, and x2, y2), I would like to calculate the angle between these two points, presuming that when y1 == y2 and x1 > x2 the angle is 180 degrees...</p>

<p>I have the below code that I have been working with (using knowledge from high school) and I just can't seem to produce the desired result.</p>

<pre><code>float xDiff = x1 - x2;
float yDiff = y1 - y2;
return (float)Math.Atan2(yDiff, xDiff) * (float)(180 / Math.PI);
</code></pre>

<p>Thanks in advance, I'm getting so frustrated...</p>
"
208,For Loop alternatives for progressive operations,"<p>I have to apply regression function progressively to a time series data (vector ""time"" and ""tm"" and I'm using a For Loop as follow:</p>

<pre><code>top&lt;-length(time)
for(k in 2:top){
    lin.regr&lt;-lm(tm[1:k] ~ log(time[1:k]))
    slope[k]&lt;-coef(lin.regr)[2]
}
</code></pre>

<p>But for vectors' length of about 10k it becomes very slow.
Is there a faster alternative (maybe using apply function)?</p>

<p>In a more easy problem: if I have a vector like x&lt;-c(1:10) how can I build a y vector containing (for example) the progressive sum of x values? 
Like:</p>

<pre><code>x
1 2 3 4 5 6 7 8 9 10
y
1  3  6 10 15 21 28 36 45 55
</code></pre>
"
209,store and access variables with attributes in a loop in R,"<p>I have a small problem with my code. Here is a sample of my code. When I run the function, I get the following results. 
Also, I am using <code>raply()</code> from <code>plyr</code> package and this function returns output as a list-array. My code                        </p>

<pre><code>EmpPval&lt;-function(dat,numberOfPermutations=100,usePlyr=TRUE)

{

  if(usePlyr)
  {
    require(plyr)
  }

   if(usePlyr)
  {

statistic &lt;- raply(numberOfPermutations,permdat(dat)$statistic,.progress=""tk"")
    browser()
  }

  else
  {    
    statistic &lt;- replicate(expr=permdat(dat)$statistic,n=numberOfPermutations,
                           simplify=TRUE)
  }

 }

&gt;statistic   #this is from one iteration

    [1] 0.0409457

    attr(,""numerator"")

    [1] 0.0007954759

    attr(,""denominator"")

    [1] 0.01942758
</code></pre>

<p>My result has attributes. Now my problem is I am unable to store these values as it is in a variable and I want to access them again like this:</p>

<pre><code>s1&lt;-attr(statistic,""numerator"")

s2&lt;-attr(statistic,""denominator"") 
</code></pre>

<p>The <code>permdat()</code> runs inside a for loop. So I will have 100 such values generated and I want to store all the 100 statistics values with their attributes. What I get now is something like this:</p>

<pre><code>&gt;statistic ##this is after it runs in a loop

[1] 0.028793900 [2] 0.073739396 [3] 0.049136225 [4] 0.058408310 [5] 0.027253176 [6] 0.019471812 [7] 0.071434025 [8] 0.038411458 [9] 0.028921401 [10] 0.021929506..... The attribute values are not stored. 
</code></pre>

<p>Can someone help me on this? Thanks in advance. </p>
"
210,Averages and Team,"<p>I have a question:</p>

<blockquote>
  <p>Suppose $5$ players each score an average of $10$ points per game. Then collectively, do they score on average $50$ points per game?</p>
</blockquote>

<p>So player 1 scores an average of 10 points per game, player 2 scores an average of 10 points per game, etc...</p>

<p>So as a team they score on average of 50 points per game?</p>

<p><strong>Edit.</strong> We want to form a team that averages 50 points per game. </p>
"
211,Randomizations and hierarchical tree,"<p>I am trying to permute (column-wise only) my data matrix a 1000 times and then do hierarchical clustering in ""R"" so I have the final tree on my data after 1000 randomizations. 
This is where I am lost. I have this loop </p>

<pre><code>    for(i in 1:1000) 
    { 
    permuted &lt;- test2_matrix[,sample(ncol(test2_matrix), 12, replace=TRUE)]; (this permutes my columns)
    d = dist(permuted, method = ""euclidean"", diag = FALSE, upper = FALSE, p = 2);
    clust = hclust(d, method = ""complete"", members=NULL);
    } 
    png (filename=""cluster_dendrogram_bootstrap.png"", width=1024, height=1024, pointsize=10) 
    plot(clust)
</code></pre>

<p>I am not sure if the final tree is a product after the 1000 randomizations or just the last tree that it calculated in the loop. Also If I want to display the bootstrap values on the tree how should I go about it?</p>

<p>Many thanks!!</p>
"
212,is it possible to reduce the weight of a best fit line (least squares) given new data points?,"<p>I have a simple best-fit-line algorithm similar to <a href=""http://faculty.cs.niu.edu/~hutchins/csci230/best-fit.htm"" rel=""nofollow"">this description</a>.  </p>

<p>Without memorizing the points history, it is easy to calculate a rolling best fit line as long as we remember (store) the intermediate values used to calculate the BFL:</p>

<pre><code>sumX,sumY           //  the sum
sumX2, sumY2,       //  sum of squares
sumXY, and count    //  sum of (X*Y), and count
</code></pre>

<p>when a new point arrives (Xn,Yn), the line simply update the sums by adding the latest value based on Xn, Yn. then calculate BFL:</p>

<pre><code>XMean = SumX / Count
YMean = SumY / Count
Slope = (SumXY - SumX * YMean) / (SumX2 - SumX * XMean)
YInt = YMean - Slope * XMean
</code></pre>

<p>to get</p>

<pre><code> Y = Slope * X + YInt
</code></pre>

<p>However, as the number of points grow, the new point will have less of an effect on the best-fit-line.</p>

<p>is there a way to mathematically reduce this weight?  for example when the count reaches 20, modify the intermediate variables to represent a line with a weight of 10 points:</p>

<pre><code>when count = 20 -&gt;

    count = count/2
    sumX = sumX/2    // this will result in the same XMean, but the same line?
    sumY = sumY/2    // same YMean here as well
</code></pre>

<p>and the values for sumXY,sumX2,sumY2 are even more perplexing to me.</p>

<p>Specifically, I'm looking for equations for these intermediate values to result to the same line but with less <em>weight</em>.</p>

<p>-TIA</p>
"
213,How to count occurrence of unknown strings in column?,"<p>I have another question.  Thanks for everyone's help and patience with an R newbie!</p>

<p>How can I count how many times a string occurs in a column? Example:</p>

<pre><code>MYdata &lt;- data.frame(fruits = c(""apples"", ""pears"", ""unknown_f"", ""unknown_f"", ""unknown_f""), 
                     veggies = c(""beans"", ""carrots"", ""carrots"", ""unknown_v"", ""unknown_v""), 
                     sales = rnorm(5, 10000, 2500))
</code></pre>

<p>The problem is that my real data set contains several thousand rows and several hundred of the unknown fruits and unknown veggies. I played around with ""table()"" and ""levels"" but without much success.  I guess it's more complicated than that.  Great would be to have an output table listing the name of each unique fruit/veggie and how many times it occurs in its column. Any hint in the right direction would be much appreciated. </p>

<p>Thanks,  </p>

<p>Marcus</p>
"
214,Paste logical conditions in R,"<p>I have now rewriten my problem to make it clearer</p>

<p>I want to replace a condition like this where var is a variable in dataframe (dataframe$var) with a paste or other solution as I do have so many condition values(?) (a, b and c  in my example).</p>

<pre><code>subdataframe&lt;-dataframe[var==""a""|var==""b""|var==""c"",]
</code></pre>

<p>I have tried to make a list(?) of the condtion values.</p>

<pre><code>sample&lt;-c(""a"",""b"",""c"")
</code></pre>

<p>And to then use paste to make the logical condition</p>

<pre><code>subdataframe&lt;-dataframe[paste(""var"",sample,sep=""=="",collapse=""|""),]
</code></pre>

<p>But that doesn't work</p>

<p>Help please =)</p>

<p>Marcus</p>
"
215,RMySQL dbWriteTable to a table with a MySQL reserved word as name,"<p>I am having trouble with the dbWriteTable command from the RMySQL package. I have to append records to a table name which is called 'order', a reserved word in MySQL.</p>

<pre><code>dbWriteTable(connection, ""`order`"", df, append = T)
</code></pre>

<p>Give as error:</p>

<blockquote>
  <p>Warning message:
  In mysqlWriteTable(conn, name, value, ...) :
   could not create table: aborting mysqlWriteTable</p>
</blockquote>

<p>Other queries like SELECT work fine as long as I put order between back ticks.</p>

<p>Any ideas how the execute the dbWriteTable command? And renaming the table is unfortunately no option.  </p>
"
216,How to pass a parameter by variable into data.table[J()],"<p>I'm brand new to the (completely marvelous) <code>data.table</code> package, and seem to have gotten stuck on a very basic, somewhat bizarre problem. I can't post the exact data set I'm working with, for which I apologize -- but I think the problem is simple enough to articulate that hopefully this will still be very clear.</p>

<p>Let's say I have a data.table like so, with key x: </p>

<pre><code>set1
   x y
1: 1 a
2: 1 b
3: 1 c
4: 2 a
</code></pre>

<p>I want to return a subset of <code>set1</code> containing all rows where <code>x == 1</code>. This is wonderfully simple in data.table: <code>set1[J(1)]</code>. Bam. Done. I can also assign <code>z &lt;- 1</code>, and call <code>set1[J(z)]</code>. Again: works great. </p>

<p>...except when I try to scale it up to my actual data set, which contains ~6M rows. When I call <code>set1[J(1674)]</code>, I get back a 78-row return that's exactly what I'm looking for. But I need to be able to look up (literally) 4M of these subsets. When I assign the value I'm searching for to a variable, <code>id &lt;- 1674</code>, and call <code>set1[J(id)]</code>... R nearly takes down my desktop. </p>

<p>Clearly <em>something</em> I don't understand is going on under the data.table hood, but I haven't been able to figure out what. Googling and slogging through Stack Overflow suggest that this should work. Out of pure whimsey, I've tried:</p>

<pre><code>id &lt;- quote(1674)
set1[J(eval(id))]
</code></pre>

<p>...but that is far, far worse. What... what's going on? </p>
"
217,Numpy savez interprets my keys as filenames -> IOError,"<p>I am using <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.savez.html"" rel=""nofollow"">numpy savez</a> as <a href=""http://stackoverflow.com/a/9233087/380038"">recommended</a> to save numpy arrays. As keys I use the names of the files I have loaded the data from. But it seems like <code>savez</code> is trying to use the filenames somehow. What should I do? I would like to avoid stripping the file names of their path and ending.</p>

<pre><code>&gt;&gt;&gt; import numpy
&gt;&gt;&gt; arrs = {'data/a.text': numpy.array([1,2]),
            'data/b.text': numpy.array([3,4]),
            'data/c.text': numpy.array([5,6])}    
&gt;&gt;&gt; numpy.savez('file.npz', **arrs)
Traceback (most recent call last):
  File ""&lt;input&gt;"", line 1, in &lt;module&gt;
  File ""/usr/lib/python2.6/dist-packages/numpy/lib/io.py"", line 305, in savez
    fid = open(filename,'wb')
IOError: [Errno 2] No such file or directory: '/tmp/data/c.text.npy'
</code></pre>
"
218,Changing the sign of a number in PHP?,"<p>Hay Guys, I have quick question.</p>

<p>I have a few floats:</p>

<pre><code>-4.50
+6.25
-8.00
-1.75
</code></pre>

<p>How can I change all these to negative floats so they become:</p>

<pre><code>-4.50
-6.25
-8.00
-1.75
</code></pre>

<p>Also i need a way to do the reverse</p>

<p>If the float is a negative, make it a positive.</p>

<p>Thanks</p>
"
219,Should one use distances (dissimilarities) or similarities in R for clustering?,"<p>I'm doing a cluster problem, and the   <code>proxy</code>   package in R provides both dist and simil functions.</p>

<p>For my purpose I need a distance matrix, so I initially used dist, and here's the code:</p>

<pre><code>distanceMatrix &lt;- dist(dfm[,-1], method='Pearson')
clusters &lt;- hclust(distanceMatrix)  
clusters$labels &lt;- dfm[,1]#colnames(dfm)[-1]
plot(clusters, labels=clusters$labels)
</code></pre>

<p>But after I ploted the image I found that the cluster result is not the way I expecte it to be, since I know what it should look like.</p>

<p>So I tried simil instead, and the code is like:</p>

<pre><code>distanceMatrix &lt;- simil(dfm[,-1], method='Pearson')
clusters &lt;- hclust(pr_simil2dist(distanceMatrix))   
clusters$labels &lt;- dfm[,1]#colnames(dfm)[-1]
plot(clusters, labels=clusters$labels)
</code></pre>

<p>This code computes a similarity matrix using simil, then convert it to distance matrix using pr_simil2dist, then I plot it and get the result I expected !</p>

<p>I'm confused about the relationship between dist and simil. According to the relationship described in the documentation, shouldn't the two code snippet has the same result?</p>

<p>Where am I wrong ?</p>

<p>Edit:</p>

<p>You can try my code with dfm of the following value, sorry for the bad indentation.</p>

<pre><code>                             Blog china kids music yahoo want wrong
                         Gawker     0    1     0     0    7     0
                  Read/WriteWeb     2    0     1     3    1     1
                 WWdN: In Exile     0    2     4     0    0     0
           ProBlogger Blog Tips     0    0     0     0    2     0
                    Seth's Blog     0    0     1     0    3     1
 The Huffington Post | Raw Feed     0    6     0     0   14     5
</code></pre>

<p>Edit:</p>

<p>Actually the sample data is taken from a very big data frame using <code>tail</code>, and I get completely different matrix using dist and simil+pr_simil2dist. The full data can found <a href=""http://kiwitobes.com/clusters/blogdata.txt"" rel=""nofollow"">here.</a></p>

<p>In case I made other silly mistakes, here's the full code of my function:</p>

<p>The code I use to read in data:</p>

<pre><code>dfm&lt;- read.table(filename, header=T, sep='\t', quote='')
</code></pre>

<p>Code for clustering:</p>

<pre><code>hcluster &lt;- function(dfm, distance='Pearson'){
    dfm &lt;- tail(dfm)[,c(1:7)] # I use this to give the sample data.
    distanceMatrix &lt;- simil(dfm[,-1], method=pearson)
    clusters &lt;- hclust(pr_simil2dist(distanceMatrix))   
    clusters$labels &lt;- dfm[,1]#colnames(dfm)[-1]
    plot(clusters, labels=clusters$labels)
}
</code></pre>

<p>Matrix using dist:</p>

<pre><code>           94         95         96         97         98
95 -0.2531580                                            
96 -0.2556859 -0.4629100                                 
97  0.9897783 -0.1581139 -0.2927700                      
98  0.8742800 -0.2760788 -0.1022397  0.9079594           
99  0.9114339 -0.5020405 -0.2810414  0.8713293  0.8096980
</code></pre>

<p>Matrix using simil+pr_simil2dist:</p>

<pre><code>           94         95         96         97         98
95 1.25315802                                            
96 1.25568595 1.46291005                                 
97 0.01022173 1.15811388 1.29277002                      
98 0.12572004 1.27607882 1.10223973 0.09204062           
99 0.08856608 1.50204055 1.28104139 0.12867065 0.19030202
</code></pre>

<p>You can see that corresponding elements in the two matrices add up to 1, which I think is not right. So there must be something I'm doing wrong.</p>

<p>Edit:</p>

<p>After I specify names in the read.table function to read in the data frame, the dist way and simil+pr_simil2dist way give the same correct result. <strong>So technically problem solved</strong>, but I don't know why my original way of handling data frame have anything to do with dist and simil. </p>

<p>Any one has a clue on that ?</p>
"
220,explanation of least squares,"<p>Can someone explain me how to start this problem: method of least squares fit, </p>

<p>$y=Bx$ to the following $n=6$ points $(3,4),(1,2),(5,4),(6,8),(3,6),(4,5)$</p>

<p>I have calculated $x$ mean and $y$ mean, and calculated $\large \frac{(x-xm)(y-ym)}{(x-xm)^{2}}$ but it doesn't give me a right answer</p>
"
221,How to get Sweave to put graphics in separate folder AND name them after the Rnw file,"<p>I've seen a few questions about this, but can't work out how to do what I want.</p>

<p>By default, Sweave creates graphics by concatenating the name of the Rnw file and the name of the graphic object label.</p>

<p>From this question (<a href=""http://stackoverflow.com/questions/4678888/make-sweave-rweavehtml-put-all-graphics-in-a-specified-folder"">Make Sweave + RweaveHTML put all graphics in a specified folder</a>) If I want all my graphics in the folder foo and to be named bar-graphic I can use</p>

<pre><code>\SweaveOpts{prefix.string=foo/bar}
</code></pre>

<p>But how can I get the graphics in folder foo but named rnwfile-graphic ?</p>
"
222,"Generating ""2D"" histogram in R","<p>I am new to R and I would like to know how to generate histograms for the following situation :</p>

<p>I initially have a regular frequency table with 2 columns : Column A is the category (or bin) and Column B is the number of cases that fall in that category </p>

<pre><code>Col A    Col B
1-10       7
11-20      4
21-30      5
</code></pre>

<p>From this initial frequency table, I create a table with 3 columns : Col A is again the category (or bin), but now Col B is the ""fraction of total cases"", so for the category 1-10, column B will have the value 7/(7+4+5) = 7/16 . Now there is also a third column, Col C which is ""fraction of total cases falling between the categories 1-20"", so for 1-10, the value for Col C would be 7/(7+4) = 7/11. The complete table would look like below :</p>

<pre><code>Col A    Col B    Col C
1-10      7/16     7/11
11-20     4/16     4/11
21-30     5/16      0
</code></pre>

<p>How do I generate a histogram from this 3-column table above ? My X axis should be the bin (1-10, 11-20 etc.) and my Y axis should be the fraction, however for every bin I have two fractions (Col B and Col C), so there will be two fraction ""bars"" for every bin in the histogram. </p>

<p>Any help would be greatly appreciated.</p>
"
223,Root mean square error in R - mixed effect model,"<p>Could you please tell me how to get/compute the value RMSE (root mean square error) in R when you perform a mixed effect model</p>

<pre><code>Data: na.omit(binh) 
       AIC      BIC    logLik
  888.6144 915.1201 -436.3072

Random effects:
 Formula: ~1 | Study
        (Intercept) Residual
StdDev:    3.304345 1.361858

Fixed effects: Eeff ~ ADF + CP + DE + ADF2 + DE2 
                Value Std.Error  DF   t-value p-value
(Intercept)  -0.66390 18.870908 158 -0.035181  0.9720
ADF           1.16693  0.424561 158  2.748556  0.0067
CP            0.25723  0.097524 158  2.637575  0.0092
DE          -36.09593 12.031791 158 -3.000046  0.0031
ADF2         -0.03708  0.011014 158 -3.366625  0.0010
DE2           4.77918  1.932924 158  2.472513  0.0145
 Correlation: 
     (Intr) ADF    CP     DE     ADF2  
ADF  -0.107                            
CP   -0.032  0.070                     
DE    0.978 -0.291 -0.043              
ADF2  0.058 -0.982 -0.045  0.250       
DE2  -0.978  0.308  0.039 -0.997 -0.265

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.28168116 -0.45260885  0.06528363  0.57071734  2.54144168 

Number of Observations: 209
Number of Groups: 46 
</code></pre>
"
224,Billboard orientation toward camera (point) without normals,"<p>So I have a circle of planes which get constructed as:</p>

<pre><code>plane.x = Math.cos(angle) * radius;
plane.z = Math.sin(angle) * radius;
plane.rotationY = (-360 / numItems) * i - 90;
</code></pre>

<p>which calculates how many angular slices there are for total number of planes (rotationY) and places each plane into it's place on a circle of radius.</p>

<p>then to update their rotation around the circle I have:</p>

<pre><code>var rotateTo:Number = (-360 / numItems) * currentItem + 90;
TweenLite.to(planesHolder, 1, { rotationY:rotateTo, ease:Quint.easeInOut } );
</code></pre>

<p>as you can see planes are circling and each is oriented 90 degrees out from the circle.
I'm using this as a reference - it's pretty much that: <a href=""http://papervision2.com/a-simple-papervision-carousel/"" rel=""nofollow"">http://papervision2.com/a-simple-papervision-carousel/</a></p>

<p>Now, what I'd like to find out is how could I calculate degree of orientation for each individual plane to always face camera without normal, if it's possible at all. I've tried plane.LookAt(camera), but that doesn't work. Basically every plane should have orientation as the one facing camera in the middle. </p>

<p>Somehow I think I can't modify that example from link to do that.</p>

<p>edit: OK I answered my own question after I wrote it. Helps to read your own thoughts written. So as I'm orienting planes individually and rotating them all as a group, what I did after tween of the group in code above, was to loop through each plane and orient it to the Y orientation of the forward plane as so:</p>

<pre><code>for (var i:int = 0; i &lt; planes.length; i++) {
    TweenLite.to(planes[i], 1, { rotationY:(-360 / numItems * rotoItem - 90), ease:Quint.easeInOut } );
} 
</code></pre>

<p>rotoItem is the one at the front. Case closed.</p>
"
225,Karatsuba algorithm too much recursion,"<p>I am trying to implement the Karatsuba multiplication algorithm in c++ but right now I am just trying to get it to work in python. </p>

<p>Here is my code:</p>

<pre><code>def mult(x, y, b, m):
    if max(x, y) &lt; b:
        return x * y

    bm = pow(b, m)
    x0 = x / bm
    x1 = x % bm
    y0 = y / bm
    y1 = y % bm

    z2 = mult(x1, y1, b, m)
    z0 = mult(x0, y0, b, m)
    z1 = mult(x1 + x0, y1 + y0, b, m) - z2 - z0

    return mult(z2, bm ** 2, b, m) + mult(z1, bm, b, m) + z0
</code></pre>

<p>What I don't get is: how should <code>z2</code>, <code>z1</code>, and <code>z0</code> be created? Is using the <code>mult</code> function recursively correct? If so, I'm messing up somewhere because the recursion isn't stopping. </p>

<p>Can someone point out where the error is?</p>
"
226,Formulae for combining statistical moments,"<p>I am writing code to calculate statistical moments (mean, variance, skewness, kurtosis) for large samples of data and have the requirement of needing to be able to calculate moments for subsections of the sample (in parallel), then combine/merge them to give the moment for the sample as a whole. </p>

<p>For example:</p>

<p>$S = \lbrace 1.0, 1.2, 2.0, 1.7, 3.4, 0.9 \rbrace $</p>

<p>$A = \lbrace 1.0, 1.2, 2.0 \rbrace$ and $B = \lbrace  1.7, 3.4, 0.9 \rbrace$</p>

<p>So $A \cup B = S$</p>

<p>I need to calculate the statistics/moments for $A$ and $B$, then combine them to give the statistics/moments for $S$</p>

<hr>

<p>Count is simple: $n_S = n_A + n_B$</p>

<p>Mean is not much worse: $\mu_S = (n_A\mu_A + n_B\mu_B) / n_S$</p>

<p>Variance is a little less pretty: $\sigma_S = [n_A\sigma_A + n_B\sigma_B + (\frac{n_An_A}{n_A+n_B})(\mu_A - \mu_B)^2] / n_S$</p>

<hr>

<p>But now I'm struggling for skewness and, in particular, kurtosis. I have all 'lesser' moments for each of the subsections of the data available and have some idea of the direction I'm heading, but am really struggling to derive the formulae needed. </p>

<p>Has anybody derived these formulae before? Could anyone point me in the right direction? These may be simple/obvious things to any with anyone with a decent amount of statistical knowledge, unfortunately that's something I completely lack...</p>
"
227,Data cleaning in Excel sheets using R,"<p>I have data in Excel sheets and I need a way to clean it. I would like remove inconsistent values, like Branch name is specified as (Computer Science and Engineering, C.S.E, C.S, Computer Science). So how can I bring all of them into single notation? </p>
"
228,"sklearn GaussianNB - bad results, [nan] probabilities","<p>I'm doing some work on gender classification for a class.  I've been using SVMLight with decent results, but I wanted to try some bayesian methods on my data as well.  My dataset consists of text data, and I've done feature reduction to pare down the feature space to a more reasonable size for some of the bayesian methods.  All of the instances are run through tf-idf and then normalized (through my own code).</p>

<p>I grabbed the sklearn toolkit because it was easy to integrate with my current codebase, but the results I'm getting from the GaussianNB are all of one class (-1 in this case), and the predicted probabilities are all [nan].</p>

<p>I've pasted some relevant code; I don't know if this is enough to go on, but I'm hoping that I'm just overlooking something obvious in using the sklearn api.  I have a couple different feature sets that I've tried pushing through it, also with the same results.  Same thing too using the training set and with cross-validation.  Any thoughts?  Could it be that my feature space simply too sparse for this to work?  I have 300-odd instances, most of which have several hundred non-zero features.</p>

<pre><code>class GNBLearner(BaseLearner):
    def __init__(self, featureCount):
        self.gnb = GaussianNB()
        self.featureCount = featureCount

    def train(self, instances, params):
        X = np.zeros( (len(instances), self.featureCount) )
        Y = [0]*len(instances)
        for i, inst in enumerate(instances):
            for idx,val in inst.data:
                X[i,idx-1] = val
            Y[i] = inst.c
        self.gnb.fit(X, Y)

    def test(self, instances, params):
        X = np.zeros( (len(instances), self.featureCount) )
        for i, inst in enumerate(instances):
            for idx,val in inst.data:
                X[i,idx-1] = val
        return self.gnb.predict(X)

    def conf_mtx(self, res, test_set):
        conf = [[0,0],[0,0]]
        for r, x in xzip(res, test_set):
            print ""pred: %d, act: %d"" % (r, x.c)
            conf[(x.c+1)/2][(r+1)/2] += 1
        return conf
</code></pre>
"
229,How did they count the number of lines of code executed at runtime?,"<p>There was a PC game released in 2001 called <a href=""http://en.wikipedia.org/wiki/Black_&amp;_White_%28video_game%29"" rel=""nofollow"">Black &amp; White</a> by <a href=""http://lionhead.com/"" rel=""nofollow"">Lionhead studios</a> in which there was a lengthy statistics page which updated in real-time. There were stats such as how many people killed, how much money you've earned, etc... but the really puzzling one was Total lines of code executed, which was into the billions and counting.</p>

<p>How would they have known this, how would they have calculated this at runtime? Did they make it up?</p>
"
230,Is the probability of getting all right answers same as wrong ones?,"<p>Well many times i have come acroos the common phase that its difficult to get all right that all wrong. Well i thought of working a bit on it and needless to say pretty much baffled , when i happen to come across the very same statement in a serial NUMB3RS,  i m  wondering how could that be true ?
supposed if we take a MCQ ( multiple choice questions ) which usually comes with 4 options , in which one of them is right , then the probability of getting the right answer is 1/4 and the probability of getting a wrong answer would be 3/4 .
Now if we have 10 questions , then the probability of getting all right would be (1/4)^10 . ? is that correct ? and the probability of getting a wrong answer would be (3/4)^10 ? if both my conjectures are true , wont that make the probability of getting all right make it much lesser than probability of all wrong.</p>
"
231,superLU for inverting a matrix,"<p>I tried to use superLU package to invert a sparse matrix.<br>
I defined the matrix as A, and defined the B matrix to be I with the same size, while nrhs is again the same size  </p>

<p>I got a result of really big negetive numbers (-infinite?)</p>

<p>Does anyone know a good tutorial for superLU that doesn't solve a single line (meaning: X is in the dimentions of nXn and not nX1)</p>
"
232,Is it possible to combine separate boxlot summaries into one and create the combined graph?,"<p>I am working with rather large datasets (appx. 4 mio rows per month with 25 numberic attributes and 4 factor attributes). I would like to create a graph that contains per month (for the last 36 months) a boxplot for each numeric attribute per product (one of the 4 factor attributes). </p>

<p>So as an example for product A:</p>

<pre><code>                    - 
      _             |          -
     _|_            |         _|_
    |   |           |        |   |
    |   |          _|_       |   |
    |   |         |   |      |---|
    |   |         |---|      |   |
    |---|         |   |      |   |
    |_ _|         |   |      |_ _|
      |           |_ _|        |
      |             |          |
      -             |          -
                    -

 --------------------------------------------------------------
    jan '10      feb '10    mar '10 ................... feb '13
</code></pre>

<p>But since these are quite large datasets I will be working with I would like some advice to get started on how to approach. My idea (but I am not sure if this is possible) is to</p>

<ul>
<li>a) extract the data per month per product </li>
<li>b) create a boxplot for that specific month (so let's say jan'10 for product A) </li>
<li>c) store the boxplot summary data somewhere </li>
<li>d) repeat a-c for all months until feb '13 </li>
<li>e) combine all the stored boxplot summary data into one </li>
<li>f) plot the combined boxplot g) repeat a-f for all other products</li>
</ul>

<p>So my main question is: <em>is it possible to combine separate boxlot summaries into one and create the combined graph as sketched above from this?</em> </p>

<p>Any help would be appreciated,</p>

<p>Thank you</p>
"
233,"Where to find an implementation of Modern Portfolio Statistics in Java or Oracle (eg sharpe ratio, sortinto ratio etc)","<p>I require an a java component or oracle package that implements all of the formulas in ""Modern Portfolio Theory"".
(They do not need to be open source, they can be commerical components.)</p>

<p>My searching has yet to find anything suitable.</p>

<p>Any help on this would be greatly appreciated, I would like to buy a component to do this rather than writing it myself.</p>

<p>Thank you</p>
"
234,numpy array subclass unexpedly shares attributes across instances,"<p>I am having a weird subclass numpy.ndarray issue that feels like<br>
<a href=""http://stackoverflow.com/questions/5561294/values-of-instance-variables-of-superclass-persist-across-instances-of-subclass"">Values of instance variables of superclass persist across instances of subclass</a><br>
But I have not been able to understand fully or make it work for for my example.  </p>

<p>Reading through 
<a href=""http://docs.scipy.org/doc/numpy/user/basics.subclassing.html#slightly-more-realistic-example-attribute-added-to-existing-array"" rel=""nofollow"">Slightly more realistic example - attribute added to existing array</a> I am trying to do pretty much exactly this. I want to add an attrs attribute to an array to hold information such as units in a dictionary. </p>

<p>Here is what I have:</p>

<pre><code>import numpy
class dmarray(numpy.ndarray):
    def __new__(cls, input_array, attrs={}):
        obj = numpy.asarray(input_array).view(cls)
        obj.attrs = attrs
        return obj

    def __array_finalize__(self, obj):
        # see InfoArray.__array_finalize__ for comments
        if obj is None:
            return
        self.attrs = getattr(obj, 'attrs', {})
</code></pre>

<p>So then to use it and demonstrate the issue</p>

<pre><code>a = dmarray([1,2,3,4])
b = dmarray([1,2,3,4])
a.attrs['foo'] = 'bar'
print(b.attrs)
#{'foo': 'bar'}
b.attrs is a.attrs
# True  # hmm....
</code></pre>

<p>So b is picking up attrs that I don't want it to.  Annoyingly it works fine if you do this:</p>

<pre><code>from datamodel import *
a = dmarray([1,2,3,4], attrs={'foo':'bar'})
b = dmarray([1,2,3,4])
b.attrs
# {}
</code></pre>

<p>So how in the world do I make this dmarray work how I want it to?</p>

<hr>

<p>Edit:
OK so this seems to fix the problem but I don't understand why.  So lets change the question to what this is doing and why it works?</p>

<pre><code>class dmarray(numpy.ndarray):
    def __new__(cls, input_array, attrs=None):
        obj = numpy.asarray(input_array).view(cls)
        return obj

    def __init__(self, input_array, attrs=None):
        if attrs == None:
            attrs = {}
        self.attrs = attrs
</code></pre>

<p>So by removing the kwarg from <code>__new__()</code> and putting it in <code>__init__()</code> it works.  I just tried this as a ""well it might work""</p>

<pre><code>a = dmarray([1,2,3,4])
b = dmarray([1,2,3,4])
a.attrs['foo'] = 'bar'
b.attrs
# {}
</code></pre>
"
235,How to combine 2 Date vectors in data frame without changing format,"<p>I'm trying to put 2 vectors of dates together in one data frame but when I do they change formats.  Any suggestions?</p>

<pre><code>&gt; head(starters)
    [1] ""2009-01-02"" ""2009-02-02"" ""2009-03-02"" ""2009-04-01""
    [5] ""2009-05-01"" ""2009-06-01""
    &gt; head(enders)
    [1] ""2009-01-30"" ""2009-02-27"" ""2009-03-31"" ""2009-04-30""
    [5] ""2009-05-29"" ""2009-06-30""
    &gt; class(enders)
    [1] ""Date""
    &gt; class(starters)
    [1] ""Date""
    &gt; comboer &lt;- data.frame(cbind(starters, enders))
    &gt; head(comboer)
      starters enders
    1    14246  14274
    2    14277  14302
    3    14305  14334
    4    14335  14364
    5    14365  14393
    6    14396  14425
</code></pre>
"
236,Order a NXM Numpy Array according to cumulative sums of each sub-array,"<p>I have 5 numpy arrays within a single nxm array, with each array containing a set of random values. What I need is to print them according to their decreasing cumulative sums. For instance, my nXm array looks like:</p>

<pre><code> v = [[1,2,3,4], [2,3,4,5], [11,21,3,4], [4,33,21,1], [2,4,6,5]]
</code></pre>

<p>and what I need it to be ordered like is: the one having the highest cumulative sum comes first. I tried printing them according to argmax but it sneaks into all the elements of individual arrays and sorts them in a descending order..</p>

<p>Is there a way?</p>
"
237,Scala environment for statistical computing and graphics?,"<p>Please advise on Scala environment for statistical computing and graphics similar to R or Weka.
Thanks!</p>
"
238,Doing a loop in R,"<p>I need to do a loop (which I haven't done before) and given the observations (column 1), I need to work out (i) which of the combinations across the variables (s1-s5) are significant (P&lt;0.05), (ii) to only keep the combinations that are significant across the variables with the corresponding p values. I thought that this would be a good way to learn how to do a loop in R. The original data is large and is similar to this one:</p>

<pre><code>ob &lt;- c(120,100,85,56,87)
s1 &lt;- c(""ab"",""aa"",""ab"",""aa"",""bb"")
s2 &lt;- c(""aa"",""aa"",""ab"",""bb"",""bb"")
s3 &lt;- c(""bb"",""ab"",""aa"",""ab"",""ab"")
s4 &lt;- c(""aa"",""ab"",""bb"",""ab"",""aa"")
s5 &lt;- c(""bb"",""ab"",""aa"",""ab"",""bb"")
dset &lt;- data.frame(ob,s1,s2,s3,s4,s5)
</code></pre>

<p>dset</p>

<pre><code>ob s1 s2 s3 s4 s5
120 ab aa bb aa bb
100 aa aa ab ab ab
85 ab ab aa bb aa
56 aa bb ab ab ab
87 bb bb ab aa bb
</code></pre>

<p>Any help would be appreciated!</p>

<p>Baz</p>
"
239,Are integers in C assumed to be handled by a hardware spec or handled in software?,"<p>Are integers in C assumed to be handled by a hardware spec or handled in software?</p>

<p>By integer, I am referring to the primitive ""int""</p>

<p>The underlying idea being that if integers in C are not hardware dependent would it be a violation of standard to have gcc implement different integer handlers. This way you could have your traditional 32 bit int handler, and then you could also have handlers for 256 bit integers, or maybe even dynamic sized ints.</p>

<p>I do know about long and long long, but I'm not sure if those are independent of that hardware provided, and I'd like to just specify the size/type of ""int"" handler to use when building a toolchain.</p>

<p>I also understand the dangers of doing this (because building building 256 bit integers with a 32 bit integer compiler would be bad!), but for those bits of code that require something like a gmp library, I think it would make code readability much better. Compatibility would be the same but the dependency would be on the compiler instead of the code itself.</p>

<p>Crazy idea, I know... but back to the original question:</p>

<p>Are integers in C assumed to be handled by a hardware spec or handled in software?</p>
"
240,Subtracting Two Columns Consisting of Both Date and Time in R,"<p>I am having a problem with subtracting two columns of a table consisting of both date and time in a single cell. </p>

<pre><code>&gt; TimeData

DEPARTURE_TIME      LAB_TIME
1/30/2010 4:18      1/30/2010 0:29
1/30/2010 4:18      1/30/2010 0:29
1/30/2010 6:49      1/30/2010 0:48
1/30/2010 6:49      1/30/2010 0:48
1/30/2010 9:42      1/30/2010 1:29
1/30/2010 9:42      1/30/2010 1:29
1/30/2010 7:25      1/30/2010 1:16
</code></pre>

<p>I need to obtain the difference between Departure Time and Lab Time in hours and minutes.</p>

<p>Do I need to separate time and date or is there a way to subtract the data in this way ?</p>

<p>I really appreciate any help.</p>
"
241,"Install R package from source, without changing PATH (Windows)","<p>I'm trying to install package <code>rpart</code> for R-2.14.0 on Windows 7, but I get the warning:</p>

<blockquote>
  <p>package ‘rpart’ is not available (for R version 2.14.0)</p>
</blockquote>

<p>So I download the tar.gz file from the package page.  I have installed Rtools but I disabled all the options, including the one that changes the PATH.  Then I do the following:</p>

<pre><code>&gt; install.packages(""C:/rpart_3.1-50.tar.gz"", type=""source"")
Installing package(s) into ‘C:/Users/backupSam/Documents/R/win-library/2.14’
(as ‘lib’ is unspecified)
inferring 'repos = NULL' from the file name
* installing *source* package 'rpart' ...
** libs

*** arch - i386
ERROR: compilation failed for package 'rpart'
* removing 'C:/Users/backupSam/Documents/R/win-library/2.14/rpart'
* restoring previous 'C:/Users/backupSam/Documents/R/win-library/2.14/rpart'
Warning messages:
1: running command 'C:/PROGRA~1/R/R-214~1.0/bin/i386/R CMD INSTALL -l ""C:/Users/backupSam/Documents/R/win-library/2.14""   ""C:/rpart_3.1-50.tar.gz""' had status 1 
2: In install.packages(""C:/rpart_3.1-50.tar.gz"", type = ""source"") :
  installation of package ‘C:/rpart_3.1-50.tar.gz’ had non-zero exit status
</code></pre>

<p>Any suggestions?</p>
"
242,How to check if m n-sized vectors are linearly independent?,"<p><strong>Disclaimer</strong><br />
This is not strictly a programming question, but most programmers soon or later have to deal with math (especially algebra), so I think that the answer could turn out to be useful to someone else in the future.</p>

<p><strong>Now the problem</strong><br />
I'm trying to check if m vectors of dimension n are linearly independent. If m == n you can just build a matrix using the vectors and check if the determinant is != 0. But what if m &lt; n?</p>

<p>Any hints?</p>

<hr>

<p>See also <a href=""http://academicearth.org/lectures/independence-basis-and-dimension"" rel=""nofollow"">this video lecture</a>.</p>
"
243,What's the best practice to round a float to 2 decimals?,"<p>I'm using eclipse + Android SDK.</p>

<p>I need to round a float value to 2 decimals. I usually use the next ""trick"" using Math library.</p>

<pre><code>float accelerometerX = accelerometerX * 100;
    accelerometerX = round(accelerometerX);
    Log.d(""Test"","""" + accelerometerX/100);
</code></pre>

<p>But I feel it is not the best way to do it.</p>

<p>Is there a library to do these type of operations?</p>

<p>Thanks in advance.</p>
"
244,Having all layers in the legend with ggplot,"<p>how could I make a legend representing all the curves that are plotted in my graph ? Presently, an automatic legend is generated for the first layer (based on the ""colour"" aesthetic), but the other layer (the black curve representing the density of ""price"" variable across all observations) in not contained in this legend. </p>

<p>I conceive that my question comes certainly from an incomplete understanding of the concepts behing ggplot package. </p>

<pre><code>ggplot(diamonds) + 
  geom_density(aes(x = price, y = ..density.., colour = cut)) +
  geom_density(aes(x = price,y = ..density..))
</code></pre>

<p><img src=""http://i.stack.imgur.com/Kxa73.jpg"" alt=""enter image description here""></p>
"
245,Can I use a lambda expression here (selecting records from a numpy.core.records.recarray)?,"<p>I have the following code snippet, which loads data from a CSV file into a numpy.core.records.recarray:</p>

<pre><code>r = mlab.csv2rec(datafile, delimiter=',', names=('dt', 'val'))
data = zip(date2num(r['dt']),r['val']) # Need to filter for records lying between two dates here ...
</code></pre>

<p>I want to only 'zip' records that have dates falling bewteen (say) '2000-01-01' and 2000-03-01'</p>

<p>I understand the concept of lambda functions - but I haven't used them before. It would be cool if I could use a lambda to filter the records between the required dates (like in pseudocode below):</p>

<pre><code>data = zip(lambda: date2num(r['dt']),r['val'] if r['dt'] &gt; '2000-01-01' and r['dt'] &lt; '2000-03-01' )
</code></pre>

<p>What is the Pythonic way to extract a subset of data from the rec.array, based on specified indixes (i.e. dates)?</p>
"
246,How to set alpha in R?,"<p>I have <a href=""http://rss.acs.unt.edu/Rdoc/library/coin/html/LocationTests.html"" rel=""nofollow"">this example</a> from the coin package of R:</p>

<pre><code>  library(coin)
  library(multcomp)
  ### Length of YOY Gizzard Shad from Kokosing Lake, Ohio,
  ### sampled in Summer 1984, Hollander &amp; Wolfe (1999), Table 6.3, page 200
  YOY &lt;- data.frame(length = c(46, 28, 46, 37, 32, 41, 42, 45, 38, 44, 
                               42, 60, 32, 42, 45, 58, 27, 51, 42, 52, 
                               38, 33, 26, 25, 28, 28, 26, 27, 27, 27, 
                               31, 30, 27, 29, 30, 25, 25, 24, 27, 30),
                    site = factor(c(rep(""I"", 10), rep(""II"", 10),
                                    rep(""III"", 10), rep(""IV"", 10))))

  ### Nemenyi-Damico-Wolfe-Dunn test (joint ranking)
  ### Hollander &amp; Wolfe (1999), page 244 
  ### (where Steel-Dwass results are given)
  NDWD &lt;- oneway_test(length ~ site, data = YOY,
      ytrafo = function(data) trafo(data, numeric_trafo = rank),
      xtrafo = function(data) trafo(data, factor_trafo = function(x)
          model.matrix(~x - 1) %*% t(contrMat(table(x), ""Tukey""))),
      teststat = ""max"", distribution = approximate(B = 90000))

  ### global p-value
  print(pvalue(NDWD))

  ### sites (I = II) != (III = IV) at alpha = 0.01 (page 244)
  print(pvalue(NDWD, method = ""single-step""))
</code></pre>

<p>I want to assign alpha a different value, how can I do this??</p>

<p>This doesn't work!</p>

<pre><code>  library(coin)
  library(multcomp)
  ### Length of YOY Gizzard Shad from Kokosing Lake, Ohio,
  ### sampled in Summer 1984, Hollander &amp; Wolfe (1999), Table 6.3, page 200
  YOY &lt;- data.frame(length = c(46, 28, 46, 37, 32, 41, 42, 45, 38, 44, 
                               42, 60, 32, 42, 45, 58, 27, 51, 42, 52, 
                               38, 33, 26, 25, 28, 28, 26, 27, 27, 27, 
                               31, 30, 27, 29, 30, 25, 25, 24, 27, 30),
                    site = factor(c(rep(""I"", 10), rep(""II"", 10),
                                    rep(""III"", 10), rep(""IV"", 10))))

  ### Nemenyi-Damico-Wolfe-Dunn test (joint ranking)
  ### Hollander &amp; Wolfe (1999), page 244 
  ### (where Steel-Dwass results are given)
  NDWD &lt;- oneway_test(length ~ site, data = YOY,
      ytrafo = function(data) trafo(data, numeric_trafo = rank),
      xtrafo = function(data) trafo(data, factor_trafo = function(x)
          model.matrix(~x - 1) %*% t(contrMat(table(x), ""Tukey""))),
      teststat = ""max"", distribution = approximate(B = 90000),
      alpha = 0.05)

  ### global p-value
  print(pvalue(NDWD))

  ### sites (I = II) != (III = IV) at alpha = 0.05 (default was 0.01) (page 244)
  print(pvalue(NDWD, method = ""single-step""))
</code></pre>
"
247,Parallelism in Julia. Features and Limitations,"<p>In their <a href=""http://arxiv.org/abs/1209.5145"" rel=""nofollow"">arXiv paper</a>, the original authors of Julia mention the following: </p>

<blockquote>
  <p><code>2.14 Parallelism</code>. 
  Parallel execution is provided by a message-based multi-processing system implemented in Julia in the standard library.
  The language design supports the implementation of such libraries by
  providing symmetric coroutines, which can also be thought of as
  cooperatively scheduled threads. This feature allows asynchronous
  communication to be hidden inside libraries, rather than requiring the
  user to set up callbacks. <strong>Julia does not currently support native
  threads, which is a limitation, but has the advantage of avoiding the
  complexities of synchronized use of shared memory.</strong></p>
</blockquote>

<p>What do they mean by saying that Julia does not support <strong>native threads</strong>? What is a native thread?</p>

<p>Do other interpreted languages such as <strong>Python</strong> or <strong>R</strong> support this type of parallelism? Is Julia alone in this?</p>
"
248,rep() first level of lists,"<p>I wish to expand lists to a given length using <code>rep()</code>, as in:</p>

<pre><code>n = 5
l = 1:3
rep(l, length=n)
</code></pre>

<p>However, my lists come in two flavours, nested or not nested:</p>

<pre><code>l1 &lt;- list(a=1, b=2)
l2 &lt;- list(list(a=1, b=2), list(x=1, y=2))

rep(l2, length=n) # as desired

rep(l1, length=n) # result is a single list, where I really want 
rep(list(l1), length=n) # instead
</code></pre>

<p>To deal with this problem I probably need to identify the problematic <code>l1</code> as being ""first-level"" and wrap it into <code>list()</code> before applying <code>rep()</code>. What is the best way to do this?</p>
"
249,How to extract certain path types in igraph?,"<p><strong>TLDR: I'd like to extract the edge types of every path between two vertices in igraph.  Is there a relatively sane way to do this?</strong></p>

<hr>

<p>The clinic I work for recently undertook a rather large (1400-person) tuberculosis contact investigation in a high school.  I have class schedules for all of the students and teachers (!) and have put them into a network (using igraph in R), with each student and each room-period combination as a vertex (e.g., the class in Room 123 in Period 1 is a vertex with a directed edge to the class that's in Room 123 for Period 2).  I also know which rooms share ventilation systems - a plausible but unlikely mechanism for infection.  The graph is directed out from sole source case, so every path on the network has only two people in it - the source and a contact, separated by a variable number of room-period vertices.  Conceptually, there are four kinds of paths: </p>

<ul>
<li>personal-contact exposures (source -> contact only)</li>
<li>shared-class exposures (source -> room-period -> contact)</li>
<li>next-period exposures (source-> Room 123 Period 1 -> Room 123 Period
2 -> contact)</li>
<li>ventilation exposures (source -> Room 123 Period 1 -> Room 125 Period
1 -> contact)</li>
</ul>

<p>Every edge has an attribute indicating whether it's a person-to-person exposure, same-room-different-period, or ventilation edge.</p>

<p>As an intermediate step toward modeling infection on this network, I'd like to just get a simple count of how many exposures of each type a student has had.  For example, a student might have shared a class with the source, then later have been in a room the source had been in but a period later, and perhaps the next day been in a ventilation-adjacent room.  That student's indicators would then be:</p>

<pre><code>personal.contact: 0
shared.class:     1
next.period:      1
vent:             1
</code></pre>

<p>I'm not sure how best to get this kind of info, though - I see functions for getting <em>shortest</em> paths, which makes identifying personal contact links easy, but I think I need to evaluat <em>all paths</em> (which seems like a crazy thing to ask for on a typical social network, but isn't so mad when only the source and the room-periods have out-edges).  If I could get to the point where each source-to-contact path were represented by an ordered vector of edge types, I think I could subset them to my criteria easily.  I just don't know how to get there.  If igraph isn't the right framework for this and I just need to write some big horrible loops over the students' schedules, so be it!  But I'd appreciate some guidance before I dive down that hole.</p>

<hr>

<p>Here's a sample graph of a contact with each of the three indirect paths:</p>

<pre><code># Strings ain't factors
options(stringsAsFactors = FALSE)  
library(igraph)

# Create a sample case
edgelist &lt;- data.frame(out.id = c(""source"", ""source"", 
                                  ""source"", ""Rm 123 Period 1"", 
                                  ""Rm 125 Period 2"", ""Rm 125 Period 3"", 
                                  ""Rm 127 Period 4"", ""Rm 129 Period 4""),
                       in.id = c(""Rm 123 Period 1"", ""Rm 125 Period 2"", 
                                 ""Rm 127 Period 4"", ""contact"", 
                                 ""Rm 125 Period 3"", ""contact"", 
                                 ""Rm 129 Period 4"", ""contact""),
                       edge.type = c(""Source in class"", ""Source in class"",
                                     ""Source in class"", ""Student in class"",
                                     ""Class-to-class"", 
                                     ""Student in class"", ""Vent link"",
                                     ""Student in class""
                                     )
)

samp.graph &lt;- graph.data.frame(edgelist, directed = TRUE)

# Label the vertices with meaningful names
V(samp.graph)$label &lt;- V(samp.graph)$name

plot(samp.graph, layout = layout.fruchterman.reingold)
</code></pre>
"
250,Split screen (with unequal windows) plotting in R,"<p>I know I can use <code>par(mfrow=c(1, 2))</code> to create a plot with a split screen.  However, I'd really like to create a plot where 2/3 of the window is used to plot one graph, and 1/3 of the window is used to plot another.  Is this possible? </p>
"
251,Evaluating a string as a mathematical expression in JavaScript,"<p>How do I parse and evaluate a mathematical expression in a string (e.g. <code>'1+1'</code>) without invoking <code>eval(string)</code> to yield its numerical value?</p>

<p>With that example, I want the function to accept <code>'1+1'</code> and return <code>2</code>.</p>
"
252,finding cells in a grid that intersect a given rectangle,"<p>(I'll tag this for both Java and language-agnostic, since I think the idea doesn't really require Java, but that's my specific application, so IDK which of the two tags is appropriate).</p>

<p>Let's say I have a grid with an arbitrary number of rows and columns, and and arbitrary cell size.</p>

<p>This grid represents a 2d space.  Now let's say I have a rectangle somewhere in that 2d space - in the past, I remember being able to get back all the cells that intersected the rectangle, (without having to loop), but the math is escaping me at the moment.</p>

<p>To firm up the example, lets say there are 12 rows and 10 columns.  Cells are 256 square (so rows at 256 tall, and columns are 256 wide).  If there was a rectangle at x:400, y:300 that was 200x200, I know that it would intersect the second and third columns in the second row.</p>

<p>So if cell structure was defined like so:</p>

<pre><code>// reference[rows][columns]    
SomeCellClass[][] cells = SomeCellClass[12][10]
</code></pre>

<p>Then the intersections would be <code>SomeCellClass[1][1]</code> and <code>SomeCellClass[1][2]</code></p>

<p>And ideally the return would be something like</p>

<pre><code>private SomeCellClass[] blah(){
  // do work
  SomeCellClass[] product = new SomeCellClass[total];
  SomeCellClass[0] = // first one that intersects...
  SomeCellClass[1] = // second one that intersects...
  // etc...
}
</code></pre>

<p>I remember it has something do do with dividing the rectangle position and dimension by cell size and flooring/ceiling to get back the index, but can't get my head around the specifics.  Again, I get how to do this with a loop but would like to be able to use just math and array indices.</p>

<p>Any help would be appreciated.</p>

<p>TYIA.</p>
"
253,Simple way to calculate point of intersection between two polygons in C#,"<p>I've got two polygons defined as a list of Vectors, I've managed to write routines to transform and intersect these two polygons (seen below Frame 1). Using line-intersection I can figure out whether these collide, and have written a working Collide() function. </p>

<p>This is to be used in a variable step timed game, and therefore (as shown below) in Frame 1 the right polygon is not colliding, it's perfectly normal for on Frame 2 for the polygons to be right inside each other, with the right polygon having moved to the left.</p>

<p>My question is, what is the best way to figure out the moment of intersection? In the example, let's assume in Frame 1 the right polygon is at X = 300, Frame 2 it moved -100 and is now at 200, and that's all I know by the time Frame 2 comes about, it was at 300, now it's at 200. What I want to know is when did it actually collide, at what X value, here it was probably about 250.</p>

<p><img src=""http://i.stack.imgur.com/wJ20e.png"" alt=""Polygon intersect""></p>

<p>I'm preferably looking for a C# source code solution to this problem.
Maybe there's a better way of approaching this for games?</p>
"
254,Fastest way to import millions of files in R?,"<p>I have 15 million CSV files, each with two columns (integer and float), and between 5 and 500 rows. Each file looks something like:</p>

<pre><code>3453,0.034
31,0.031
567,0.456
...
</code></pre>

<p>Currently, I am iterating over all the files, and using <code>read.csv()</code> to import each file into a big list. Here's a simplified version:</p>

<pre><code>allFileNames = Sys.glob(sprintf(""%s/*/*/results/*/*"", dir))

s$scores = list()

for (i in 1:length(allFileNames)){
        if ((i %% 1000) == 0){
            cat(sprintf(""%d of %d\n"", i, length(allFileNames)))
        }

        fileName = allFileNames[i]
        approachID = getApproachID(fileName) 
        bugID = getBugID(fileName)

        size = file.info(fileName)$size
        if (!is.na(size) &amp;&amp; size &gt; 0){ # make sure file exists and is not empty
            tmp = read.csv(fileName, header=F, colClasses=c(""integer"", ""numeric""))
            colnames(tmp) = c(""fileCode"", ""score"")
            s$scores[[approachID]][[bugID]]  = tmp
        } else {
            # File does not exist, or is empty. 
            s$scores[[approachID]][[bugID]] = matrix(-1, ncol=2, nrow=1)
        }
    }

tmp = read.csv(fileName, header=F, colClasses=c(""integer"", ""numeric"")
</code></pre>

<p>Later in my code, I go back through each matrix in the list, and calculate some metrics.</p>

<p>After starting this import process, it looks like it will take on the order of 3 to 5 days to complete. Is there a faster way to do this?</p>

<p><strong>EDIT</strong>: I added more details about my code.</p>
"
255,Random Perturbation of Data to get Training Data for Neural Networks,"<p>I am working on Soil Spectral Classification using neural networks and I have data from my Professor obtained from his lab which consists of spectral reflectance from wavelength 1200 nm to 2400 nm. He only has 270 samples.</p>

<p>I have been unable to train the network for accuracy more than 74% since the training data is very less (only 270 samples). I was concerned that my Matlab code is not correct, but when I used the Neural Net Toolbox in Matlab, I got the same results...nothing more than 75% accuracy.</p>

<p>When I talked to my Professor about it, he said that he does not have any more data, but asked me to do random perturbation on this data to obtain more data. I have research online about random perturbation of data, but have come up short.</p>

<p>Can someone point me in the right direction for performing random perturbation on 270 samples of data so that I can get more data? </p>

<p>Also, since by doing this, I will be constructing 'fake' data, I don't see how the neural network would be any better cos isn't the point of neural nets using actual real valid data to train the network?</p>

<p>Thanks,</p>

<p>Faisal.</p>
"
256,"Matching Columns, Creating Loop in R","<p>I have the following question:</p>

<p>I have data frame which looks like this. I have prices, 3 X's and 2 R's.</p>

<pre><code>Date    Name  Price  Interest
01.02.10 X  120     0.2
01.02.10 R  120     0.3
01.02.10 X  130     0.8
01.02.10 X  140     0.4
01.02.10 R  130     0.2
etc.
</code></pre>

<p>I would like to tell R to look for pairs of X&amp;Rs with the same price, and delete the rest. So this should result: 2 X's and 2'Rs (in this case).</p>

<pre><code>Date    Name  Price  Interest
01.02.10 X  120     0.2
01.02.10 R  120     0.3
01.02.10 X  130     0.8
01.02.10 R  130     0.2
etc.
</code></pre>

<p>To make it clearer (hopefully): I have a lot of different prices for each date. Each row either has an X or an R in it. There are a lot of pairs on each date, i.e. for example X, Price = 120 &amp; R, Price = 120 on Date 1. But there are also Prices which only match one Name, for example there is a Price = 140 only for Name = X. So what i would like R to do is: check for machting Names for one Price (i.e. there exists the same Price for one X and one R) and delete the rest. What actually would result is the same number of X's and R's because I'm looking for pairs.</p>

<p>I'm sorry not to be able to post something I tried. I just couldn't think of anything.</p>

<p>Now, to the next problem:
If the pairs are there, I would like to tell R to check each line. If the Name is X, I want it to calculate a new price, if not just print the existing price.
I tried</p>

<pre><code>xx &lt;- if(Name == ""X""){Price + 100*interest} else print{Price}
</code></pre>

<p>but it didn't work.</p>

<p>Thanks for help </p>

<p>Cheers
Dani</p>
"
257,Generate random geo-coordinate based on distance,"<p>I want to create a method that returns a random <code>CLLocationCoordinate2D</code> (or <code>CLLocation</code> object) based on another <code>CLLocationCoordinate2D</code> (or <code>CLLocation</code>). The generated coordinate should be within a certain distance (<strong>not</strong> degree, so based on a <code>CLLocationDistance</code> value or equivalent, <strong>not</strong> <code>CLLocationDegrees</code>) of the base coordinate in any random direction. So I'm basically generating a random coordinate within a circular boundary.</p>

<p>I understand how to do this easily in a regular coordinate system (just use some basic trig to calculate a new coordinate from a random angle and radius from the center of the boundary), however I'm stuck on how to do it in a latitude/longitude system. I could easily do this using trig if I wanted to generate a random coordinate based on degree differences, but those results would get skewed the closer I get to a pole, so I want to do it based on distance.</p>

<p>I couldn't find anything in the documentation that might be able to help me. If there were some way to get a new coordinate by displacing an existing coordinate based on some direction and distance, that would certainly be helpful, but I haven't found anything for that in documentation or on the web. Any ideas?</p>
"
258,How are operators called which affect the first operand instead of 'return' something?,"<p>I'm talking about operators which not return a value but modify (overwrite) the first operand.</p>

<p>Example in pseudo-code:</p>

<pre><code>      add  :=  return op1 + op2
increment  :=  op1 = op1 + op2
</code></pre>

<p>Given this mapping schema:</p>

<pre><code>add -&gt; increment
subtract -&gt; decrement
</code></pre>

<p>What could possibly be the names for other operators?</p>

<pre><code>multiply, divide, power, ... (what else?)
</code></pre>

<p>I was thinking about <code>add-&gt;selfAdd</code>, <code>multiply-&gt;selfMultiply</code>, but these names are somehow stupid.</p>

<p><em>NOTE: What's all this for? It's for an experimental programming language. Because of certain circumstances there may be only words, no operator signs, so I can't use <code>++</code> for <code>increment</code> or <code>*=</code> for <code>selfMultiply</code>.</em></p>
"
259,Time Until Extinction for a Pure Death Process Where the Time is Exponentially Distributed,"<p>Let $X(t)$ be a pure death process starting from $X(0)=N$. Assume that the death parameters are $\mu_1, \mu_2,\dots,\mu_N$. Let $T$ be an independent exponentially distributed random variable with parameter $\theta$. Show that $Pr\{X(T)=0\} = \prod_{i=1}^{N} \frac{\mu_i}{\mu_i+\theta}$.</p>

<p>My thoughts are that I should condition on $T=t$ and integrate so we would have</p>

<p>$$Pr\{X(T)=0\} = \int_0^\infty Pr\{X(t) = 0 | T = t\} Pr\{T=t\}dt$$</p>

<p>In the book we are given the formula for $P_n(t) = Pr\{X(t) = n\}$ but it is messy, here it is,
$$P_n(t) = \mu_{n+1}\dots\mu_{N} [ A_{n,n} e^{-\mu_n t} + \dots + A_{N,n}e^{-\mu_N t} ]$$
Where,
$$A_{k,n} = \frac{1}{(\mu_N - \mu_k) \dots (\mu_{k+1} - \mu_k)(\mu_{k-1} - \mu_k) \dots (\mu_n - \mu_k)}$$
Using my approach leads to some very ugly algebra and some terms that just don't seem to cancel. I tried doing it for the case of $N=2$ but still could not get it in the right form. I feel like I might be approaching this problem wrong but I don't see any other way to go about it.</p>
"
260,Sample variance derivation,"<p>I have quite a simple question but I can't for the life of me figure it out.</p>

<p>For a set of iid samples $\,\,X_1, X_2, .., X_n\,\,$ from distribution with mean $\,\mu$.</p>

<p>If you are given the sample variance as</p>

<p>$$ S^2 = \frac{1}{n-1}\sum\limits_{i=1}^{n}\left(X_i - \bar{X}\right)^2 $$</p>

<p>How can you write the following?</p>

<p>$$ S^2 = \frac{1}{n-1}\left[\sum\limits_{i=1}^{n}\left(X_i - \mu\right)^2 - n\left(\mu - \bar{X}\right)^2\right] $$</p>

<p>All texts that cover this just skip the details but I can't work it out myself. I get stuck after expanding like so</p>

<p>$$ S^2 = \frac{1}{n-1}\sum\limits_{i=1}^{n}\left(X_i^2 -2X_i\bar{X} + \bar{X}^2\right) $$</p>

<p>What am I missing?</p>

<p><strong>Edit:</strong>
A similarly equivalent expression is often given that I also can't derive but which may be more obvious is</p>

<p>$$  S^2 = \frac{1}{n-1}\left[\sum\limits_{i=1}^{n}X_i^2 - n\bar{X}^2\right] $$</p>
"
261,Line/Plane intersection based on points,"<p>I have two points in space, L1 and L2 that defines two points on a line.</p>

<p>I have three points in space, P1, P2 and P3 that 3 points on a plane.</p>

<p>So given these inputs, at what point does the line intersect the plane?</p>

<p>Fx. the plane equation A*x+B*y+C*z+D=0 is:</p>

<pre><code>A = p1.Y * (p2.Z - p3.Z) + p2.Y * (p3.Z - p1.Z) + p3.Y * (p1.Z - p2.Z)
B = p1.Z * (p2.X - p3.X) + p2.Z * (p3.X - p1.X) + p3.Z * (p1.X - p2.X)
C = p1.X * (p2.Y - p3.Y) + p2.X * (p3.Y - p1.Y) + p3.X * (p1.Y - p2.Y)
D = -(p1.X * (p2.Y * p3.Z - p3.Y * p2.Z) + p2.X * (p3.Y * p1.Z - p1.Y * p3.Z) + p3.X * (p1.Y * p2.Z - p2.Y * p1.Z))
</code></pre>

<p>But what about the rest?</p>
"
262,my rotation matrix for numpy (python) isn't working,"<p>i was making a program to display matrices under various transforms, and all of them work except for my rotation matrix. ive tried fiddling with it, but nothing seems to work</p>

<pre><code>y = input(""how many degrees do you want to rotate the shape around the origin?:    "")
j = array([(cos(int(y)), -sin(int(y))), (sin(int(y)), cos(int(y)))])
print(j.dot(w))
input(""enter to exit"")
</code></pre>
"
263,"Numpy Import ""Aborted""","<p>I am working on a linux server with a root installed <code>python2.7</code> (<code>/usr/bin/python2.7</code>). Then I tried to install Numpy into this by downloading the source and doing <code>python2.7 setup.py build; python2.7 setup.py install --user</code>. Numpy neatly installs to <code>~/.local/lib/python2.7/site-packages/numpy</code>. I get:</p>

<pre><code>$ python2.7
Python 2.7.2+ (default, Dec 22 2011, 12:26:43)
[GCC 4.4.5] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import numpy
Aborted
$
</code></pre>

<p>Where do I even start? I've never seen this before!</p>

<p>(FYI, the default python (2.6), has a working numpy install in <code>/usr/lib/pymodules/python2.6/numpy</code>)</p>

<p>As requested, a <strong>stacktrace</strong>, (and thanks for the instructions on that!). Whole thing <a href=""http://pastebin.com/psZDMB8q"" rel=""nofollow"">here on pastebin</a>.</p>

<pre><code>Program received signal SIGABRT, Aborted.
0x00002aaaabdb31b5 in *__GI_raise (sig=&lt;value optimized out&gt;)
    at ../nptl/sysdeps/unix/sysv/linux/raise.c:64
64  ../nptl/sysdeps/unix/sysv/linux/raise.c: No such file or directory.
    in ../nptl/sysdeps/unix/sysv/linux/raise.c
(gdb) bt
#0  0x00002aaaabdb31b5 in *__GI_raise (sig=&lt;value optimized out&gt;) at ../nptl/sysdeps/unix/sysv/linux/raise.c:64
#1  0x00002aaaabdb5fc0 in *__GI_abort () at abort.c:92
#2  0x00002aaab03fb9bd in free () from /usr/lib/python2.7/lib-dynload/_ctypes.so
#3  0x00002aaab03f8312 in ?? () from /usr/lib/python2.7/lib-dynload/_ctypes.so
#4  0x00002aaab03f8924 in ffi_closure_alloc () from /usr/lib/python2.7/lib-dynload/_ctypes.so
#5  0x00002aaab03f0af2 in _ctypes_alloc_callback () from /usr/lib/python2.7/lib-dynload/_ctypes.so
#6  0x00002aaab03eee68 in ?? () from /usr/lib/python2.7/lib-dynload/_ctypes.so
#7  0x00000000004b6ed5 in ?? ()
#8  0x0000000000425cdc in PyObject_Call ()
</code></pre>
"
264,C++ using E in a mathematical equation,"<p>I'm trying to solve aX<sup>2</sup> + bX + c = 0 but I can't seem to make it work with using the math header (which I'm not supposed to use).</p>

<pre><code>printf(""%E"",(-b+(b*b-4*a*c)E0.5)/2a);
</code></pre>
"
265,center-of-gravity class for a Java program,"<p>I will probably have to implement a center-of-gravity class but I will ask help in seeking such a Java class before I do.  I suspect this has been implemented by others as part of a math library.</p>

<p>In a space of n-dimensions, suppose each dimension is discrete.  So for example in 3 dimensions, you can have an X dimension with a range of <code>[0..a]</code>.  You also have a Y dimension with a range of <code>[0..b]</code> and a Z dimension with a range of <code>[0..c]</code>.  The implementation should be general so that the number of dimensions can be greater than 3 and also generally <code>a not equal to b</code> where <code>a</code> and <code>b</code> are the maximum coordinates of their respective dimensions.</p>

<p>Each point in the space is a double precision float (non-negative).</p>

<p>Find the coordinate of the center-of-gravity.</p>
"
266,"scatter plot with loess line, loess to not show line in a given region","<p>I plot here values over length for a chromosome</p>

<p><img src=""http://i.stack.imgur.com/7uTVy.png"" alt=""enter image description here""></p>

<p>The middle region without points contains no data and should not get a loess line.
How can I modify my code to stop the loess line over this region?
The data is continuous but I could add lines to mark the blank region with some special value or add a column with a label?? but how to use this in the command?</p>

<p>my current command: </p>

<pre><code>library(IDPmisc)

# plot settings (edit here)
spanv&lt;-0.05
pointcol1=""#E69F00""
pointcol2=""#56B4E9""
pointcol3=""#009E73""
points=20
linecol=""green""
xlabs=paste(onechr, "" position"", "" (loess-span="", spanv, "")"", sep="""")

data1&lt;-NaRV.omit(data[,c(2,7)]) # keep only x and y for the relevant data 
                                # and clean NA and Inf
ylabs='E / A - ratio'
p1&lt;-ggplot(data1, aes(x=start, y=E.R)) +
ylim(0,5) +
geom_point(shape=points, col=pointcol1, na.rm=T) +
geom_hline(aes(yintercept=1, col=linecol)) +
geom_smooth(method=""loess"", span=spanv, fullrange=F, se=T, na.rm=T) +
xlab(xlabs) +
ylab(ylabs)
</code></pre>
"
267,Compressing big number (or string) to small value,"<p>Friends,</p>

<p>Here is the situation: An ASP.NET page has following query string parameter.
MyServer.com/ShowSomething.aspx?IDs=1000000012,1000000021,1000000013,1000000022&amp;...</p>

<p>Here IDs parameter will always have numbers separated by something, in this case "","". Currently there are 4 numbers but normally they would be in between 3-7. </p>

<p>Now, I am looking for method to convert each big number from above into smallest possible value; specifically compressing value of <strong>IDs</strong> query string parameter. Both, compressing each number algorithm or compressing whole value of <strong>IDs</strong> query string parameter are welcome.</p>

<p>1) Encode or decode is not an issue; just compressing the value IDs query string parameter.
2) Creating some unique small value for IDs and then retrieving its value from some data source is out of scope.</p>

<p><strong>Please provide some algorithm to compress such big number to small value or algorithm to compress value of IDs query string parameter.</strong></p>
"
268,Offset Rotation Calculations in .NET2,"<p>I wondered if someone can help me out with this problem I have.   I am using .NET 2 environment.</p>

<p>I have 2 points at different locations on a 2D plane as two PointF</p>

<p>I have a known  rotation (in Degrees) at the first point, I need to know how to calculate what that rotation will be at the second ""offset"" point.</p>

<p>In other words, I want to rotate the second point by an unknown amount but the effect at the first point needs to be the known rotation value.</p>

<p>I have tried all sorts to solve this puzzle, my math and trig skills are rather rusty.  Can anyone help me out?</p>

<p>Many thanks.</p>
"
269,Completeness and MLE of a discrete distribution,"<p>The random variable X takes on the values 0, 1, or 2 according to the following probability
distribution:</p>

<p>$P(X = 0) = P(X = 1) = p^2, \ and P(X = 2) = 1 - 2p^2, for \ p \in C.$</p>

<p>(1) Determine whether this family of probability distributions is complete when C is the
open interval $(0, \frac{1}{\sqrt{2}})$.</p>

<p>(2) Suppose that C = $\lbrace \frac{1}{3}, \frac{2}{3}  \rbrace$, that is, the only two possible values of p are $\frac{1}{3}$ and $\frac{2}{3}$.
For a single X observation, find the maximum likelihood estimator, $\hat{p}_{MLE}$, for p.</p>

<p>For (1), I set $0 = E_p[g(x)] = g(0)p^2 + g(1)p^2 + g(2)(1-2p^2)$ </p>

<p>$= g(0)p^2 + g(1)p^2 - 2g(2)p^2 + g(2) = p^2 \Bigl(g(0) + g(1) - 2g(2)\Bigr) + g(2) $.  Thus $E_p[g(x)] = 0$ if $g(2) = 0$ and $g(1) = -g(0)$.  Therefore it is not complete.</p>

<p>For (2), I set up a table and got $\hat{\theta}_{x=0,1}=\frac{2}{3}$ and $\hat{\theta}_{x=2}=\frac{1}{3}$.</p>

<p>Not sure if I am simplifying this too much.  Any assistance is greatly appreciated.</p>
"
270,Creating an X number of nnet models programatically in R,"<p>I was hoping I could get some help on the following problem. Basically, I want to create a number of nnet models programatically based on the length of a particular vector, PredVector. Each value in PredVector indirectly refers to a column in my data set, PSTrain.</p>

<p>The code is as follows:</p>

<pre><code>PredVector &lt;- c(1, 3, 5)
for (i in 1:length(PredVector)) {
   modelName &lt;- paste(""nnModel"", PredVector[i], sep="""")
   modelForm &lt;- paste(""TPlus"", PredVector[i], ""~."", sep="""")
   as.formula(paste(modelName, ""&lt;- nnet("", modelForm, "", PSTrain, size=5, maxit=2000, linout=F)""))
}
</code></pre>

<p>I was hoping for three models to be created: nnModel1, nnModel3 and nnModel5. However, while the code successfully runs the nnet model three times at the desired settings, the models are not saved to my workspace.</p>

<p>Any ideas on how to solve this problem?</p>

<p>Thanks in advance!</p>
"
271,Determining The Best Arrangement Of Skills,"<p>I have no idea if this question is correct for this forum, but I figured I'd give it a shot. I am trying to figure out the best order to execute skills in a game (I know). Anyway here is the skill information:</p>

<pre><code>SkillName   SkillBar    CoolDown    ApproximateDuration Rank
Skill 1     2           40          2                   1
Skill 2     3           15          3.5                 2
Skill 3     3           7           3                   3
Skill 4     3           16          3.3                 4
Skill 5     3           12          3.9                 5
Skill 6     1           24          3.6                 6
Skill 7     1           10          1.7                 7
</code></pre>

<p>The Rank column indicates the priority of the skill (i.e. if it is off cool down the lowest rank (highest priority) skill should be run. What kind of math would be required to arrange these skills such that the skills are executed in the most efficient way? It's probably really simple and I will try to come up with something, but maybe one of you guys out there might find it interesting...?</p>

<p>Note: Each skill takes 'x' duration to execute (ApproximateDuration) which should be factored in.</p>

<p>Edit:</p>

<p>The following assumption(s) can be made at the beginning:</p>

<ul>
<li>All of the skills will be off cooldown (usable).</li>
</ul>

<p>Here is how I would start:</p>

<ol>
<li>Select all skills that are off cooldown and pull the lowest rank (highest priority) skill and run it.</li>
<li>It has now executed <strong>Skill 1</strong> which took 2 seconds to execute. Our total running time is 2 seconds and the cooldown on <strong>Skill 1</strong> begins.</li>
<li>Select all skills that are off cooldown and pull the lowest rank (highest priority) skill and run it.</li>
<li>It has now executed <strong>Skill 2</strong> which took 3.5 seconds to execute. Our total running time is 5.5 seconds and the cooldown on <strong>Skill 2</strong> begins. <strong>Skill 1</strong>'s cooldown is now at 38 seconds.</li>
<li>Repeat... sort of? I guess this is the algorithm?</li>
</ol>
"
272,"geom.point in ggplot2, conditional shape","<p>I'm putting together my first plot with ggplot2. I need to set a shape for values == 0. Here's my dataset and what I got so far :</p>

<pre><code>structure(list(Var1 = structure(c(1L, 2L, 3L, 4L, 5L, 6L, 7L,
8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 1L, 2L, 3L, 4L, 5L,
6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 1L, 2L, 3L,
4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 1L,
2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L,
16L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,
14L, 15L, 16L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,
12L, 13L, 14L, 15L, 16L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L,
10L, 11L, 12L, 13L, 14L, 15L, 16L, 1L, 2L, 3L, 4L, 5L, 6L, 7L,
8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 1L, 2L, 3L, 4L, 5L,
6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 1L, 2L, 3L,
4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 1L,
2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L,
16L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,
14L, 15L, 16L), .Label = c(""MD-1"", ""MD-2"", ""MD-3"", ""MD-4"", ""ME-1"",
""ME-2"", ""ME-3"", ""ME-4"", ""ME-5"", ""ME-6"", ""MF-1"", ""MF-2"", ""MF-4"",
""MF-6"", ""MF-7"", ""MF-8""), class = ""factor""), Var2 = structure(c(1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L,
4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 10L,
10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L,
10L, 10L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L,
11L, 11L, 11L, 11L, 11L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L,
12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L), .Label = c(""FD-1"", ""FD-2"",
""FD-5"", ""FD-6"", ""FD-7"", ""FE-2"", ""FE-3"", ""FE-4"", ""FE-5"", ""FE-6"",
""FF-1"", ""FF-2""), class = ""factor""), Freq = c(35L, 4L, 5L, 2L,
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L,
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L,
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 2L, 0L,
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 2L, 4L, 0L, 0L,
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
14L, 15L, 4L, 3L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
0L, 1L, 0L, 1L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
0L, 3L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L,
0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 13L, 2L, 5L, 7L, 0L, 0L, 0L,
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 2L, 0L, 0L, 1L, 1L)), .Names = c(""Var1"",
""Var2"", ""Freq""), row.names = c(NA, -192L), class = ""data.frame"")
</code></pre>

<p>Here's the base of my plot</p>

<pre><code>p &lt;- ggplot(mat.bub, aes(Var1, Var2)) 
p + geom_point(aes(size = Freq))
</code></pre>

<p>Now, how to set geom.point to a specific shape if Freq==0 ? Here's what I tried so far:</p>

<pre><code>p &lt;- ggplot(mat.bub, aes(Var1, Var2,size=Freq)) 
p + geom_point(aes(Var1[Freq==0], Var2[Freq==0]), colour=""black"", shape=3, size=5, na.rm = T)
</code></pre>

<p>Inspired from this answer : </p>

<p><a href=""http://stackoverflow.com/questions/8583594/modifying-the-shape-for-a-subset-of-points-with-ggplot2"">Modifying the shape for a subset of points with ggplot2</a></p>

<p>But I get an ""arguments imply differing number of rows: 162, 192"" error. Of course Var1 and Var2 are not numerical, that's what's different from the mtcars example.</p>

<p>How could I achieve this conditional shaping ? What am I missing ?</p>

<p>Thanx for any help !</p>
"
273,"Algorithm for spotting the ""big"" value changes in a list of measurements","<p>I have a sensor that measures the volume of a liquid.
This liquid will be consumed slowly and be refilled when is needed.
What I want to detect is the the times that this liquid is ""stolen"" or filled.
By stolen I mean sudden drop in the volume of the liquid. The opposite will be considered filling. Values taken from the sensor have smaller spikes that should be ignored given enough measurements that will help so. </p>

<p>Is there any statistics method (documentation) or programming algorithm (any language) or even better an sql function/query (any db) that does the above described scenario?</p>

<p><img src=""http://i.stack.imgur.com/DJtnR.png"" alt=""enter image description here""></p>
"
274,regex from first character to the end of the string,"<p>In a <a href=""http://stackoverflow.com/questions/15895050/using-gsub-to-extract-character-string-before-white-space-in-r"">related post</a> someone asked how to grab from beginning of string to first occurrence of a character.  I'd like to extend my own knowledge of regex by asking how to grab from a certain character of the string to the end.</p>

<p><em><strong>How could I use regex (not <code>strsplit</code>) with gsub to grab from the beginning of the first space to the end of the string?</em></strong></p>

<pre><code>dob &lt;- c(""9/9/43 12:00 AM/PM"", ""9/17/88 12:00 AM/PM"", ""11/21/48 12:00 AM/PM"")
</code></pre>

<p>Here I tried: <code>gsub("".*? "", """", dob)</code>  but it grabs from the last space not the first so I tried <code>gsub("".{1}? "", """", dob)</code> but it is overly greedy because of the period.</p>

<p>Final solution would be the same as:</p>

<pre><code>sapply(lapply(strsplit(dob, ""\\s+""), ""["", 2:3), paste, collapse="" "")
##[1] ""12:00 AM/PM"" ""12:00 AM/PM"" ""12:00 AM/PM""
</code></pre>

<p><strong>NOTE: R regex is not identical to regex in general</strong></p>
"
275,Can someone explain this: 0.2 + 0.1 = 0.30000000000000004?,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stackoverflow.com/questions/56947/how-is-floating-point-stored-when-does-it-matter"">How is floating point stored? When does it matter?</a>  </p>
</blockquote>



<p>Why does the following occur in the Python Interpreter?</p>

<pre><code>&gt;&gt;&gt; 0.1+0.1+0.1-0.3
5.551115123125783e-17
&gt;&gt;&gt; 0.1+0.1
0.2
&gt;&gt;&gt; 0.2+0.1
0.30000000000000004
&gt;&gt;&gt; 0.3-0.3
0.0
&gt;&gt;&gt; 0.2+0.1
0.30000000000000004
&gt;&gt;&gt; 
</code></pre>

<p>Why doesn't <code>0.2 + 0.1 = 0.3</code>?</p>
"
276,finding distribution $XY$ in bivariate normal distribution,"<p>suppose $(X,Y)\sim\mathcal{N}(0,0,1,1,\rho)$.
how can find distribution $$Z=XY$$
please explain completely</p>
"
277,Code/library for Poisson Disk Sampling in C/C++,"<p>I'm looking for a library that implements Poisson Disk Sampling in C or C++, or another one that would be easily translatable. Preferable not incredible long source, but if it is that's okay too.</p>
"
278,Adding a line break to code blocks in R Markdown,"<p>I am using the knitr package with R Markdown to create an HTML report. I am having some trouble keeping my code on separate lines when using '+'.</p>

<p>For example,</p>

<pre><code>```{r}
ggplot2(mydata, aes(x, y)) +
   geom_point()
```
</code></pre>

<p>will return the following the the HTML document</p>

<pre><code>ggplot2(mydata, aes(x, y)) + geom_point()
</code></pre>

<p>Normally this is fine, but the problem arises once I start adding additional lines, which I want to keep separate to make the code easier to follow. Running the following:</p>

<pre><code>```{r}
ggplot2(mydata, aes(x, y)) +
   geom_point() +
   geom_line() +
   opts(panel.background = theme_rect(fill = ""lightsteelblue2""),
        panel.border = theme_rect(col = ""grey""),
        panel.grid.major = theme_line(col = ""grey90""),
        axis.ticks = theme_blank(),
        axis.text.x  = theme_text (size = 14, vjust = 0),
        axis.text.y  = theme_text (size = 14, hjust = 1.3))
```
</code></pre>

<p>Will result in all the code coming out in one line, making it harder to follow:</p>

<pre><code>ggplot2(mydata, aes(x, y)) + geom_point() + geom_line() + opts(panel.background = theme_rect(fill = ""lightsteelblue2""), panel.border = theme_rect(col = ""grey""), panel.grid.major = theme_line(col = ""grey90""), axis.ticks = theme_blank(), axis.text.x  = theme_text (size = 14, vjust = 0), axis.text.y  = theme_text (size = 14, hjust = 1.3))
</code></pre>

<p>Any help in solving this would be greatly appreciated!</p>
"
279,How can I add a background grid using ggplot2?,"<p>I'd like to add background grid to the center of the plot and then hide the standard gridlines.  The corner points of the grid are stored in the pts data frame and I've tried using geom_tile, but it doesn't appear to use the specified points.  Thanks in advance for your help.</p>

<pre><code>library(ggplot2)  
pts &lt;- data.frame(
        x=c(170,170,170,177.5,177.5,177.5,185,185,185), 
        y=c(-35,-25,-15,-35,-25,-15,-35,-25,-15))  
ggplot(quakes, aes(long, lat)) + 
    geom_point(shape = 1) + 
    geom_tile(data=pts,aes(x=x,y=y),fill=""transparent"",colour=""black"") +
    opts(
        panel.grid.major=theme_blank(),
        panel.grid.minor=theme_blank()
    )
</code></pre>
"
280,What does the right parameter do when creating a histogram in R?,"<p>I am trying to figure out what the right parameter in the hist function in R does. The documentation is unfortunately unclear to someone without a deep understanding of statistics such as myself. </p>

<p>The documentation as <a href=""http://www.stat.ucl.ac.be/ISdidactique/Rhelp/library/base/html/hist.html"" rel=""nofollow"">stated online</a> is:</p>

<blockquote>
  <p>right logical; if TRUE, the histograms cells are right-closed (left open) intervals.</p>
</blockquote>

<p>What does it mean to be right-closed (or left open) intervals?</p>
"
281,ggplot2:scatterplots for all possible combinations of variables,"<p>I want to plot graphs for all possible combinations of variables. My code is below:</p>

<pre><code> set.seed(12345)
a &lt;- data.frame(Glabel=LETTERS[1:7],   A=rnorm(7, mean = 0, sd = 1),  B=rnorm(7, mean = 0, sd = 1),  C=rnorm(7, mean = 0, sd = 1))
T &lt;- data.frame(Tlabel=LETTERS[11:20], A=rnorm(10, mean = 0, sd = 1), B=rnorm(10, mean = 0, sd = 1), C=rnorm(10, mean = 0, sd = 1))

library(ggplot2)
for(i in 2:(ncol(a)-1))
{
 for(j in (i+1):ncol(a))
 {
  r &lt;- 0.08
  p &lt;- ggplot(data=a, mapping=aes(x=a[, i], y=a[, j])) + geom_point() + theme_bw()
  p &lt;- p + geom_text(data=a, mapping=aes(x=a[, i], y=a[, j], label=Glabel),
                 size=3, vjust=1.35, colour=""black"")
  p &lt;- p + geom_segment(data = T, aes(xend = T[ ,i], yend=T[ ,j]),
                    x=0, y=0, colour=""black"",
                    arrow=arrow(angle=25, length=unit(0.25, ""cm"")))
  p &lt;- p + geom_text(data=T, aes(x=T[ ,i], y=T[ ,j], label=Tlabel), size=3, vjust=0, colour=""red"")
dev.new()
  print(p)
} 
 }
</code></pre>

<p>This code works fine. But the method used here is not recommended (<a href=""http://stackoverflow.com/q/7452683/707145"">See @baptiste comment</a>) and does not work in function. I want to know what is the best and recommended way to accomplish this task. Thanks in advance for your help.</p>
"
282,Aggregate rows in a large matrix by rowname,"<p>I would like to aggregate the rows of a matrix by adding the values in rows that have the same rowname. My current approach is as follows:</p>

<pre><code>&gt; M
  a b c d
1 1 1 2 0
1 2 3 4 2
2 3 0 1 2
3 4 2 5 2
&gt; index &lt;- as.numeric(rownames(M))
&gt; M &lt;- cbind(M,index)
&gt; Dfmat &lt;- data.frame(M)
&gt; Dfmat &lt;- aggregate(. ~ index, data = Dfmat, sum)
&gt; M &lt;- as.matrix(Dfmat)
&gt; rownames(M) &lt;- M[,""index""]
&gt; M &lt;- subset(M, select= -index)
&gt; M
   a b c d
 1 3 4 6 2
 2 3 0 1 2
 3 4 2 5 2
</code></pre>

<p>The problem of this appraoch is that i need to apply it to a number of very large matrices (up to 1.000 rows and 30.000 columns). In these cases the computation time is very high (Same problem when using ddply). Is there a more eficcient to come up with the solution? Does it help that the original input matrices are DocumentTermMatrix from the tm package? As far as I know they are stored in a sparse matrix format.</p>
"
283,Generalize a query that involves multiple instances of three tables SQL,"<p>I am doing a formula to make some statistics in SQL, but I do not know how to improve a query I am working on...</p>

<p>I made a <a href=""http://sqlfiddle.com/#!2/d2ff6/9"" rel=""nofollow"">sqlfiddle so you can understand me better</a></p>

<p>So I have 3 tables and I need to solve  a formula varying the indexes, i,j... </p>

<pre><code>i j
---
1 1
1 2
1 3
2 1
2 2
2 3
3 1
3 2
3 3
</code></pre>

<p>and then do some sqrt and pow. I want the result in a table but I do not know how to generalize all those long queries into one...</p>
"
284,Numpy: Why is numpy.array([2]).any() > 1 False?,"<pre><code>&gt;&gt;&gt; import numpy
&gt;&gt;&gt; numpy.array([2]) &gt; 1
array([ True], dtype=bool)
&gt;&gt;&gt; numpy.array([2]).any() &gt; 1
False
</code></pre>

<p>Shouldn't any() test all elements of the array and return True?</p>
"
285,Simple scatter plot in R not working,"<p>I have a data frame ""myframe"":</p>

<pre><code>&gt; myframe
myframe
   exp obs
8    6  10
9    7  11
10   7  10
12   7   9
17   9   8
29   7   8
31   7   7
37   5   5
43   9  12
</code></pre>

<p>and I want to plot the two columns. </p>

<pre><code>plot(myframe$exp,myframe$obs)
</code></pre>

<p>and I get
<img src=""http://i.stack.imgur.com/Vhrqw.png"" alt=""enter image description here""></p>

<p>Why is this happening and how do I fix it?</p>

<p>Here is the output of dput(myframe):</p>

<pre><code>&gt; dput(myframe)
dput(myframe)
structure(list(exp = c(6L, 7L, 7L, 7L, 9L, 7L, 7L, 5L, 9L), obs = structure(c(1L, 
2L, 1L, 19L, 18L, 18L, 17L, 16L, 3L), .Label = c(""10"", ""11"", 
""12"", ""13"", ""14"", ""15"", ""16"", ""17"", ""18"", ""19"", ""20"", ""21"", ""22"", 
""23"", ""25"", ""5"", ""7"", ""8"", ""9"", ""b"", ""y""), class = ""factor"")), .Names = c(""exp"", 
""obs""), row.names = c(8L, 9L, 10L, 12L, 17L, 29L, 31L, 37L, 43L
), class = ""data.frame"")
&gt; 
</code></pre>
"
286,foreach within package function: does not work on first call,"<p>I am trying to add parallel computation option to an R (netresponse) package based on doMC and multicore. The script works ok, but only on the second trial. </p>

<p>To reproduce the bug, start R and run the script below. It gets stuck on the last line. After interrupting with ctrl-c I get a few messages of ""select: Interrupted system call"". Then, running the same script again will give the expected result without problems.</p>

<p>Is some further initialization needed to get this work properly already on the first run? Or any other tips?</p>

<p>thanks for your support,
- L</p>

<hr>

<pre><code>require(netresponse)
require(multicore)
require(doMC)   
registerDoMC(3)
print(getDoParWorkers())
res &lt;- foreach(i = 1:100, .combine = cbind, 
        .packages = ""netresponse"") %dopar% netresponse::vdp.mixt(matrix(rnorm(1000), 100, 10))
</code></pre>
"
287,Are annual physical examinations valuable?,"<p>It seems to be commonly accepted that having a physician do a physical exam every year is beneficial. However, others seem to have concluded otherwise, for example see the Oct 17, 2012 article in the Washington Post <a href=""http://www.washingtonpost.com/blogs/ezra-klein/wp/2012/10/17/are-general-physicals-pretty-much-useless/?hpid=z10"">Are general physicals pretty much useless?</a>, which cites (with my emphasis):</p>

<blockquote>
  <p>Researchers at the Cochrane Review looked at more than 16 studies with 182,880 patients – all of whom were offered a general check up, but only some of whom who accepted. Those patients were followed between four and 22 years, depending on the study, to look at death rates for each group.</p>
  
  <p>The big takeaway: <strong>“There was no effect on the risk of death, or on the risk of death due to cardiovascular diseases or cancer.”</strong></p>
</blockquote>

<p>My skepticism is two-fold, then.</p>

<p>First, for my own edification, is the purpose of physical examinations to reduce risk, or risk of death due to cardiovascular disease or cancer? Or does the annual physical exam have other purposes?</p>

<p>In any case, about the conclusion - is this an uncontroverted study by a reputable organization following established principles and drawing sound conclusions based on an unbiased sampling of observable phenomena?</p>
"
288,How to extract unique levels from 2 columns in a data frame in r,"<p>I have the data.frame   </p>

<pre><code>df&lt;-data.frame(""Site.1"" = c(""A"", ""B"", ""C""),
               ""Site.2"" = c(""D"", ""B"", ""B""),
               ""Tsim"" = c(2, 4, 7), 
               ""Jaccard"" = c(5, 7, 1))

#    Site.1 Site.2 Tsim Jaccard
#  1      A      D    2       5
#  2      B      B    4       7
#  3      C      B    7       1
</code></pre>

<p>I can get the unique levels for each column using</p>

<pre><code>top.x&lt;-unique(df[1:2,c(""Site.1"")])
top.x

# [1] A B
# Levels: A B C

top.y&lt;-unique(df[1:2,c(""Site.2"")])
top.y

# [1] D B
# Levels: B D
</code></pre>

<p>How do I get the unique levels for both columns and turn them into a vector i.e:</p>

<pre><code>v &lt;- c(""A"", ""B"", ""D"")
v
# [1] ""A"" ""B"" ""D""
</code></pre>
"
289,using Amazon-EC2 or Linode for hosting a debian server with R regularly extracting information from the web,"<p>I have hacked my NAS-01g into a debian server and use it to regularly download stock quote and earthquake information from the web. I was on a trip last week and I turned off my sever at home, but when I come back, I can no long gain access to the server. Considering re-configuring the server as very time-consuming, I am thinking of migrating my existing server to cloud.</p>

<p>I have a few requirements here:</p>

<ul>
<li>server on 24/7</li>
<li>use cron to hourly call R to extract data from somewhere, say yahoo finance</li>
<li>(optional) backup and encrypt my gmail account</li>
<li>(optional) host django server and I am learning to use it now</li>
</ul>

<p>I am thinking of using amazon-EC2 or Linode. I have tried amazon-EC2 a bit, but the pricing scheme seems very complicated for me, and I want the server to be as cheap as possible as it is not really that mission critical work. I wonder Linode is simpler for a non-system admin like me.</p>

<p>Hope my question won't be considered as off-topic here.</p>

<p>Thanks in advance.</p>
"
290,Longest string in numpy object_ array,"<p>I'm using a numpy object_ array to store variable length strings, e.g.</p>

<pre><code>a = np.array(['hello','world','!'],dtype=np.object_)
</code></pre>

<p>Is there an easy way to find the length of the longest string in the array without looping over all elements?</p>
"
291,Why is cpu performance frequently used as example for linear regression methods?,"<p>I've seen several lecture notes and books using prediciton of cpu performance as an example for linear regression methods. Why do they do this?</p>

<p>I would, for instance, expect there to be a non-linear relationship between cache size and performance. Once a program fits within the cache of the cpu, increasing the size further would probably not improve performance.</p>

<p><a href=""http://cs-people.bu.edu/dgs/courses/cs105/12spring/lectures/data_mining_estimation.pdf"" rel=""nofollow"">http://cs-people.bu.edu/dgs/courses/cs105/12spring/lectures/data_mining_estimation.pdf</a></p>
"
292,organize text on geom_point using geom_text,"<p>I have a line plot with some time points that are hard to distinguish by the coloring alone and I would therefore like to label the time points on the plot, but the labels overlap (see plot below) in a way where it is hard to read the labels.</p>

<p>The plot currently look like this,</p>

<p><img src=""http://i.stack.imgur.com/2bsBm.png"" alt=""current plot""></p>

<p>I wonder if there is a way to 'stack' the labels or some way (script) that can ensure they do not overlap. Something like this,</p>

<p><img src=""http://i.stack.imgur.com/cmGOX.png"" alt="" - - &gt; ""></p>

<p>Any help would be appreciated.</p>

<p>Here is the code I used to produce the plot,</p>

<pre><code> require(ggplot2)
 require(plyr)
 require(reshape)

# create sample data
set.seed(666)
dfn &lt;- data.frame(
Referral  = seq(as.Date(""2007-01-15""), len= 26, by=""23 day""),
VISIT01  = seq(as.Date(""2008-06-15""), len= 24, by=""15 day"")[sample(30, 26)],
VISIT02  = seq(as.Date(""2008-12-15""), len= 24, by=""15 day"")[sample(30, 26)],
VISIT03  = seq(as.Date(""2009-01-01""), len= 24, by=""15 day"")[sample(30, 26)],
VISIT04  = seq(as.Date(""2009-03-30""), len= 24, by=""60 day"")[sample(30, 26)],
VISIT05  = seq(as.Date(""2010-11-30""), len= 24, by=""6 day"")[sample(30, 26)],
VISIT06  = seq(as.Date(""2011-01-30""), len= 24, by=""6 day"")[sample(30, 26)],
Discharge = seq(as.Date(""2012-03-30""), len= 24, by=""30 day"")[sample(30, 26)],
Patient  = factor(1:26, labels = LETTERS),
openCase  = rep(0:1, 100)[sample(100, 26)])

 # set today's data for cases that do not have an Discharge date
 dfn$Discharge[ is.na(dfn$Discharge) ] &lt;- as.Date(""2014-01-30"")

 mdfn &lt;- melt(dfn, id=c('Patient', 'openCase'), variable_name = ""Visit"")
 names(mdfn)[4] &lt;- 'Year' # rename 

 # order data in mdfn by 'Referral' in dfn
 mdfn$Patient &lt;- factor(mdfn$Patient,levels = 
 (dfn$Patient[order(dfn$Referral)]),ordered = TRUE)

 # subset a dataset to avoid 'Discharge' for cases that are not closed 
 mdfn2 &lt;- subset(mdfn,!(Visit==""Discharge"" &amp; Year &gt; as.Date(""2014-01-01"")))

 # the plot as it looks now
 ggplot(mdfn, aes(Year, Patient)) +
     geom_blank() +
     geom_line(data = mdfn[mdfn$openCase == 0,], colour = ""black"") +
     geom_line(data = mdfn[mdfn$openCase == 1,], colour = ""grey"") +
     geom_point(data = mdfn2, aes(colour = Visit), size = 4, shape = 124) + 
     geom_text(data=mdfn2, mapping=aes(x=Year, y=Patient, 
     label=substr(Visit, 1, 7), colour=Visit), size=2, 
     vjust=-.4, hjust=-.1, angle = 00) 
</code></pre>
"
293,ggplot2 - printing plot balloons memory,"<p>Is it expected that printing a large-ish ggplot to PDF will cause the RSession memory to balloon?  I have a ggplot2 object that is around 72 megabytes.  My RSession grows to over 2 gig when printing to PDF.  Is this expected? Are there ways to optimize performance?  I find that the resulting PDFs are huge ~25meg and I have to use an external program to shrink them down (50kb with no visual loss!).  Is there a way to print to PDF with lower quality graphics?  Or perhaps some parameter to print or ggplot that I haven't considered?</p>
"
294,Request for Statistics textbook,"<p>I am looking for a textbook on Statistical Analysis. Unfortunately most of the books I have seen, such as Statistics by DeGroot et al., are quite the opposite of the terse and lean textbooks I prefer (such as any book by Milnor).</p>

<p>Can someone suggest to me an introductory or perhaps even intermediate statistics textbook which is under 300 pages. It can assume that I know measure theory but not much probability theory (though I doubt that would be necessary).  </p>

<p>The textbook should teach me enough statistical analysis as is required in an (Business/Financial) Analysts job.</p>

<p>Thank you for the suggestions.</p>
"
295,Selecting indices for a 2d array in numpy,"<p>This works quite well in 1 dimension:</p>

<pre><code># This will sort bar by the order of the values in foo
(Pdb) bar = np.array([1,2,3])
(Pdb) foo = np.array([5,4,6])
(Pdb) bar[np.argsort(foo)]
array([2, 1, 3])
</code></pre>

<p>But how do I do that in two dimensions?  Argsort works nicely, but the select no longer works:</p>

<pre><code>(Pdb) foo = np.array([[5,4,6], [9,8,7]])
(Pdb) bar = np.array([[1,2,3], [1,2,3]])
(Pdb)  bar[np.argsort(foo)]
*** IndexError: index (2) out of range (0&lt;=index&lt;=1) in dimension 0
(Pdb) 
</code></pre>

<p>I would expect this to output:</p>

<pre><code>array([[2, 1, 3], [3, 2, 1]])
</code></pre>

<p>Any clue how to do it?</p>

<p>Thanks!
/YGA</p>

<p>Edit: <code>take()</code> would seem to do the right thing, but it really only takes elements from the first row (super confusing).</p>

<p>You can see that if I change the values of bar:</p>

<pre><code>(Pdb) bar = np.array([[""1"",""2"",""3""], [""A"", ""B"", ""C""]])
(Pdb) bar.take(np.argsort(foo))
array([['2', '1', '3'],
       ['3', '2', '1']], 
      dtype='|S1')
(Pdb) 
</code></pre>
"
296,"Which are the best LGPL Libraries for Numerical Methods, Optimization Techniques, DOE, Sensitivity Analysis?","<p>I wish to perform some mathematical operations including Design of Experiments, Optimization Techniques, Sensitivity Analysis and Six-Sigma based Robust design for Aerospace Design project. I am developing applications for mechanical design engineers.</p>

<p>I understand there are a few libraries like DAKOTA from sandia national labs that help application developers with numerical methods. Is there any other library that you are aware of? It would be better if it can be used in commercial projects without any license violation (like LGPL).</p>

<p>Many Thanks.</p>
"
297,Few outliers not removed,"<p>I am working with vast amount of data which consists of outliers.The code works well with most of the dataset but does not work with few.</p>

<p>This sample data:</p>

<pre><code>set.seed(100)
m=rnorm(200)
m[1]=100   #inserting outlier
m[2]=50
</code></pre>

<p>My code is : </p>

<pre><code>library(outliers)
lg=outlier(m, logical=TRUE)
for(i in 1:length(lg)){
if(lg[i]==c(""TRUE"")){ 
 m[i]=NA }}
</code></pre>

<p>This replaces outliers with NAs.
Now in this case 100 is removed but 50 is not removed.
Same thing is happening with my dataset. I am not able to figure out why.
I wish to receive help on this.</p>

<p>Thank you for reading.</p>
"
298,"php, n range of number-price combinations between two given combinations","<p>I would need some more mathematical help as I'm creating a data array for a graph.</p>

<p>I'm programming in PHP.</p>

<p>I have value pairs which correspond to amount and price. I have the first and the last, but I would need n amount of pairs in between.</p>

<p>For example I have:</p>

<p>At amount 1.000 the price is 400.
At amount 10.000 the price is 800.</p>

<p>Now I would need for example 4 or 6 or 8 or n ... ""points"" in between that would form a linear line between the beginning and the ending values. These points should be evenly distributed between the two values of course.</p>

<p>Please help or at least give me some guidance..</p>

<p>Regards!</p>
"
299,"Can i make with java script math function , with input and output","<p>My idea is this simple , but i need to make some changes : </p>

<pre><code>  &lt;input type=""text""/&gt;
    &lt;p&gt;&lt;/p&gt;
</code></pre>

<hr>

<pre><code>$(""input"").keyup(function () {
 var value = $(this).val();
 var x=value/12;
 $(""p"").text(x);
}).keyup();
</code></pre>

<p>i need to make others input that take the x(output) of the first function and make y(second input) / x = result
.</p>

<pre><code> &lt;input2 type=""text""/&gt;
    &lt;p&gt;&lt;/p&gt;
$(""input2"").keyup(function () {
 var value = $(this).val();
 var x=value/12;
 $(""p"").text(x);
}).keyup();
</code></pre>
"
300,Heads or tails probability,"<p>I'm working on a maths exercise and came across this question.</p>

<p>The probability of a ""heads"" when throwing a coin twice is 2 / 3. This could be explained by the following:</p>

<p>• The first time is ""heads"". The second throw is unnecessary. The result is H;</p>

<p>• The first time is ""tails"" and twice ""heads"". The result is TH;</p>

<p>• The first time is ""tails"" and twice ""tails"". The result is TT;</p>

<p>The outcome: {H, TH, TT}. two of the three results include a ""heads"", it follows that the probability of a ""heads"" is 2/3 </p>

<p>What's wrong with this reasoning?</p>

<p>I think the answer is 1/2, is that right?</p>

<p>Ps. my first language isn't english,</p>

<p>Thanks Jef</p>
"
301,"Difference between 12Log2 and Log[2,12]?","<p>My math is pretty weak, and I'm having confusion over the differences. I'm trying to find out the midi formula, to output frequency when I have midi value</p>

<pre><code>MidiNumber = 69+12* Log2(440/Frequency)
</code></pre>

<p>So I derived
    Frequency = (-69 + 5280 Log2 + MidiNumber)/(12 Log2)</p>

<p>If I plugin things this works correctly
    440 = (-69 + 5280 Log2 + 69)/(12 Log2)</p>

<p>If I do this though things do not work correctly
    (-69 + Log[2, 5280.] + 69)/Log[2, 12.]</p>

<p>This is the output I get in my programming, I don't know exactly the difference between the two equations. Maybe it's 12*Log2, but is that 12*Log2[1] or, ...? No idea. </p>
"
302,Complex Roots of equation solved in Pyhton,"<p>I am trying to solve the following equation,</p>

<pre><code>def f(u1, u2, u3, u4, a11, a16, a12, a66, a26, a22):
    return a11*u4-2*a16*u3+(2*a12+a66)*u2-2*a26*u1+a22
</code></pre>

<p>where <code>u1</code> to <code>u4</code> are complex variables that I want the root for <code>f() = 0</code> and <code>a11</code> to <code>a66</code> are arguments(<code>floats</code>) that need to be passed into the function.  I have looked at <code>scipy.optimize.fsolve()</code> and <code>sympy</code> but couldn't get either method to work correctly.</p>
"
303,R extract time components from semi-standard strings,"<h3>Setup</h3>

<p>I have a column of durations stored as a strings in a dataframe. I want to convert them to an appropriate time object, probably <a href=""http://stat.ethz.ch/R-manual/R-devel/library/base/html/DateTimeClasses.html"" rel=""nofollow"">POSIXlt</a>. Most of the strings are easy to parse using <a href=""http://stackoverflow.com/questions/9022908/convert-list-vector-of-strings-with-date-format-into-posix-date-class-with-r"">this method</a>:</p>

<pre><code>&gt; data &lt;- data.frame(time.string = c(
+   ""1 d 2 h 3 m 4 s"",
+   ""10 d 20 h 30 m 40 s"",
+   ""--""))
&gt; data$time.span &lt;- strptime(data$time.string, ""%j d %H h %M m %S s"")
&gt; data$time.span
[1] ""2012-01-01 02:03:04"" ""2012-01-10 20:30:40"" NA
</code></pre>

<p>Missing durations are coded <code>""--""</code> and need to be converted to <code>NA</code> - this already happens but should be preserved.</p>

<p>The challenge is that <em>the string drops zero-valued elements</em>. Thus the desired value <code>2012-01-01 02:00:14</code> would be the string <code>""1 d 2 h 14 s""</code>. However this string parses to <code>NA</code> with the simple parser:</p>

<pre><code>&gt; data2 &lt;- data.frame(time.string = c(
+  ""1 d 2 h 14 s"",
+  ""10 d 20 h 30 m 40 s"",
+  ""--""))
&gt; data2$time.span &lt;- strptime(data2$time.string, ""%j d %H h %M m %S s"")
&gt; data2$time.span
[1] NA ""2012-01-10 20:30:40"" NA
</code></pre>

<h3>Questions</h3>

<ol>
<li>What is the ""R Way"" to handle all the possible string formats? Perhaps test for and extract each element individually, then recombine?</li>
<li>Is POSIXlt the right target class? I need duration free from any specific start time, so the addition of false year and month data (<code>2012-01-</code>) is troubling.</li>
</ol>

<h3>Solution</h3>

<p>@mplourde definitely had the right idea w/ dynamic creation of a formatting string based on testing various conditions in the date format. The addition of <code>cut(Sys.Date(), breaks='years')</code> as the baseline for the <code>datediff</code> was also good, but failed to account for a critical quirk in <code>as.POSIXct()</code> <em>Note: I'm using R2.11 base, this may have been fixed in later versions</em>.</p>

<p>The output of <code>as.POSIXct()</code> changes dramatically depending on whether or not a date component is included:</p>

<pre><code>&gt; x &lt;- ""1 d 1 h 14 m 1 s""
&gt; y &lt;-     ""1 h 14 m 1 s""  # Same string, no date component
&gt; format (x)  # as specified below
[1] ""%j d %H h %M m %S s""
&gt; format (y)
[1] ""% H h % M %S s""    
&gt; as.POSIXct(x,format=format)  # Including the date baselines at year start
[1] ""2012-01-01 01:14:01 EST""
&gt; as.POSIXct(y,format=format)  # Excluding the date baselines at today start
[1] ""2012-06-26 01:14:01 EDT""
</code></pre>

<p>Thus the second argument for the <code>difftime</code> function should be:</p>

<ul>
<li>The start of the first day of the current year if the input string <em>has</em> a day component</li>
<li>The start of the <em>current</em> day if the input string <em>does not</em> have a day component</li>
</ul>

<p>This can be accomplished by changing the unit parameter on the <code>cut</code> function:</p>

<pre><code>parse.time &lt;- function (x) {
  x &lt;- as.character (x)
  break.unit &lt;- ifelse(grepl(""d"",x),""years"",""days"")  # chooses cut() unit
  format &lt;- paste(c(if (grepl(""d"", x)) ""%j d"",
                    if (grepl(""h"", x)) ""%H h"",
                    if (grepl(""m"", x)) ""%M m"",
                    if (grepl(""s"", x)) ""%S s""), collapse="" "")

  if (nchar(format) &gt; 0) {
    difftime(as.POSIXct(x, format=format), 
             cut(Sys.Date(), breaks=break.unit),
             units=""hours"")
  } else {NA}

}
</code></pre>

<p>Thanks for the assistance!</p>
"
304,"Who uses R with multicore, SNOW or CUDA package for resource intense computing?","<p>who of you in this forum uses R (http://www.r-project.org/) with the multicore, SNOW or CUDA packages, so for advanced calculations that need more power than a workstation CPU? On which hardware do you compute these scripts? At home/ at work or do you have data center access somewhere?</p>

<p>The background of these questions is the following: I am currently writing my M.Sc. thesis about R and High-Performance-Computing and need a strong knowledge about who actually uses R. I read that R had 1 million users in 2008, bu thats more or less the only user statistics I could find on this topic - so I hope for your answers!</p>

<p>Sincerely Heinrich</p>
"
305,How can I add alpha-numeric AND greek characters to geom_text() in ggplot?,"<p>I am trying to create a plot that is annotated with text that contains both alpha-numeric and greek characters. If I want to add just greek characters I can use:</p>

<pre><code>qplot(x, y) + geom_text(aes(2, 2, label=""rho""), parse=TRUE)
</code></pre>

<p>How can I annotate a plot with ""&rho; and some other text""? I would like to do something like this:</p>

<pre><code>qplot(x, y) + geom_text(aes(2, 2, label=""rho and some other text""), parse=TRUE)
</code></pre>

<p>When I try the above code, I get this error:</p>

<pre><code>Error in parse(text = lab) : &lt;text&gt;:1:5: unexpected symbol
1: rho and
       ^
</code></pre>

<p>I would also appreciate any solution that would allow me to use LaTeX in <code>geom_text()</code> for more complex use cases in the future.</p>
"
306,High power and double precision,"<p>How do you solve the below equation in some programming language of your choice?</p>

<pre><code>(1-1/X)^Y
</code></pre>

<p>Easy!</p>

<p>But how about when X &amp; Y are very big and X>>Y</p>

<p>e.g.</p>

<pre><code>(1-1/X)^Y
where 
X = 10^40
Y = 10^12
</code></pre>

<p>Looks like it should be a simple enough problem, but getting around the double precision problem before applying the power is something I was not able to figure out.</p>
"
307,Calculate radius from coordinates and find all matches within the radius in a table,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stackoverflow.com/questions/1896213/sql-determine-if-one-coordinate-is-in-radius-of-another"">SQL: Determine if one coordinate is in radius of another</a>  </p>
</blockquote>



<p>In a table I have stored latitude and longitude (as floats). Now I want to query my table to find all rows which have coordinates within a given radius (which can ben 2km, 5km, 10km etc. etc.). I really don't have a clue how to achieve this so any help will be greatly appreciated. I'm not sure how query, because I think the result will be an endless set of coordinates representing the 'border' of the radius where I somehow have to tell my table to search for results within that 'border'.</p>
"
308,Arbitrary objects as argument for lattice functions,"<p>I do not understand how to handle objects, stored in a data.frame, in certain lattice plots. In the second plot I get the error msg bellow. Is it possible to get it to work? </p>

<pre><code>require(lattice)
require(latticeExtra)
data&lt;-data.frame(a=I(list(1,2,3)),b=factor(1:3))
ecdfplot(~a|b,data=data
             ,layout=c(1,3)              
             ,panel=function(x,...){
                print(x[[1]])
                panel.xyplot(x[[1]],.5,col=2)
              }
         )
 data&lt;-data.frame(a=I(list(diag(1,2,2),diag(1,2,2),diag(1,2,2))),b=factor(1:3))
 ecdfplot(~a|b,data=data
         ,layout=c(1,3)              
         ,panel=function(x,...){
            print(x[[1]][1,1])
            panel.xyplot(x[[1]][1,1],.5,col=2)
          }
     )


Error in prepanel.default.function(darg = list(give.Rkern = FALSE, n = 50,  : 
 (list) object cannot be coerced to type 'double'
</code></pre>
"
309,Ordering the bars of a stacked bar graph in ggplot from least to greatest,"<p>Is there a way to specify that I want the bars of a stacked bar graph in with ggplot ordered in terms of the total of the four factors from least to greatest?  (so in the code below, I want to order by the total of all of the variables)  I have the total for each x value in a dataframe that that I melted to create the dataframe from which I formed the graph.</p>

<p>The code that I am using to graph is:</p>

<pre><code>ggplot(md, aes(x=factor(fullname), fill=factor(variable))) + geom_bar()
</code></pre>

<p>My current graph looks like this:</p>

<p><a href=""http://i.minus.com/i5lvxGAH0hZxE.png"" rel=""nofollow"">http://i.minus.com/i5lvxGAH0hZxE.png</a></p>

<p>The end result is I want to have a graph that looks a bit like this:</p>

<p><a href=""http://i.minus.com/kXpqozXuV0x6m.jpg"" rel=""nofollow"">http://i.minus.com/kXpqozXuV0x6m.jpg</a></p>

<p>My data looks like this:</p>

<p><img src=""http://i.minus.com/izAmjF47yfsRQ.png"" alt=""data in chart""></p>

<p>and I melt it to this form where each student has a value for each category:</p>

<p><img src=""http://i.minus.com/i1rf5HSfcpzri.png"" alt=""melted data""></p>

<p>before using the following line to graph it</p>

<pre><code>       ggplot(data=md, aes(x=fullname, y=value, fill=variable), ordered=TRUE) + geom_bar()+ opts(axis.text.x=theme_text(angle=90)) 
</code></pre>

<p>Now, I'm not really sure that I understand the way Chi does the ordering and if I can apply that to the data from either of the frames that I have.  Maybe it's helpful that that the data is ordered in the original data frame that I have, the one that I show first.</p>

<p>UPDATE:  We figured it out.  See this thread for the answer:
<a href=""http://stackoverflow.com/questions/8186436/order-stacked-bar-graph-in-ggplot"">Order Stacked Bar Graph in ggplot</a></p>
"
310,Calculating y distance after 3d rotation?,"<p>I'll start off by saying that I'm quite bad in math,</p>

<p>I'm trying to calculate the distance between a circle and the center of the screen after rotating an image that contains that circle by 45 degrees in 3d,</p>

<p>(The y distance of the object changes as the image rotates)</p>

<p><img src=""http://i.stack.imgur.com/xYX75.jpg"" alt=""image""></p>

<p>I hope I made myself clear enough,
Thanks in advance!</p>
"
311,Gaussian blur image histogram of Y channel,"<p>I'm new to computer vision and image processing, anyway I'm trying to calculate the histogram of  image y_channel which has previously been blurred with cv2.GaussianBlur and converted from BGR to YCr-cb color space. However the end result isn't quite what I was expecting, it doesn't seems to have the typical look of a Gaussian distribution. The following is my image and plot.</p>

<p><img src=""http://i.stack.imgur.com/xBZRz.png"" alt=""face image""></p>

<p><img src=""http://i.stack.imgur.com/6LeRX.png"" alt=""histogram""></p>

<p>And this is the code snippet.</p>

<pre><code>    cv2.imwrite(""/home/carlo/face.png"", roi2)    
    img = cv2.imread('/home/carlo/face.png')
    yuma = cv2.split(img)[0]
    Hist = yuma.flatten().tolist()
    grayscales  = np.unique(Hist)
    frequencies = [Hist.count(x) for x in grayscales]
    plt.figure()
    plt.bar(grayscales,frequencies,color='g',edgecolor='k')
    plt.show()
</code></pre>

<p>Can anyone tell what I'm doing wrong?
Thanks</p>
"
312,Vim with R-plugin and LaTeX-Suite results in backslash in insert mode misbehaving,"<p>Here's what happens. I'm using Vim + LaTeX-Suite to edit TeX files in Vim. This could be in the Terminal or in MacVim.</p>

<p>I happily</p>

<pre><code>Insert lots of $\LaTeX \commands$ etc. I love using the $\backslash$. 
</code></pre>

<p>TeX works great. No problem.</p>

<p>Then I go and open up a .R file in the same window (different tab). R-Plugin for Vim uses the <code>&lt;Leader&gt;</code> key (mapped to <code>\</code> as per usual) to execute commands, e.g. I type <code>\sa</code> to send the selection to R and execute and move the window down. Life is nice.</p>

<p>Problem: even though while editing an R file, Vim is nice enough not to bug me in insert mode when I type <code>\</code>, for some reason when I switch back to the tab to edit the TeX file, then type <code>\</code> in insert mode, it moves the cursor <em>left</em> of the <code>\</code> and pauses as though waiting for the rest of the command, before then re-moving to the right of the <code>\</code> and moving on as I type. </p>

<p>Below shows what happens <em>just</em> from typing <code>\</code> in insert mode; obviously I could reproduce this by moving the cursor to the left with the arrow keys, but that's not how this happened--the cursor just moves left for a split second as though waiting for the R command to finish being input.</p>

<p><img src=""http://i.stack.imgur.com/7XgrQ.png"" alt=""""></p>

<p>So: how can I stop the annoying behavior in the TeX file insert mode, without sacrificing other functionality? Note, (a) I don't expect mapping <code>&lt;Leader&gt;</code> to a different key to help since then that key will just have the same left-cursor-move problem in TeX; (b), I like the leader as <code>\</code> anyway so I don't want to change it.</p>
"
313,probabilistic latent semantic analysis R,"<p>Is there a package that supports probabilistic latent semantic analysis for R? I found the LSA package, but is there one that specifically performs pLSA? Thanks.</p>
"
314,Boxed geom_text with ggplot2,"<p>I am developing a graphic with ggplot2 wherein I need to superimpose text over other graphical elements. Depending on the color of the elements underlying the text, it can be difficult to read the text. Is there a way to draw geom_text in a bounding box with a semi-transparent background?</p>

<p>I can do this with plotrix:</p>

<pre><code>library(plotrix)
Labels &lt;- c(""Alabama"", ""Alaska"", ""Arizona"", ""Arkansas"")
SampleFrame &lt;- data.frame(X = 1:10, Y = 1:10)
TextFrame &lt;- data.frame(X = 4:7, Y = 4:7, LAB = Labels)
### plotrix ###
plot(SampleFrame, pch = 20, cex = 20)
boxed.labels(TextFrame$X, TextFrame$Y, TextFrame$LAB,
 bg = ""#ffffff99"", border = FALSE,
 xpad = 3/2, ypad = 3/2)
</code></pre>

<p>But I do not know of a way to achieve similar results with ggplot2:</p>

<pre><code>### ggplot2 ###
library(ggplot2)
Plot &lt;- ggplot(data = SampleFrame,
 aes(x = X, y = Y)) + geom_point(size = 20)
Plot &lt;- Plot + geom_text(data = TextFrame,
 aes(x = X, y = Y, label = LAB))
print(Plot)
</code></pre>

<p>As you can see, the black text labels are impossible to perceive where they overlap the black geom_points in the background.</p>
"
315,Unable to plot kernel density in MASS package - In min(x) : no non-missing arguments to min; returning Inf,"<p>When trying to plot my data I get these error messages:</p>

<pre><code>1: In min(x) : no non-missing arguments to min; returning Inf
2: In max(x) : no non-missing arguments to max; returning -Inf
</code></pre>

<p>This reproducible code works:</p>

<pre><code># some dummy data
library(mvtnorm)
dat &lt;- rmvnorm(10000, mean=c(x=4,y=4), sigma=matrix(c(1,0.5,0.5,1), ncol=2))
dat &lt;- as.data.frame(dat)

# 2d density plot
library(MASS)
kdexy &lt;- kde2d(dat$x,dat$y, n=50)
image(kdexy, col=grey(seq(1,0.2,length=10)))
</code></pre>

<p>But using my real data doesn't:</p>

<pre><code>kdexy &lt;- kde2d(temp$V1, temp$V2, n=50)
image(kdexy, col=grey(seq(1,0.2,length=10)))
</code></pre>

<p>Yet the structure of the two data sets is the same (the dummy data [dat] and the real data [temp]):</p>

<pre><code>&gt; str(dat$x)
 num [1:80000] 0.669 0.609 -0.633 0.565 0.559 ...
&gt; str(temp$V1)
 num [1:823180] 0 0 0 0 0.0146 ...
&gt; summary(dat$x)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-4.2270 -0.1902  0.4841  0.4900  1.1600  5.3570 
&gt; summary(temp$V1)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-1.0000  0.0000  0.0000 -0.0289  0.0000  0.9844 
&gt; range(dat$x)
[1] -4.227400  5.357184
&gt; range(temp$V1)
[1] -1.000000  0.984375
&gt; str(temp$V2)
 num [1:823180] 1 1 15.5 15.5 18.5 ...
&gt; summary(temp$V2)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   1.00    1.00   18.45   23.55   35.96  116.10 
&gt; range(temp$V2)
[1]   1.0000 116.0829
</code></pre>

<p>They are both stored in dataframes and the only difference I'm aware of is the length, and that <code>temp$V1</code> is bounded at <code>-1</code> and <code>1</code>.</p>

<p>The output of <code>kdexy &lt;- kde2d()</code> differs between the two datasets. In the example data, the <code>Z</code> sections is populated with very small numbers; in the real dataset, every point is filled with 'NaN'.</p>
"
316,R - plotting multiple rasters using matrix layout,"<p>In R (Win64), I'm trying to plot a combination of raster images and histograms in a single plot window using the layout() command with a matrix defining the layout. Here's some sample code with simplified data:</p>

<pre><code>library(raster)

r &lt;- raster(ncols=5, nrows=5, xmn=1, xmx=5, ymn=1, ymx=5)
rast1 &lt;- rasterize(expand.grid(1:5,1:5), r, rnorm(25))
rast2 &lt;- rasterize(expand.grid(1:5,1:5), r, rnorm(25))
rast3 &lt;- rasterize(expand.grid(1:5,1:5), r, rnorm(25))

layout(matrix(c(1,2,3,4,1,2,3,5,1,2,3,6), 3, 4, byrow=T))
layout.show(6)

plot(rast1, axes=F, ann=F, legend=F, box=F, useRaster=T)
plot(rast2, axes=F, ann=F, legend=F, box=F, useRaster=T)
plot(rast3, axes=F, ann=F, legend=F, box=F, useRaster=T)
hist(rnorm(100), ann=F, axes=F)
hist(rnorm(100), ann=F, axes=F)
hist(rnorm(100), ann=F, axes=F)
</code></pre>

<p>As you can see, I'm trying to plot three raster images (rast1, rast2, rast3) that span 1 column and 3 rows each, with 3 histograms beside them, each of which spans 1 column and 1 row. The layout.show() command gives the idea.</p>

<p>When I run this code, it seems like the first plot (raster) command also resets the layout of the plot window, causing all subsequent plots to plot in a standard 3x4 grid (with the 5th plot now overlapping the first). The layout setup seems sound, as I can plot six histograms in the proper layout. But the raster plots mess things up.</p>

<p>I suspect there is something about the plot() command in {raster} that is messing with the layout() command, but I have no idea why or how. Is there some other way to achieve this layout? Another raster-based command? Is there some way to reset the layout between raster plots?</p>

<p>Thanks in advance.</p>
"
317,Mathematical equation manipulation in Python,"<p>Actually I want to develop a GUI application which displays a given mathematical equation. And when you click upon a particular variable in the equation to signify that it is the unknown variable ie., to be calculated. The equation transforms itself to evaluate the required unknown variable.</p>

<p>For example:</p>

<hr>

<pre><code>a = (b+c*d)/e
</code></pre>

<p><i>Let us suppose that I click upon ""d"" to signify that it is the unknown variable. Then the equation should be re-structured to:</i></p>

<pre><code>d = (a*e - b)/c
</code></pre>

<hr>

<p>As of now, I just want to know how I can go about rearranging the given equation based on user input. One suggestion I got from my brother was to use pre-fix/post-fix notational representation in back end to evaluate it.</p>

<p>Is that the only way to go or is there any simpler suggestion?
Also, I will be using not only basic mathematical functions but also trignometric and calculus (basic I think. No partial differential calculus and all that) as well. I think that the pre/post-fix notation evaluation might not be helpful in evaluation higher mathematical functions. </p>

<p>But that is just my opinion, so please point out if I am wrong.
Also, I will be using <a href=""http://en.wikipedia.org/wiki/SymPy"" rel=""nofollow"">SymPy</a> for mathematical evaluation so evaluation of a given mathematical equation is not a problem, creating a specific equation from a given generic one is my main problem.</p>
"
318,why is this $\bar X$ a.s. irrational?,"<p>Suppose that $X_1, X_2,\cdots, X_n$ are iid random variables from $N(\theta,1)$, $\theta$ is rational. Then we know that $\bar X \sim N(\theta,1/n)$. It is said that $\bar X$ is almost surely irrational. I am wondering why is it irrational a.s.? How can I interpret it?</p>
"
319,Need a way to work out a rank from this data,"<p>I'm stuck - this is probably pretty remedial for you, but hey, you'll help me out.</p>

<p>I need a way to ""score"" a difference in two numbers. The first number is the amount of deaths and the second number is the amount spent on trying to stop the deaths.</p>

<p>I need a way to rank these. So the less money spent and the more deaths the worse.</p>

<pre><code>Deaths | Money  | Rank
20     | 0      | High
10     | 0      | Med
0      | 0      | Low

20     | 100000 | Med/Low: high death count but they're spending lots of money
20     | 50000  | Med: high death count but there's room for more spending
5      | 100000 | Low: low deaths and lots of money spent
5      | 50000  | Low/Med: low deaths but room for more money spent.
</code></pre>

<p>I don't care which way round this works. If a high rank is a small number its fine. I don't mind, as long as it can rank them.</p>

<p>So to summarise: The higher the deaths the higher they should rank and the lower the amount spent the higher they should rank. Thus those with the most POTENTIAL (higher deaths and less money) come top and those with the least potential (lowest deaths and least money spent) come bottom.</p>

<p>Please help!</p>

<p>Thanks a lot.</p>

<p>PS: not sure how to tag this!</p>
"
320,Editing cell entries in a variable in a data frame inside a list of data frames,"<p>Define: </p>

<pre><code>&gt; dats &lt;- list( df1 = data.frame(a=sample(1:3), b = as.factor(rep(""325.049072M"",3))),
+       df2 = data.frame(a=sample(1:3), b = as.factor(rep(""325.049072M"",3))))
&gt; dats
$df1
  a           b
1 3 325.049072M
2 2 325.049072M
3 1 325.049072M

$df2
  a           b
1 2 325.049072M
2 1 325.049072M
3 3 325.049072M
</code></pre>

<p>I want to remove the M character from column b in each data frame.</p>

<p>In a simple framework:</p>

<pre><code>&gt; t&lt;-c(""325.049072M"",""325.049072M"")
&gt; t
[1] ""325.049072M"" ""325.049072M""
&gt; t &lt;- substr(t, 1, nchar(t)-1)
&gt; t
[1] ""325.049072"" ""325.049072""
</code></pre>

<p>But in a nested one, how to proceed?  Here is one sorry attempt:</p>

<pre><code>&gt; dats &lt;- list( df1 = data.frame(a=sample(1:3), b = as.factor(rep(""325.049072M"",3))),
+       df2 = data.frame(a=sample(1:3), b = as.factor(rep(""325.049072M"",3))))
&gt; dats
$df1
  a           b
1 3 325.049072M
2 1 325.049072M
3 2 325.049072M

$df2
  a           b
1 2 325.049072M
2 3 325.049072M
3 1 325.049072M

&gt; for(i in seq(along=dats)) {
+   dats[[i]][""b""] &lt;- 
+           substr(dats[[i]][""b""], 1, nchar(dats[[i]][""b""])-1)
+ }
&gt; dats
$df1
  a         b
1 3 c(1, 1, 1
2 1 c(1, 1, 1
3 2 c(1, 1, 1

$df2
  a         b
1 2 c(1, 1, 1
2 3 c(1, 1, 1
3 1 c(1, 1, 1
</code></pre>
"
321,How do I add values in an array when there is a null entry?,"<p>I want to create a real time-series array.  Currently, I am using the statistics gem to pull out values for each 'day':</p>

<pre><code>define_statistic :sent_count, :count
=&gt; :all, :group =&gt; 'DATE(date_sent)',    
:filter_on =&gt; {:email_id =&gt; 'email_id
&gt; = ?'}, :order =&gt; 'DATE(date_sent) ASC'
</code></pre>

<p>What this does is create an array where there are values for a date, for example</p>

<pre><code>[[""12-20-2010"",1], [""12-24-2010"",3]]
</code></pre>

<p>But I need it to fill in the null values, so it looks more like:</p>

<pre><code>[[""12-20-2010"",1], [""12-21-2010"",0], [""12-22-2010"",0], [""12-23-2010"",0], [""12-24-2010"",3]]
</code></pre>

<p>Notice how the second example has ""0"" values for the days that were missing from the first array. </p>
"
322,Function to rotate a point around another point,"<p>so I've got a math function here that <em>supposed</em> to return a rotated point, and takes an original point, point to rotate around (origin) and radians to rotate it. </p>

<p>However it rotating only at half speed (aka 180 degree movement = 90 degree rotation)</p>

<pre><code>sf::Vector2f RotatePoint(sf::Vector2f origin, sf::Vector2f point, float radian) {   
    float s = sin(radian);   
    float c = cos(radian);  

    // translate point back to origin:  
    point.x -= origin.x;   
    point.y -= origin.y;   

    // rotate point   
    float xnew = point.x * c - point.y * s;   
    float ynew = point.x * s + point.y * c; 

    // translate point back to global coords:
    sf::Vector2f TranslatedPoint;
    TranslatedPoint.x = xnew + origin.x;  
    TranslatedPoint.y = ynew + origin.y; 

    return TranslatedPoint;
} 
</code></pre>
"
323,condition on non-commutative Khinchine inequality,"<p>Let $\epsilon=(\epsilon_1,\ldots \epsilon_M)$ be Rademacher sequence. And let $B_j, j=1, \ldots, M$ be complex valued random matrices of the same dimension. Choose $n\in \mathbb{N}$. Then, the non-commutative Khinchine inequality states that
$$
E(\|\sum_{j=1}^M\epsilon_jB_j\|^{2n}_{S_{2n}})\leq \frac{(2n)!}{2^nn!}\max\{\|(\sum_{j=1}^MB_jB^*_j)^{1/2}\|^{2n}_{S_{2n}}, \|(\sum_{j=1}^MB^*_jB_j)^{1/2}\|^{2n}_{S_{2n}}\},
$$
where $\|\cdot\|_{S_{2n}}$ is a <a href=""http://en.wikipedia.org/wiki/Schatten_norm"" rel=""nofollow"">Schatten norm</a>, and $B_jB^*_j$ together with $B_j^*B_j$ are positive self-adjoint matrices.</p>

<p>I am curious, if this is a critical condition for Rademacher random variables $\epsilon_j, j=1, \ldots, M$ been independent. I mean, would it make science if one can consider some dependence between $\epsilon_j$, say sum of them is equal to 1.</p>

<p>Thank you.</p>
"
324,Python (numpy): drop columns by index,"<p>I've got a numpy array and would like to remove some columns based on index. Is there an in-built function for it or some elegant way for such an operation?</p>

<p>Something like: </p>

<pre><code>arr = [234, 235, 23, 6, 3, 6, 23]
elim = [3, 5, 6]

arr = arr.drop[elim]

output: [234, 235, 23, 3]
</code></pre>
"
325,"Why does 65.6*100%10 equals to 9, instead of 0 in PHP?","<pre>echo 65.7 * 100 % 10 // 0

echo 65.6 * 100 % 10 // 9 &lt;---

echo 6560 % 10       // 0

echo 65.5 * 100 % 10 // 0
</pre>

<p>Can someone please explain why?</p>

<p><strong>EDIT:</strong></p>

<p>for human or non-programmers, the result 9 is apprently wrong.</p>

<p>how can I prevent this <code>""error""</code> when programming?</p>
"
326,"In R, How Do I Create a Data Frame with Unique Values from One Column of another Data Frame?","<p>I'm trying to learn R, but I'm stuck on something that seems simple. I know SQL, and the easiest way for me to communicate my question is with that language. Can someone help me with a translation from SQL to R?</p>

<p>I've figured out that this:</p>

<pre><code>    SELECT col1, sum(col2) FROM table1 GROUP BY col1
</code></pre>

<p>translates into this:</p>

<pre><code>    aggregate(x=table1$col2, by=list(table1$col1), FUN=sum)
</code></pre>

<p>And I've figured out that this:</p>

<pre><code>    SELECT col1, col2 FROM table1 GROUP BY col1, col2
</code></pre>

<p>translates into this:</p>

<pre><code>    unique(table1[,c(""col1"",""col2"")])
</code></pre>

<p>But what is the translation for this?</p>

<pre><code>    SELECT col1 FROM table1 GROUP BY col1
</code></pre>

<p>For some reason, the ""unique"" function seems to switch to a different return type when working on only one column, so it doesn't work as I would expect.</p>

<p>-TC</p>
"
327,R-convert day-of-year to datetime classes,"<p>I have a big csv file where the first column is the day of the year and the second column is the time in minutes (ie. 00:00): year of measurement=2010</p>

<pre><code>1,0,2.701,104.3,5.02,2.985,13.71,645.97,1018.9,6.249,4.166,.501 
1,1,2.701,104.3,5.02,2.985,13.71,645.97,1018.9,6.249,4.166,.501 
1,2,2.737,104.3,5.228,3.006,13.71,645.97,1018.9,6.249,4.166,.518
1,3,2.805,104.3,4.958,3.027,13.71,645.97,1018.8,7.08,3.749,.767
1,4,2.821,104.3,4.999,3.006,13.71,645.97,1018.9,7.08,4.166,.79
1,5,2.847,104.2,5.208,3.048,13.72,645.97,1018.9,6.249,4.166,.567
1,6,2.881,104.2,4.853,3.027,13.71,645.97,1018.7,6.666,4.166,.688
</code></pre>

<p>I want to merge column 1 and 2 using datetime classes in R such that I get one single column starting with 2010-01-01 00:00:00</p>

<p>Cheers,
Navin</p>
"
328,Drawing a barplot for a range of values containing positive and negative numbers,"<p>I wish to draw a barplot for the values ranging between -0.5 to 0.5, and am using the code typed below:  </p>

<pre><code>barplot(c(-0.08,0.02,-0.06,-0.07,-0.07,0.46), names.arg=c('a','b','c','d','e','f'),ylim=c(-0.5,0.5), col=""dodgerblue4"", xpd = FALSE,axes=TRUE,cex.names=1,
axis.lty=1, ylab="""",space=2,las = 2,tck=-0.02,cex.axis=0.6,mgp=c(1, .3, 0))

box()
</code></pre>

<p>However I also want to add some base-line running horizontally at <code>y=0.0</code> to avoid the bars hanging loosely inside the box. Can I do this inside barplot function?</p>
"
329,Cannot save gglot2 graph with ggsave on Mac,"<p>After sucessfully creating a graph with this code:</p>

<pre><code>qplot(ethcat3, Percent, data=g_02_1b_pov, geom=""bar"", stat=""identity"")
</code></pre>

<p>I would like to save the graph as a png, pdf, or jpeg file. I used following code:</p>

<pre><code>ggsave(ﬁle=""test.png"") 
</code></pre>

<p>Instead of outputting the graph, I receive following (error) message:</p>

<pre><code>Saving 7.82 x 3.75 in image
Error in eval(expr, envir, enclos) : attempt to apply non-function
</code></pre>

<p>and no file is saved. (Instead of png, I also tried the extensions pdf, and jpeg).</p>

<p>I found in a forum that I should use the command: dev_mode() and has_devel()</p>

<p>the output of dev_mode is:</p>

<pre><code>&gt; dev_mode()
Dev mode: OFF
</code></pre>

<p>and</p>

<pre><code>&gt; has_devel()
/Library/Frameworks/R.framework/Resources/bin/R --vanilla CMD SHLIB foo.c 

sh: make: command not found
Error: Command failed (1)
</code></pre>

<p>However, I don't know what to do with this feedback?</p>

<p>Can anyone help me with this problem? I think I used the same approach in the past on a PC and it worked without a problem. I'm wondering if it is connected to my installation on a mac?</p>

<p>Thanks a lot!</p>

<p>Addition: @DWin asked for the <code>sessionInfo()</code> output:</p>

<pre><code>d&gt; sessionInfo() 
R version 2.15.1 (2012-06-22)
Platform: x86_64-apple-darwin9.8.0/x86_64 (64-bit)

locale:
[1] C/en_US.UTF-8/C/C/C/C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] devtools_0.8    reshape2_1.2.1  plyr_1.7.1      ggplot2_0.9.2.1

loaded via a namespace (and not attached):
 [1] MASS_7.3-18        RColorBrewer_1.0-5 RCurl_1.95-3       colorspace_1.2-0  
 [5] dichromat_1.2-4    digest_0.5.2       evaluate_0.4.2     grid_2.15.1       
 [9] gtable_0.1.1       httr_0.2           labeling_0.1       memoise_0.1       
[13] munsell_0.4        parallel_2.15.1    proto_0.3-9.2      scales_0.2.2      
[17] stringr_0.6.1      tools_2.15.1       whisker_0.1    
</code></pre>
"
330,How to change the date format in R,"<p>I have some date format as follow:</p>

<pre><code>        V1  V2   V3
1 20100420 915   120
2 20100420 920   150
3 20100420 925   270
4 20100420 1530  281
</code></pre>

<p>3 columns per row, the 1th row means: 2010-04-20 09:15 120</p>

<p>now I want to change it to 1 column(time series):</p>

<pre><code>                   V3
1 20100420 09:15   120
2 20100420 09:20   150
3 20100420 09:25   270
4 20100420 15:30   281
</code></pre>

<p>or:</p>

<pre><code>                   V3
1 20100420 9:15    120
2 20100420 9:20    150
3 20100420 9:25    270
4 20100420 15:30   281
</code></pre>

<p>How could I achieve it in R?</p>
"
331,slicing a numpy array,"<p>I'm reading <a href=""http://scipy-lectures.github.com/"" rel=""nofollow"">this</a> tutorial to get started in Python and I have this simple question!</p>

<p>The exercise in the tutorial says:</p>

<blockquote>
  <p>Generate a 10 x 3 array of random numbers (in range [0,1]). For each row, pick the number closest to 0.5.</p>
  
  <p>Use abs and argsort to find the column j closest for each row.</p>
  
  <p>Use fancy indexing to extract the numbers. (Hint: a[i,j] – the array i must contain the row numbers corresponding to stuff in j.)</p>
</blockquote>

<p>So I did everything but I feel the slicing method I used (and the initialisation of <code>b</code>) is not Pythonic at all:</p>

<pre><code>In [155]: a = np.random.rand(10,3)

In [156]: mask = np.argmin(abs(a-0.5), axis = 1)

In [155]: b = np.ones(mask.size)

In [155]: for j in range(0,mask.size):
    b[j] = a[j,mask[j]]
</code></pre>

<p>What is the other way of doing this without using the <code>for</code> loop.</p>
"
332,"recommended online resources for customizing R plots, especially for statistics?","<p>Could someone please recommend some online tutorials or pages that show you how to customize R plots, especially for customizing plots related to statistics and statistical graphics? Things like heatmaps, scatter plots, dendrograms, etc. - a resource that explains how to get these plots and tweak their layout, axis, spacing, labeling and so on.</p>

<p>Tutorials rich in examples with code would be best. thank you.</p>
"
333,Get percentiles of data-set with group by month,"<p>I have a SQL table with a whole load of records that look like this:</p>

<pre><code>| Date       | Score |
+ -----------+-------+
| 01/01/2010 |     4 |
| 02/01/2010 |     6 |
| 03/01/2010 |    10 |
  ...
| 16/03/2010 |     2 |
</code></pre>

<p>I'm plotting this on a chart, so I get a nice line across the graph indicating score-over-time.  Lovely.</p>

<p>Now, what I need to do is include the average score on the chart, so we can see how that changes over time, so I can simply add this to the mix:</p>

<pre><code>SELECT 
    YEAR(SCOREDATE) 'Year', MONTH(SCOREDATE) 'Month',
    MIN(SCORE) MinScore, 
    AVG(SCORE) AverageScore, 
    MAX(SCORE) MaxScore
FROM SCORES
GROUP BY YEAR(SCOREDATE), MONTH(SCOREDATE) 
ORDER BY YEAR(SCOREDATE), MONTH(SCOREDATE) 
</code></pre>

<p>That's no problem so far.</p>

<p>The problem is, how can I easily calculate the percentiles at each time-period?  I'm not sure that's the correct phrase.  What I need in total is:</p>

<ul>
<li>A line on the chart for the score (easy)</li>
<li>A line on the chart for the average (easy)</li>
<li>A line on the chart showing the band that 95% of the scores occupy (stumped)</li>
</ul>

<p>It's the third one that I don't get.  I need to calculate the 5% percentile figures, which I can do singly:</p>

<pre><code>SELECT MAX(SubQ.SCORE) FROM
    (SELECT TOP 45 PERCENT SCORE 
    FROM SCORES
    WHERE YEAR(SCOREDATE) = 2010 AND MONTH(SCOREDATE) = 1
    ORDER BY SCORE ASC) AS SubQ

SELECT MIN(SubQ.SCORE) FROM
    (SELECT TOP 45 PERCENT SCORE 
    FROM SCORES
    WHERE YEAR(SCOREDATE) = 2010 AND MONTH(SCOREDATE) = 1
    ORDER BY SCORE DESC) AS SubQ
</code></pre>

<p>But I can't work out how to get a table of all the months.</p>

<pre><code>| Date       | Average | 45% | 55% |
+ -----------+---------+-----+-----+
| 01/01/2010 |      13 |  11 |  15 |
| 02/01/2010 |      10 |   8 |  12 |
| 03/01/2010 |       5 |   4 |  10 |
  ...
| 16/03/2010 |       7 |   7 |   9 |
</code></pre>

<p>At the moment I'm going to have to load this lot up into my app, and calculate the figures myself.  Or run a larger number of individual queries and collate the results.</p>
"
334,"Map/Reduce: any theoretical foundation beyond ""howto""?","<p>For a while I was thinking that you just need a map to a monoid, and then reduce would do reduction according to monoid's multiplication.</p>

<p>First, this is not exactly how monoids work, and second, this is not exactly how map/reduce works in practice.</p>

<p>Namely, take the ubiquitous ""count"" example. If there's nothing to count, any map/reduce engine will return an empty dataset, not a neutral element. Bummer.</p>

<p>Besides, in a monoid, an operation is defined for two elements. We can easily extend it to finite sequences, or, due to associativity, to finite ordered sets. But there's no way to extend it to arbitrary ""collections"" unless we actually have a sigma-algebra.</p>

<p>So, what's the theory? I tried to figure it out, but I could not; and I tried to go google it but found nothing.</p>
"
335,Transformation of beta distribution into gamma distribution,"<p>How can I convert a Beta Distribution to a Gamma Distribution?
Strictly speaking, I want to transform parameters of a Beta Distribution to parameters of the corresponding Gamma Distribution. I  have mean value, alpha and beta parameters of a Beta Distribution and I want to transform them to those of a Gamma Distribution.</p>
"
336,How to format a pivot like table in R that includes records for all time and id values?,"<p>I'm collecting logs in R with 3 columns:<br>
week, probe, and number of observations.<br>
There aren't record when there is no observation.</p>

<pre><code>week=c(1,2,2,4)  
probe=c(""A"",""C"",""B"",""C"")  
obs=c(2,4,3,1)
logs=data.frame(week,probe,obs)

logs

week probe obs  
1     A   2
2     C   4
2     B   3
4     C   1
</code></pre>

<p>I want to reformat the data so that it includes all weeks and all probes even if there was no observation, so that it looks like this:</p>

<pre><code>week probe obs  
1     A   2  
1     B   0  
1     C   0  
1     D   0  
2     A   0  
2     B   0  
2     C   3  
2     D   4  
3     A   0  
3     B   0  
3     C   0  
3     D   0  
4     A   0  
4     B   0  
4     C   1  
4     D   0  
</code></pre>

<p>I have the list of all probes here:  </p>

<pre><code>allprobes=c(""A"",""B"",""C"",""D"")
</code></pre>

<p>and I want to look at these weeks:  </p>

<pre><code>allweeks=c(1:4)
</code></pre>

<p>I've been looking at melt, cast, reshape, but I only manage to get 1 line per id or month... 
as I actually want to keep the original format of the logs. 
It seems easy enough at first but I'm now stuck...
Any advice on how to get the data formatted this way?</p>

<p>Thanks a lot for any help.</p>
"
337,create new variables on string contents,"<p>If I have these strings:</p>

<pre><code>dat &lt;- data.frame(xxs = c(""PElookx.PElookxstd"",""POaftGx.POlookGxstd""))
</code></pre>

<p>how can I create a new variable where for instance if the string contains <code>PE</code> I want <code>NOW</code> or <code>PO</code> I would get <code>LATER</code></p>

<pre><code>newxxs &lt;- (`NOW`,`LATER`)
</code></pre>

<p>I kind of know how to use grep to do this:</p>

<pre><code>dat$newxss &lt;- NA
dat$newxss[grep(""PE"",dat$xxs)] &lt;- ""NOW""
dat$newxss[grep(""PO"",dat$xxs)] &lt;- ""LATER""
</code></pre>

<p>Is there a easier way than lots of <code>grep</code>s? As I will have to do this for multiple bits of strings for the same new column and for many new columns.</p>
"
338,How can I improve the efficiency of this numpy loop,"<p>I've got a numpy array containing labels. I'd like to get calculate a number for each label based on its size and bounding box. How can I write this more efficiently so that it's realistic to use on large arrays (~15000 labels)?</p>

<pre><code>A = array([[ 1, 1, 0, 3, 3],
           [ 1, 1, 0, 0, 0],
           [ 1, 0, 0, 2, 2],
           [ 1, 0, 2, 2, 2]] )

B = zeros( 4 )

for label in range(1, 4):
    # get the bounding box of the label
    label_points = argwhere( A == label )
    (y0, x0), (y1, x1) = label_points.min(0), label_points.max(0) + 1

    # assume I've computed the size of each label in a numpy array size_A
    B[ label ] = myfunc(y0, x0, y1, x1, size_A[label])
</code></pre>
"
339,"How to convert get Days,Weeks,Months,Years from Days(in terms of daily savings)","<p>So I am working on a little savings app for Android. 
I have three values thats I need to work with.
I have a value for the daily amount of savings, another for the monthly savings, and another for the yearly savings.
I then want to put in an object to purchase. And the output the number of years,months,days,weeks till you can buy this object.</p>

<p>I want it to be formated as 
You will be able to afford this item in 1 Month 2 Weeks 3 Days</p>

<pre><code>double days_till;
 int years;
 int months;
 int weeks;


 days_till = price/ daily_savings;


 years = (int) (days_till/365);

 int new_days_till = (int) ((days_till - years) * 365);

 months = new_days_till/30;

 int new_days_till2 = new_days_till - months*30;

 weeks = new_days_till2/7;

 int new_days_till3 = new_days_till2 - 7 * weeks;


 String days_till_string = String.valueOf(years) + "" years"" + String.valueOf(months) + "" months"" + String.valueOf(weeks) + "" weeks"" + String.valueOf(new_days_till3) + ""days"";
</code></pre>
"
340,Apple Push Notification Service Statistics,"<p>Is it possible to find anywhere information about Apple Push Notification Service usage - how many people are using, and how many people are declining them? Or maybe, somebody already have this sort of information?</p>

<p>Thank you!</p>
"
341,map points between two triangles in 3D space,"<p><strong>EDIT</strong></p>

<p><strong>I don't know is it important, but destination triangle angles may be different than these of source. Does that fact makes transformation non-affine ? (i'm not sure)</strong></p>

<p><img src=""http://i.stack.imgur.com/uDlay.png"" alt=""alt text""></p>

<p>I have two triangles in 3D space. Given that i know (x,y,z) of point in first triangle and i know vectors V1,V2,V3. I need to find point (x',y',z'). What transformation i should do to point (x,y,z) with vectors V1,V2,V3 to get that transformed point in the second triangle ?</p>

<p>Thanks for help !!! </p>
"
342,Fast way of getting index of match in list,"<p>Given a list <code>a</code> containing vectors of unequal length and a vector <code>b</code> containing some elements from the vectors in <code>a</code>, I want to get a vector of equal length to <code>b</code> containing the index in <code>a</code> where the element in <code>b</code> matches (this is a bad explanation I know)...</p>

<p>The following code does the job:</p>

<pre><code>a &lt;- list(1:3, 4:5, 6:9)
b &lt;- c(2, 3, 5, 8)

sapply(b, function(x, list) which(unlist(lapply(list, function(y, z) z %in% y, z=x))), list=a)
[1] 1 1 2 3
</code></pre>

<p>Replacing the <code>sapply</code> with a for loop achieves the same of course</p>

<p>The problem is that this code will be used with list and vectors with a length above 1000. On a real life set the function takes around 15 seconds (both the for loop and the <code>sapply</code>).</p>

<p>Does anyone have an idea how to speed this up, safe for a parallel approach? I have failed to see a vectorized approach (and I cannot program in C, though that would probably be the fastest).</p>

<p><strong>Edit:</strong></p>

<p>Will just emphasize Aaron's elegant solution using match() which gave a speed increase in the order of 1667 times (from 15 to 0.009)</p>

<p>I expanded a bit on it to allow multiple matches (the return is then a list)</p>

<pre><code>a &lt;- list(1:3, 3:5, 3:7)
b &lt;- c(3, 5)
g &lt;- rep(seq_along(a), sapply(a, length))
sapply(b, function(x) g[which(unlist(a) %in% x)])
[[1]]
[1] 1 2 3

[[2]]
[1] 2 3
</code></pre>

<p>The runtime for this was 0.169 which is arguably quite slower, but on the other hand more flexible</p>
"
343,varying axis values in facet_wrap,"<p>I am working with a Danish dataset on immigrants by country of origin and age group. I transformed the data so I can see the top countries of origin for each age group.
I am plotting it using facet_wrap. What I would like to do is, since different age groups come from quite different areas, to show a different set of values for one axis in each facet. For example, those that are between 0 and 10 years old come from countries x,y and z, while those 10-20 years of age come from countries q, r, z and so on.</p>

<p>In my current version, it shows the entire set of values, including countries that are not in the top 10. I would like to show just the top ten countries of origin for each facet, in effect having different axis labels for each. (And, if it is possible, sorting by high to low for each facet).
Here is what I have so far:</p>

<pre><code>library(ggplot2)
library(reshape)
###load and inspect data
load(url('http://dl.dropbox.com/u/7446674/dk_census.rda'))
head(dk_census)

###reshape for plotting--keep just a few age groups
dk_census.m &lt;- melt(dk_census[dk_census$Age %in% c('0-9 år', '10-19 år','20-29 år','30-39 år'),c(1,2,4)])

###get top 10 observations for each age group, store in data frame
top10 &lt;- by(dk_census.m[order(dk_census.m$Age,-dk_census.m$value),], dk_census.m$Age,     head, n=10)
top10.df&lt;-do.call(""rbind"", as.list(top10))
top10.df

###plot
ggplot(data=top10.df, aes(x=as.factor(Country), y=value)) +
  geom_bar(stat=""identity"")+
  coord_flip() +
  facet_wrap(~Age)+
  labs(title=""Immigrants By Country by Age"",x=""Country of Origin"",y=""Population"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/WGUnf.png"" alt=""immigrant chart""></p>
"
344,Subset multiple columns in R - more elegant code?,"<p>I am subsetting a dataframe according to multiple criteria across several columns. I am choosing the rows in the dataframe that contain any one of several values defined in the vector ""criteria"" in any one of three different columns.</p>

<p>I have some code that works, but wonder what other (more elegant?) ways there are to do this.  Here is what I've done:</p>

<pre><code>criteria &lt;-c(1:10)
subset1 &lt;-subset(data, data[, ""Col1""] %in% criteria | data[, ""Col2""]
 %in% criteria | data[, ""Col3""] %in% criteria)
</code></pre>

<p>Suggestions warmly welcomed. (I am an R beginner, so very simple explanations about what you are suggesting are also warmly welcomed.) </p>
"
345,"Idea of developing ""Calendar"" from scratch","<p>What's the broad math idea behind the calendar if you need to build one from scratch without depending on OS or other libraries ?</p>
"
346,Looking to contribute to open source or ideas for something open source,"<p>I hold degrees in computer science and mathematics. Both of these fields are extremely interesting to me. I particularly like high performance computing, statistical computing, c++, distributed computing, and algorithms. I would very much like to experience open source programming but don't really have any ideas of my own and would like to participate/contribute (if that's possible) to open source projects which match my aforementioned interests.</p>

<p>Is this possible? And if so how would I go about doing this? What are some open source projects of this nature which I could contribute to, extend, mirror, and how do I go about doing so?</p>

<p>Thank you, any advice is highly appreciated</p>
"
347,How to avoid some values in the legend?,"<p>The piece of code given below will read and show image of the file with a legend.
how can I tell R to not take into consideration the values of ,for example -9999 ,or any other values when drawing legend.</p>

<pre><code>conne &lt;- file(""C:\\DEM.bin"", ""rb"")
a=readBin(conne, numeric(), size=4,  n=360*720, signed=TRUE)  
y&lt;-matrix(data=a,ncol=360,nrow=720)
image(t(t(y)),ylim=c(1,0))
image.plot(t(t(y)), add = FALSE, legend.shrink = 0.9,legend.width = 1.2,
           legend.mar = NULL, graphics.reset = FALSE, horizontal = FALSE, bigplot = NULL,
           smallplot = NULL,legend.only = FALSE,lab.breaks=NULL, axis.args=NULL))
</code></pre>
"
348,R built in Web server,"<p>Since R 2.13, R come with a built in web server. 
Is there a simple way to create a local web server using R with custom port number ?</p>

<p>In python it will be  (to use ""http://localhost:8080"") :</p>

<pre><code>python -m SimpleHTTPServer 8080
</code></pre>

<p>I'm aware of Rook but I look for a simple solution.</p>

<p>Thank's</p>
"
349,Percentage calculation around 0.5 (0.4 = -20% and 0.6 = +20%),"<p>I'm in a strange situation where I have a value of 0.5 and I want to convert the values from 0.5 to 1 to be a percentage and from 0.5 to 0 to be a negative percentage.</p>

<p>As it says in the title 0.4 should be -20%, 0.3 should be -40% and 0.1 should be -80%.</p>

<p>I'm sure this is a simple problem, but my mind is just refusing to figure it out :)</p>

<p>Can anyone help? :)</p>
"
350,Compute fast log base 2 ceiling,"<p>What is a fast way to compute the <code>(long int) ceiling(log_2(i))</code>, where the input and output are 64-bit integers? Solutions for signed or unsigned integers are acceptable. I suspect the best way will be a bit-twiddling method similar to those found <a href=""http://graphics.stanford.edu/~seander/bithacks.html#IntegerLogObvious"" rel=""nofollow"">here</a>, but rather than attempt my own I would like to use something that is already well tested. A general solution will work for all positive values.</p>

<p>For instance, the values for 2,3,4,5,6,7,8 are 1,2,2,3,3,3,3 </p>

<p>Edit: So far the best route seems to be to compute the integer/floor log base 2 (the position of the MSB) using any number of fast existing bithacks or register methods, and then to add one if the input is not a power of two. The fast bitwise check for powers of two is <code>(n&amp;(n-1))</code>.</p>

<p>Another method might be to try a binary search on exponents <code>e</code> until <code>1&lt;&lt;e</code> is greater than or equal to the input.  </p>
"
351,How do I reverse my sound volume math for my volume slider?,"<p>I'm building a video player and am kinda stuck at the volume slider part. It's a YouTube style vertical slider, meaning if the slider is in the top position volume should be 100% and if the slider is dragged to the bottom position sound should be 0. Currently it's doing the opposite of what I want :(</p>

<p>Dragging the slider down will make the sound louder, while dragging up lowers it.</p>

<p>Here is my code below dealing with the volume slider.</p>

<pre><code>// Sound Controller Settings ······························
   soundController = new SoundController();
   soundContrColor = soundController.colorChip;
   soundContrGray  = soundController.grayCover;
   soundContrGray.visible     = false;
   soundController.visible    = true;
   soundController.buttonMode = true;
   soundController.soundSlider.addEventListener(MouseEvent.MOUSE_DOWN, sliderDown);

// SoundController Button Mouse Events ························
   public function sliderDown(event:MouseEvent):void
   {
       soundController.soundSlider.startDrag(false, dragBounds);
       soundController.soundSlider.addEventListener(MouseEvent.MOUSE_MOVE, sliderMove);
       soundController.soundSlider.addEventListener(MouseEvent.MOUSE_UP, sliderUp);
       soundContrGray.visible = true;
   }

   public function sliderMove(event:MouseEvent):void
       {
       soundContrGray.height = soundController.soundSlider.y;
       userVolume = Math.round(((soundContrGray.y * soundContrGray.height) / 10) - 4);
       //userVolume = soundContrGray.height;
       setVolume(userVolume);

       trace(""soundController.mouseY = ""+soundController.soundSlider.y);
       trace(""soundContrColor.height = ""+Math.round(soundContrGray.height));
       trace(""userVolume             = ""+userVolume+""\r"");

       event.updateAfterEvent();
       }

    public function sliderUp(event:MouseEvent):void
    {
        lastVolPoint = soundContrGray.height;
        setVolume(userVolume);
        event.updateAfterEvent();

        soundController.soundSlider.stopDrag();
        soundController.soundSlider.removeEventListener(MouseEvent.MOUSE_MOVE, sliderMove);
        soundController.soundSlider.removeEventListener(MouseEvent.MOUSE_UP, sliderUp);
    }
</code></pre>

<p>[TRACES] when I drag all the way to the top:</p>

<pre><code>soundController.mouseY = 6
soundContrGray.height  = 6
userVolume             = 0
</code></pre>

<p>[TRACES] when I drag all the way down:</p>

<pre><code>soundController.mouseY = 56
soundContrGray.height  = 56
userVolume             = 30
</code></pre>

<p>I believe this is where the problem lies:</p>

<pre><code>userVolume = Math.round(((soundContrGray.y * soundContrGray.height) / 10) - 4);
</code></pre>

<p><em>The <strong>(-4)</strong> is an offset value so when you drag it all the way to turn it off, it's 0 and not 4.</em><br/>
I need to reverse this somehow, so the traces above will swap... going down will make userVolume = 4 and going up will make it 30.</p>

<p>Thanks in advance for anyone that takes a look at this one! :)</p>
"
352,Create curve 'algorithm' after capturing points from user?,"<p>I am capturing some points on an X, Y plane to represent some values. I ask the user to select a few points and I want the system to then generate a curve following the trend that the user creates. How do I calculate this? So say it is this:</p>

<p>Y = dollar amount
X = unit count</p>

<p>user input: (2500, 200), (4500, 500), (9500, 1000)</p>

<p>Is there a way I can calculate some sort of curve to follow those points so I would know based off that selection what Y = 100 would be on the same scale/trend?</p>

<p>EDIT: People keep asking for the nature of the curve, yes logarithmic. But I'd also like to check out some other options. It's for pricing the the restraint is that the as X increases Y should always be higher. However the rate of change of the curve should change related to the two adjacent points that the user selected, we could probably require a certain number of points. Does that help?</p>

<p>EDIT: Math is hard.</p>

<p>EDIT: Maybe a parabola then?</p>
"
353,dynamically plotting with ggplot2,"<p>i'm new to ggplot and i'm trying to automatically plot multiple subset of the data to different pdf files , but i'm encoutering an error and need your help.</p>

<p>that's my code :</p>

<pre><code>library(ggplot2)
t=read.table(""../All.txt"",stringsAsFactors=FALSE)
names( t ) &lt;- c(""A"",""C"",""G"",""T"",""(A-T)/(A+T)"",""(G-C)/(G+T)"",""(A+T)/(G+C)"",""accession"",""Phylum"",""Order"",""Class"")
    phy=unique(c(t$Phylum))
    for (x in phy){ 
    if(x==""???:???"")
    {
        x=""unknown""
    }
    pdf(paste(x,"".pdf"") , width=25, height=15)
    test&lt;-subset(t, Phylum==x)
    dat &lt;- melt(test, measure=c(""A"", ""C"" , ""G"" , ""T"" , ""(A-T)/(A+T)"", ""(G-C)/(G+T)"",""(A+T)/(G+C)""))
    ggplot(dat, aes(Class,value , color=variable))  + geom_boxplot() +geom_jitter()   +  facet_grid(variable~., scales=""free_y"")
    }
</code></pre>

<p>the error is :</p>

<pre><code>argument implies differing number of row: 0,1
</code></pre>

<p>how can i fix this error? thanks for your help</p>
"
354,Finding quaternion representing the rotation from one vector to another,"<p>I have two vectors u and v. Is there a way of finding a quaternion representing the rotation from u to v?</p>
"
355,R & RCurl: Error 54 in libcurl,"<p>I am trying to get some data in json format in using RCurl.</p>

<p>I have to use POST to enter the username and password such as:</p>

<pre><code>postForm(url1, user=x$USERNAME, pass=x$PASSWORD)
</code></pre>

<p>I get the following error:</p>

<pre><code>Error in function (type, msg, asError = TRUE)  : 
</code></pre>

<p>SSL read: error:00000000:lib(0):func(0):reason(0), errno 54</p>

<p>If I researched the correct error number 54 from the <a href=""http://curl.haxx.se/libcurl/c/libcurl-errors.html"" rel=""nofollow"">libcurl</a> site:</p>

<blockquote>
  <p>CURLE_SSL_ENGINE_SETFAILED (54)</p>
  
  <p>Failed setting the selected SSL crypto engine as default!</p>
</blockquote>

<p>If this is the correct error how would I select the SSL engine?</p>
"
356,why setting dtype on numpy array changes dimensions?,"<p>I am tryng to understand how to set dtypes of an array. My original numpy array dimensions are (583760, 7) i.e. 583760 rows and 7 columns. I am setting dtype as follows</p>

<pre><code>&gt;&gt;&gt; allRics.shape
(583760, 7)
&gt;&gt;&gt; allRics.dtype = [('idx', np.float), ('opened', np.float), ('time', np.float),('trdp1',np.float),('trdp0',np.float),('dt',np.float),('value',np.float)]
&gt;&gt;&gt; allRics.shape
(583760, 1)
</code></pre>

<p>Why is there a change in the original shape of the array? What causes this change? I am basically trying to sort original numpy array by <code>time</code> column and thats why I am setting the <code>dtype</code>. But after the dimension change, I am not able to sort array</p>

<pre><code>&gt;&gt;&gt; x=np.sort(allRics,order='time')
</code></pre>

<p>there is no change in the output of the above command. Could you please advice?</p>
"
357,"calculating the number of ""open hours"" per day between two dates","<p>I have a data frame with start dates and end dates, along with the number of people registered for an event. I would like to calculate the number of hours each party is present for within a specific timeframe (e.g., 07:00 - 17:00)</p>

<p>If I use the following example data.frame...</p>

<pre><code>d &lt;- data.frame(startDate = c(as.POSIXct(""2011-06-04 08:00:00""), as.POSIXct(""2011-06-03 08:00:00""),
                          as.POSIXct(""2011-09-12 10:00:00"")),
            endDate = c(as.POSIXct(""2011-06-06 11:00:00""), as.POSIXct(""2011-06-04 11:00:00""),
                        as.POSIXct(""2011-09-12 18:00:00"")),
            partysize = c(124,442,323))
open &lt;- ""07:00""
close &lt;- ""17:00""
</code></pre>

<p>I would like my result set to look something like this:</p>

<pre><code>day                     numhours  partysize
2011-06-04                     9        124
2011-06-05                    10        124
2011-06-06                     4        124
2011-06-03                     9        442
2011-06-04                     4        442
2011-09-12                     7        323
</code></pre>

<p><em>note: numhours is the number of hours the date was included between the open and close times</em></p>

<p>Thanks in advance,
--JT</p>
"
358,Output precision is reduced to the precision of the operands,"<p>I am getting warning message on the following expression.</p>

<pre><code>(400*ev.PageBounds.Width)/2400
</code></pre>

<p>Warning message is <strong>Output precision is reduced to the precision of the operands</strong>.</p>

<p>Why? Can I just ignore it?</p>

<p>Thanks,</p>
"
359,attempting to script r commands to run,"<p>I have files in two different directories that I would like to merge the same files.</p>

<p>for example:</p>

<p>in c:/data1, I have files like this:</p>

<p>teamA.Rdata, teamB.RData, teamC.RDate</p>

<p>in c:/data2 I have the same files:</p>

<p>teamA.Rdata, teamB.RData, teamC.RDate</p>

<p>The file names are the same but the content of the files are different. I like to write an r script to combile files with the same names. For example, I would like to merge teamA.RData file locate in C:/data1 and c:/data2 directories:</p>

<p>I have written this piece of code:</p>

<pre><code>for (i in dir(""c:/data1"", pattern = ""^team""))

     {

      print(i)

      for (r in dir(""c:/data2"", pattern= ""^team""))

        {

             print(r)
             if(i==r) {
             print(""yesssssssssssssssssssssss"")
             }
         }
   }
</code></pre>

<p>I put this line of code to R console, I get a print out. However, when I tried to run this in via an rscript, I dont get anything even though I have print statements in the code. Is there something that I have to do in rscript to make this happen?</p>
"
360,Statistics Help,"<p>I have some questions that I'm not sure of. Any help is appreciated.</p>

<ol>
<li><p>48.7% of Americans have brown eyes. A convention has 5000 people in attendance. Find the mean (expected value),the standard deviation, and the variance for the number of brown-eyed people at such gatherings of 5000 people.</p></li>
<li><p>A recent survey shows that 70% of families regularly schedule family vacations for the month of April. Find the probability that at least 10 out of 15 randomly selected families regularly schedule family vacations for the month of April.</p></li>
</ol>

<p>35% of American households subscribe to TIVO. Suppose that 12 American households are randomly chosen. (Answer both parts for full credit.)</p>

<p>a) How many of them would you expect to have TIVO? <strong>ANSWER-4 OR 4.2</strong></p>

<p>b) What is the probability that 2 OR 3 of them have TIVO?</p>

<ol>
<li><p>Type O blood is found in 44% of Americans. Suppose 7 samples of blood are tested at random. Find the probability that exactly 2 of the tested samples will be Type O.</p></li>
<li><p>Forty percent of Clinton County residents are uncomfortable using the Internet. Suppose 11 such people are randomly selected. What is the probability exactly 5 are uncomfortable using the Internet?</p></li>
<li><p>A basketball player with a history of making 80% of the foul shots taken during games. (Answer all parts for full credit)</p></li>
</ol>

<p>1.What is the probability that he will make exactly 6 of 10 foul shots?
2.What is the probability that he will make at least 6 of 10 foul shots?
3.What is the probability that he will MISS at least one of 10 foul shots?</p>

<ol>
<li>Given the following probability distribution, calculate the mean, variance, and standard deviation. </li>
</ol>

<p>x        P(x)</p>

<p>1        0.125</p>

<p>2        0.236 </p>

<p>3        0.543 </p>

<p>4        0.096 </p>
"
361,My unsigned short pointers are returning unexpected results. Why?,"<p>I'm writing in C for OSX, running on a 64-bit machine in 32-bit mode. I'm compiling using GCC for 386. The project is large; I've not seen any odd behavior from the compiler (possibly until now.) The application is multithreaded, and this code is meant to be, but is being run single-threaded at this time. I am compiling using the position independent clib, and using posix threads, when threading. This code, btw, acts exactly the same if I <em>do</em> thread it.</p>

<p>The following is a simplified procedure that demonstrates the problem in the simplest form. Basically, I'm moving 16-bit image channels here from one set of three RGB channels (mr,mg,mb) to another set of 3 RGB channels (lr,lg,lb) of exactly the same size. The fragment, as I'm about to dump it, works perfectly:</p>

<pre><code>void lrip_me(   unsigned short *mr, // RIP from source image
                unsigned short *mg,
                unsigned short *mb,
                unsigned short *lr, // into RGB layer
                unsigned short *lg,
                unsigned short *lb,
                struct layer *lay,
                long start,long finish)
{
long xw,yw;
long xf,yf;
long x,y;
unsigned long offset;

    xw = lay-&gt;parent-&gt;x;
    yw = lay-&gt;parent-&gt;y;
    xf = xw - 1;
    yf = yw - 1;

    for (y=start; y&lt;finish; y++)
    {
        for (x=0; x&lt;xw; x++)
        {
            offset = (y * xw) + x;
            if (x==0 || x==xf || y==0 || y==yf) // then on edge
            {
                lr[offset] = mr[offset];
                lg[offset] = mg[offset];
                lb[offset] = mb[offset];
            }
            else
            {
                lr[offset] = mr[offset];
                lg[offset] = mg[offset];
                lb[offset] = mb[offset];
            }
        }
    }
}
</code></pre>

<p>As you can see, the action at the edge of the image and inside the edges is the same; I'm simply moving the data. And this works -- the output of this is an image in the lr,lg, and lb channels.</p>

<p>BUT. If, inside the else clause, I change the lines to read...</p>

<pre><code>            lr[offset] = mr[offset-xw];
            lg[offset] = mg[offset-xw];
            lb[offset] = mb[offset-xw];
</code></pre>

<p>...you'd expect the interior of the image to move one scan line, as the data fetches would be coming from a full scan line's distance away but going to an un-shifted target location. Instead, the output looks completely random. Like the shorts were loaded on the wrong boundaries, perhaps, or from somewhere else. This does the same thing...</p>

<pre><code>            lr[offset] = mr[offset-1];
            lg[offset] = mg[offset-1];
            lb[offset] = mb[offset-1];
</code></pre>

<p>...there, I'd expect the image to move one pixel horizontally. No. Same result -- purest hash.</p>

<p>I have tried changing to pointer math instead of arrays //*(mr+offset-1)// and get exactly the same result. I've compiled it in a library, and inside objc environment. I've tried using even offset, thinking perhaps the compiler is confused about the size of the data. I get the same results every time. The offset value can be used as is, but ONLY as is. It can't be modified inside the brackets, and furthermore, it can't be modified OUTSIDE the brackets, just being treated as a long.</p>

<p>I have NO idea what is going on here. I would surely appreciate it if someone would help me understand what is.</p>

<p>A little background; this is a RIP (remove isolated pixel) operation, or at least, it will be, if I can get at the 8 pixels surrounding the one the loop is currently looking at. That's why the edges are treated differently; I don't want to run the 8-pixel lookaround on the edges, only inside them where I'll always have all 8 to look at:</p>

<pre><code>ooo
oxo
ooo
</code></pre>

<p>In this test case, I've removed all the RIP code, as it adds a lot of complexity and doesn't shed any light on the problem. It's simply (hah!) a situtation where array and pointer offsets don't seem to work sensibly.</p>
"
362,Extracting data used to make a smooth plot in mgcv,"<p><a href=""http://stats.stackexchange.com/questions/7795/how-to-obtain-the-values-used-in-plot-gam-in-mgcv"">This thread</a> from a couple of years ago describes how to extract data used to plot the smooth components of a fitted gam model.  It works, but only when there is one smooth variable.  I've got more than one smooth variable, and unfortunately I can only extract the smooths from the last of the series.  Here is an example:</p>

<pre><code>library(mgcv)
a = rnorm(100)
b = runif(100)
y = a*b/(a+b)

mod = gam(y~s(a)+s(b))
summary(mod)

plotData &lt;- list()
trace(mgcv:::plot.gam, at=list(c(25,3,3,3)), 
        #this gets you to the location where plot.gam calls plot.mgcv.smooth (see ?trace)
        #plot.mgcv.smooth is the function that does the actual plotting and
        #we simply assign its main argument into the global workspace
        #so we can work with it later.....
        quote({
                    #browser()
                    plotData &lt;&lt;- c(plotData, pd[[i]])
                }))
plot(mod,pages=1)
plotData
</code></pre>

<p>I'm trying to get the estimated smooth functions for both <code>a</code> and <code>b</code>, but the list <code>plotData</code> only gives me estimates for <code>b</code>.  I've looked into the guts of the <code>plot.gam</code> function, and I'm having a difficult time understanding how it works.  If anybody has already solved this problem, I'd be grateful.</p>
"
363,Variance of a max function,"<p>Say $x_1$ and $x_2$ are normal random variables with known means and standard deviations and $C$ is a constant. If $y = \max(x_1,x_2,C)$, what is $\mathrm{Var}(y)$?</p>

<p>Well, I forgot to tell that $x_1$ and $x_2$ are independent. </p>
"
364,Data cleaning in Excel sheets using R,"<p>I have data in Excel sheets and I need a way to clean it. I would like remove inconsistent values, like Branch name is specified as (Computer Science and Engineering, C.S.E, C.S, Computer Science). So how can I bring all of them into single notation? </p>
"
365,Point to Line distance (2D) and Coordintates of intersection,"<p>So I need to know the Distance from a point to a line (in 2D space), given two coordinates of the line (AB).</p>

<p>Here is what I have so far:</p>

<pre><code>public double pointToLineDistance(Point A, Point B, Point P)
{
    double normalLength = Math.sqrt((B.x - A.x) * (B.x - A.x) + (B.y - A.y) * (B.y - A.y));
    return Math.abs((P.x - A.x) * (B.y - A.y) - (P.y - A.y) * (B.x - A.x)) / normalLength;
}
</code></pre>

<p>But I also need to get the coordinates of the point where the perpendicular line intersects with the AB line (it's ok if it's outside this segment).</p>

<p>Any ideas?</p>
"
366,Performance of joblib Parallel loop with numpy ndarray,"<p>I'm doing some statistical computations in python using numpy. My current implementation is not parallelized so far. So I was looking into python joblib Parallel for a simple loop-parallelization.</p>

<p>My non-parallelized part of the code looks like this:</p>

<pre><code>def calcRADMatInt( i, j , RADMat, pdfMu, pdfSigma):
  if i==j:
    RADMat[i, j] = 0.0
  else:
    RADMat[i, j] = calcRAD( pdfMu[i], np.squeeze( pdfSigma[i]), pdfMu[j], np.squeeze( pdfSigma[j]) )
    RADMat[j, i] = RADMat[i,j]

def caldRADMat(....):

....
....
  RADMat = np.zeros( (numLandmark, numLandmark) )

  for i in range( 0, numLandmark):
    for j in range( i, numLandmark)):
      calcRADMatInt( i, j, RADMat, pdfMu, pdfSigma)

....
....
</code></pre>

<p>I've tried to parallelize it like this:</p>

<pre><code>def caldRADMat(....):
....
....

  RADMat = np.zeros( (numLandmark, numLandmark) )

  for i in range( 0, numLandmark):
    Parallel(n_jobs=8)(delayed(calcRADMatInt)( i, j, RADMat, pdfMu, pdfSigma) for j in    range( i, numLandmark))

....
....
</code></pre>

<p>However, the resulting parallel code runs significantly slower than the non-parallelized version.</p>

<p>So I guess my actual questions are:
Am I using joblib Parallel correctly?
Is this the right way to parallelize computation of numpy ndarray elements?</p>
"
367,Divide-and-conquer approach for hierarchical clustering,"<p>I have a huge data set (33K), each represented as a bit-vector of 275-dimensions. Basically my data set can be represented as a $33000 \times 275$ matrix. I want to cluster these bit-vectors. I have tried single link hierarchical clustering on a small data set, $3000 \times 275$, and the result is promising.</p>

<p>I know that single link hierarchical clustering algorithm is not scalable as the time complexity is $O(n^2)$. I am planning to apply a divide-and-conquer approach, i.e., divide the dataset into chunks of equal size and cluster each chunks individually and finally merge the clustered chunks based on distance (if: $d(C_1,C_2)&lt; t$; then: merge $C_1$ and $C_2$).</p>

<p>The time complexity for my new approach is $O(p) + O(pq)$, where $p$ is the number of chunks and $q$ the average number of clusters in each chunk. Note: I assume that when hierarchical clustering is applied, each chuck will take same amount of time and its constant for all chunks, thus $O(n^2)$ will become $O(1)$.</p>

<p>I want to know, whether the above mentioned clustering approach is feasible and efficient. Or is there any logic flaws in applying divide-and-conquer approach for clustering.</p>
"
368,Comparing floating point numbers with numpy and scipy,"<p>Is there any epsilon constant (as in Matlab) in numpy or scipy modules to compare floating point numbers?</p>
"
369,How to categorize continuous data?,"<p>I have two dependent continuous variables and i want to use their combined values to predict the value of a third binary variable. How do i go about discretizing/categorizing the values? I am not looking for clustering algorithms,  i'm specifically interested in obtaining 'meaningful' discrete categories i can subsequently use in in a Bayesian classifier. 
Pointers to papers, books, online courses, all very much appreciated!</p>
"
370,How to plot a ROC curve for a knn model,"<p>I am using ROCR package and i was wondering how can one plot a ROC curve for knn model in R? Is there any way to plot it all with this package?</p>

<p>I don't know how to use the prediction function of ROCR for knn. Here's my example, i am using isolet dataset from UCI repository where i renamed the class attribute as y: </p>

<pre><code>cl&lt;-factor(isolet_training$y)
knn_isolet&lt;-knn(isolet_training, isolet_testing, cl, k=2, prob=TRUE)
</code></pre>

<p>Now my question is, what are the arguments to pass to the prediction function of ROC. I tried the 2 below alternatives which are not working:</p>

<pre><code>library(ROCR)
pred_knn&lt;-prediction(knn_isolet$y, cl)
pred_knn&lt;-prediction(knn_isolet$y, isolet_testing$y)
</code></pre>
"
371,How to obtain multiple colours for geom_line conditional on a specific value,"<p>I have the following dataframe that I would like to plot. I was wondering if it is possible to color portions of the lines connecting my outcome variable(stackOne$y) in a different color, depending on whether it is less than a certain value or not. For example, I would like portions of the lines falling below 2.2 to be red in color.</p>

<pre><code>id&lt;-rep(c(1,2,3),each=3)
y&lt;-rnorm(9,2,1)
x&lt;-rep(c(1,2,3),3)
stackOne&lt;-data.frame(cbind(id,y,x))
p&lt;-ggplot(stackOne,aes(x=x,y=y)) +
geom_point()+
geom_line(aes(group=id))
p
</code></pre>

<p>Thanks!</p>
"
372,Flipped Parabola in C++,"<p>I'm programming a game requiring evaluation of the function y = -x^2+49. When I type the following to my TI-84, I have to type it -(x)^2 + 49 or -x^2 + 49, and NOT (-x)^2 + 49. And here's how I implemented it in C++</p>

<pre><code>pp[i].death = -(j)^2 + 49;  // Upside down hyperbola
</code></pre>

<p>I put a breakpoint to track the value, and it just doesn't makes sense.  My j is in the range [-7, +7] so it should return results in the range [0, 49].  But the C++ code gives me 52 or some random number that's way off. Answers would be appreciated.</p>
"
373,Math questions at a programmer interview?,"<p>So I went to an interview at Samsung here in Dallas, Texas. The way the recruiter described the job, he didn't make it sound like it was too math-oriented. The job basically involved graphics programming and C++. Yes, math is implied in graphics programming, especially shaders, but I still wasn't expecting this...</p>

<p>The whole interview lasted about an hour and a half and they asked me nothing but math-related questions. They didn't ask me a single programming question, which I found odd. About all they did was ask me how to write certain math routines as a C++ function, but that's about it. What about programming philosophy questions? Design patterns? Code-correctness? Constness? Exception safety? Thread safety? There are a zillion topics that they could have covered. But they didn't.</p>

<p>The main concern I have is that they didn't ask any programming questions. This basically implies to me that any programmer who is good at math can get a job here, but they might put out terrible code.</p>

<p>Of course, I think I bombed the interview because I haven't used any sort of linear algebra in about a year and I forget math easily if I haven't used it in practice for a while. Are any of my other fellow programmers out there this way? I'm a game programmer too, so this seems especially odd. The more I learn, the more old knowledge that gets ""popped"" out of my ""stack"" (memory).</p>

<p>My question is: Does this interview seem suspicious? Is this a typical interview that large corporations have? During the interview they told me that Google's interview process is similar. They have multiple, consecutive interviews where the math problems get more advanced.</p>
"
374,Upper-bound on the sum of two dependent Gaussians.,"<p>Let $X$ and $Y$ be two dependent normally distributed continuous random variables (their marginals are $\mathcal{N}(0, 1)$).
I would like to find an upper bound on the probability that one is greater than the other by a given threshold, i.e. find a $\theta$ such that $P(Y &gt; X + \delta) &lt; \theta$, $\delta, \theta \in \mathbb{R}$</p>

<p>If $X$ and $Y$ were independent it would be easy as I could directly compute their joint density, but in the case were they are allowed to be dependent all I could do is find a numerical solution by discretizing the joint density (which looked far from trivial).</p>
"
375,`cex.lab` Axis label exceeds plot region,"<p>I want to create a plot with magnified axis labels using <code>cex.lab=2</code> but the label exceeds the plot region. Any ideas on how can I solve this?  </p>

<p>Here is an example of the issue:</p>

<pre><code>plot(1:10,1:10,ylab=~gamma,cex.lab=2)
</code></pre>

<p>Which produces a graph with a beheaded $\gamma$</p>

<p><img src=""http://i.stack.imgur.com/OvjFo.jpg"" alt=""enter image description here""></p>

<p>I have done some search before asking the question both in google and in this site but my google foo betrayed me this time.</p>
"
376,"What is the $\operatorname{cov}(X, \max(X,Y))$ and $\operatorname{cov}(X, \min(X,Y))$ where $X,Y \sim N(0,1)$?","<p>having trouble with this one. The exact questions is the $\operatorname{cov}(X, \max(X,Y))$ and $\operatorname{cov}(X, \min(X,Y))$ where $X,Y \sim N(0,1)$. </p>

<p>i think the way to calculate it is to get 
$$\begin{align}
\operatorname{cov}(X, \max(X, Y) + \min(X,Y)) &amp; = \operatorname{cov}(X, X+Y) \\
&amp; = \operatorname{cov}(X, \max(X,Y)) + \operatorname{cov}(x, \min(X,Y)) \\
\end{align}$$</p>

<p>and 
$$\begin{align}
\operatorname{cov}(X, \max(X,Y) - \min(X,Y)) &amp; = \operatorname{cov}(X, \operatorname{abs}(X-Y)) \\
&amp; = \operatorname{cov}(X, \max(X,Y)) - \operatorname{cov}(X, \min(X,Y)) \\
\end{align}$$</p>

<p>although this is pretty much as difficult to solve as $\operatorname{cov}(X, \max(X,Y))$  unless there is some particular trick. Anyone can help with this?</p>
"
377,Printing full page world map plot,"<p>I am trying get a full page plot of a world map on a A4 landscape PDF. When I use the following</p>

<pre><code>pdf(""temp.pdf"", paper=""a4r"")
#par(mar=rep(0,4))
map(""world"", mar = rep(0,4))
dev.off()
</code></pre>

<p>I get</p>

<p><img src=""http://i.stack.imgur.com/yGZL1.png"" alt=""enter image description here""></p>

<p>I can not seem to get the plot to fill the page. Any tips? </p>
"
378,"How to construct an axis label with both normal, italic and bold font","<p>I would like to construct the following x axis label</p>

<p>bla bla <em>bli bli</em> bla <strong>blom</strong> bla</p>

<p>(normal normal italic italic normal bold italic)</p>

<p>Is it possible?</p>

<p>Thanks a lot</p>

<p>EDIT: Is it also possible to change the font?</p>
"
379,Algorithm design manual solutions error?,"<p>I've read about Big O notation from many sources, including <a href=""http://rads.stackoverflow.com/amzn/click/0387948600"" rel=""nofollow"">Skiena</a> and the <a href=""http://en.wikipedia.org/wiki/Big_O_notation"" rel=""nofollow"">Wikipedia</a> entry, the <a href=""http://en.wikipedia.org/wiki/Big_O_notation#Example"" rel=""nofollow"">Example</a> section of which states:</p>

<blockquote>
  <p>In typical usage, the formal definition of O notation is not used
  directly; rather, the O notation for a function f(x) is derived by the
  following simplification rules:</p>
  
  <ul>
  <li><p>If f(x) is a sum of several terms, the one with the largest growth rate is kept, and all others omitted.</p></li>
  <li><p>If f(x) is a product of several factors, any <code>constants</code> (terms in the product that do not depend on x) are <code>omitted</code>.</p></li>
  </ul>
</blockquote>

<p>The <a href=""http://www2.algorithm.cs.sunysb.edu:8080/mediawiki/index.php/TADM2E_2.2"" rel=""nofollow"">solution</a> to <a href=""http://www2.algorithm.cs.sunysb.edu:8080/mediawiki/index.php/Algo-analysis-TADM2E"" rel=""nofollow"">problem 2.2</a> is O((n^3)/3).  Shouldn't the ""/3"" be omitted, or am I missing something?</p>
"
380,sample distribution of sample variance for a random sample,"<p>I'm given that n=2 and a simple table showing this...</p>

<p>x        0          1        5
p(x)    .25        .25      .5</p>

<p>I found the sample distribution for the sample mean to be this...
_
x      0      .5     1       2.5     3     5
  _
p(x) .0625   .125   .0625    .25    .25   .25  </p>

<p>I also discovered that the mean is 2.75, but I'm lost on how to find the sample variance for these points...I'd appreciate any feedback or tips! </p>
"
381,How can I pass values in ddply based on a column?,"<p>I want to be able to pass two sets of values GROUPED BY the column <code>Category</code>. Is there a way I can do this using <code>ddply</code> from package <code>plyr</code>?</p>

<p>I want to do something like this:</p>

<pre><code>ddply(idata.frame(data), .(Category), wilcox.test, data[Type==""PRE"",], data[Type==""POST"",])
</code></pre>

<p><code>wilcox.test</code> is the following function:</p>

<pre><code>Description

Performs one- and two-sample Wilcoxon tests on vectors of data; the latter is also known as ‘Mann-Whitney’ test.

Usage

wilcox.test(x, ...)

Arguments

x   
numeric vector of data values. Non-finite (e.g. infinite or missing) values will be omitted.

y   
an optional numeric vector of data values: as with x non-finite values will be omitted.

.... rest of the arguments snipped ....
</code></pre>

<p>I have the following output from <code>dput</code>:</p>

<pre><code>structure(list(Category = c(""A"", ""C"", 
""B"", ""C"", ""D"", ""E"", 
""C"", ""A"", ""F"", ""B"", 
""E"", ""C"", ""C"", ""A"", 
""C"", ""A"", ""B"", ""H"", 
""I"", ""A""), Type = c(""POST"", ""POST"", 
""POST"", ""POST"", ""PRE"", ""POST"", ""POST"", ""PRE"", ""POST"", 
""POST"", ""POST"", ""POST"", ""POST"", ""PRE"", ""PRE"", ""POST"", 
""POST"", ""POST"", ""POST"", ""POST""), Value = c(1560638113, 
1283621, 561329742, 2727503, 938032, 4233577690, 0, 4209749646, 
111467236, 174667894, 1071501854, 720499, 2195611, 1117814707, 
1181525, 1493315101, 253416809, 327012982, 538595522, 3023339026
)), .Names = c(""Category"", ""Type"", ""Value""), row.names = c(21406L, 
123351L, 59875L, 45186L, 126720L, 94153L, 48067L, 159371L, 54303L, 
63318L, 104100L, 58162L, 41945L, 159794L, 57757L, 178622L, 83812L, 
130655L, 30860L, 24513L), class = ""data.frame"")
</code></pre>

<p>Any suggestions?</p>
"
382,transfrom this function using normal programming code and without using R functions,"<p>i have this function in R from a previous question here</p>

<pre><code>shift &lt;- function(d, k) rbind( tail(d,k), head(d,-k), deparse.level = 0 )
</code></pre>

<p>this function will rotate the data frame d by K, that's mean it will take K rows from the end of the data frame and place them on the top.</p>

<p>i want to create the same function(in the same language) but without using R pre-made functions(head, tail,...), but only using basics of programming.(for , ...)</p>

<p>how this can be done ??</p>

<p>thanks</p>
"
383,How is AIC calculated in stepAIC,"<p>Here is a very simple lm model from ?lm</p>

<pre><code>ctl &lt;- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt &lt;- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group &lt;- gl(2,10,20, labels=c(""Ctl"",""Trt""))
weight &lt;- c(ctl, trt)
lm.D9 &lt;- lm(weight ~ group)
</code></pre>

<p>If I use stepAIC to lm.D9, on the very first line it says AIC = -12.58</p>

<pre><code>require(MASS)
stepAIC(lm.D9)
</code></pre>

<p>If I use AIC directly on lm.D9, it gives a different value 46.17648</p>

<pre><code>AIC(lm.D9)
</code></pre>

<p>My question is why the 2 AIC values are different. Thanks!</p>
"
384,Using a BY variable in coxph( ) or survreg( ),"<p>I've got the output of some simulations that look something like this:</p>

<pre><code>Run,ID,Time,Var1,Outcome
1,1,6,0.5,1
1,2,4,0.25,1
1,3,2,0.9,1
2,1,5,0.07,1
...
10,3,9,0.08,1
</code></pre>

<p>Basically a series of M studies of N individuals (in actuality M = 1000 and N = 123). I'd like to run a Cox model (preferably) or a parametric regression model (if I must) to estimate the effect of <code>Var1</code> on survival time. What I want to do is estimate the effect for each ""Run"" (to produce 1,000 estimates) and then dump all those estimates into a single data frame, matrix, etc. where I can look at their distribution.</p>

<p>If I were using SAS, the code would look something like this:</p>

<pre><code>ods output ParameterEstimates=work.parameters;
proc phreg model time*outcome(0) = Var1;
   BY Run;
run;
ods output close;
</code></pre>

<p>But since this is a side project, and I'm trying to force myself to do side projects in R in order to learn it, I can't so much fall back on SAS. As far as I can tell from the coxph() documentation, there's no easy way to include a by-variable. My guess is this is going to involve loops and subsets.</p>

<p>Any suggestions?</p>
"
385,Creating a local R package repository,"<p>I would like to create a local  R package repository such that users in my company can install packages from it and the system admins can update the local repo periodically. Access to the CRAN mirrors is currently denied.</p>

<p>Is there a simple way to do this?</p>

<p>Thank you for your time.</p>

<p>EDIT: I apologize for the oversight. The guide is where I should have looked first. Thank you.</p>
"
386,Working with decision trees,"<p>I know tl;dr;</p>

<p>I'll try to explain my problem without bothering you with ton's of crappy code. I'm working on a school assignment. We have pictures of smurfs and we have to find them with foreground background analysis. I have a Decision Tree in java that has all the data (HSV histograms) 1 one single node. Then tries to find the best attribute (from the histogram data) to split the tree on. Then executes the split and creates a left and a right sub tree with the data split over both node-trees. All the data is still kept in the main tree to be able to calculate the gini index.</p>

<p>So after 26 minutes of analysing smurfs my pc has a giant tree with splits and other data. Now my question is, can anyone give me a global idea of how to analyse a new picture and determine which pixels could be ""smurf pixels"". I know i have to generate a new array of data points with the HSV histograms of the new smurf and then i need to use the generated tree to determine which pixels belong to a smurf.</p>

<p>Can anyone give me a pointer on how to do this?</p>

<p>Some additional information.<br />Every Decision Tree object has a Split object that has the best attribute to split on, the value to split on and a gini index.</p>

<p>If i need to provide any additional information I'd like to hear it.</p>
"
387,how to install scipy for python?,"<p>I have Python 2.7, and I have <a href=""http://docs.python.org/library/distutils.html"" rel=""nofollow"">distutils</a> installed. </p>

<p>I downloaded the latest version of <a href=""http://sourceforge.net/projects/scipy/files/scipy/"" rel=""nofollow"">Scipy for win 32</a>. </p>

<p>For the life of me, I do not understand how to install it. </p>

<p>From the <a href=""http://www.scipy.org/Installing_SciPy/Windows"" rel=""nofollow"">directions on the site</a>, it states:</p>

<blockquote>
  <p>If you already have Python installed, the easiest way to install Numpy
  and Scipy is to download and install the binary distribution from
  <a href=""http://www.scipy.org/Download"" rel=""nofollow"">Download</a>.</p>
</blockquote>

<p>I have followed the above directions and <a href=""http://sourceforge.net/projects/scipy/files/scipy/"" rel=""nofollow"">downloaded this</a>. </p>

<p><img src=""http://i.stack.imgur.com/GSJEg.png"" alt=""enter image description here""></p>

<p><strong>I cannot figure what to do now!</strong></p>

<p>How do I finish getting scipy installed?</p>
"
388,sklearn.svm.SVC doesn't give the index of support vectors for sparse dataset?,"<p>sklearn.svm.SVC doesn't give the index of support vectors for sparse dataset. Is there any hack/way to get the index of SVs?</p>
"
389,create a ribbon plot,"<p>I'm wanting to create a ribbon plot (really it's a line plot of multiple groups of a categorical variable) but displayed in a 3d style.  This would look something like this:</p>

<p><img src=""http://i.stack.imgur.com/MBjCI.gif"" alt=""enter image description here""></p>

<p>So maybe we would want to plot the following sample data as a ribbon plot:</p>

<pre><code>set.seed(10)
fun &lt;- function(i) data.frame(person=rep(LETTERS[i], 26), 
    letter=letters, count=sample(0:100, 26, T))
dat &lt;- do.call(rbind, lapply(1:10, function(i) fun(i)))

library(ggplot2) #a traditional 2-d line plot of the data
ggplot(data=dat, aes(x=letter, y=count, group=person, color=person)) + 
    geom_line()
</code></pre>

<p>How can this be achieved in R?  I know there may be better ways to display the data but my interest currently is in producing the ribbon style plot.</p>
"
390,Strange C arithmetical behavior,"<p>I have a problem with this piece of C code:</p>

<pre><code>int y = 0, h = 640, ih = 640;
h = y + h - max(0, (y + h) - ih);
</code></pre>

<p>It should set h to 640, but instead it is setted to 0!</p>

<p>You can see it running here: <a href=""http://ideone.com/zBZSsr"" rel=""nofollow"">http://ideone.com/zBZSsr</a></p>

<p>Any idea about this strange behavior? Am I doing something wrong?</p>
"
391,"In SciPy, fancy indexing for csr_matrices","<p>I am new to Python, so forgive me ahead of time if this is an elementary question, but I have searched around and have not found a satisfying answer.</p>

<p>I am trying to do the following using NumPy and SciPy:</p>

<pre><code>I,J = x[:,0], x[:1]               # x is a two column array of (r,c) pairs
V = ones(len(I))
G = sparse.coo_matrix((V,(I,J)))  # G's dimensions are 1032570x1032570
G = G + transpose(G)
r,c = G.nonzero()
G[r,c] = 1
...
NotImplementedError: Fancy indexing in assignment not supported for csr matrices
</code></pre>

<p>Pretty much, I want all the nonzero values to equal 1 after adding the transpose, but I get the fancy indexing error messages.  </p>

<p>Alternatively, if I could show that the matrix G is symmetric, adding the transpose would not be necessary.  </p>

<p>Any insight into either approach would be very much appreciated.</p>
"
392,Vectorise find closest date function,"<p>I would like to pass in a vector of dates, and have returned the closest date from a second vector of (partially matching) dates. </p>

<p>The following function does what I require for a single date, however i cannot figure out how to generalise this to the case where <code>searchDate</code> is a vector of dates.  </p>

<pre><code>closestDate &lt;- function(searchDate, dateList, roundDown=FALSE){
  if (roundDown) {
    dist2date &lt;- as.Date(dateList) - as.Date(searchDate)
    closest &lt;- which(max(dist2date[dist2date&lt;=0]) == dist2date)
  } else {
    dist2date &lt;- as.Date(dateList) - as.Date(searchDate)
    closest &lt;- which(min(dist2date[dist2date&gt;=0]) == dist2date)
  }
  return(dateList[closest])
}

dateSeq &lt;- seq(as.Date(""2011-01-01""), as.Date(""2012-12-19""), by='day')
oddDates &lt;- dateSeq[as.logical(1:length(dateSeq) %%2)]

closestDate('2012-12-14', oddDates)
[1] ""2012-12-15""

miscDatesLong &lt;- rep(c('2012-12-14', '2012-12-16', '2012-12-18'), 100 )
closestDate(miscDatesLong, oddDates)

closestDate(miscDatesLong, oddDates)
[1] ""2012-12-15"" ""2012-12-17"" ""2012-12-19""
Warning message:
In unclass(time1) - unclass(time2) :
  longer object length is not a multiple of shorter object length
</code></pre>

<p>Could someone please help? </p>
"
393,Rotation with atan2,"<p>I have an object that I can rotate around (with snapping to 90 degrees).
The problem is that it only works one way around with a slight 'swipe'.</p>

<p>This is what I have:</p>

<pre><code>static float deltaAngle;
</code></pre>

<p>In the did start dragging method:</p>

<pre><code>deltaAngle = atan2f(location.y - self.selectedView.center.y,
                    location.x - self.selectedView.center.x);

self.startTransform = self.selectedView.transform;
</code></pre>

<p>In the did drag method:</p>

<pre><code>float angle = atan2f(location.y - self.selectedView.center.y,
                     location.x - self.selectedView.center.x);

float newAngle = deltaAngle - angle;

self.selectedView.transform = CGAffineTransformRotate(self.startTransform, -newAngle);
</code></pre>

<p>In the did end dragging method:</p>

<pre><code>CGFloat angle = atan2(self.selectedView.transform.b, self.selectedView.transform.a);
CGFloat degrees = radiansToDegrees(angle);
CGFloat snappedDegrees = floorf(degrees / 90.0) * 90.0;
</code></pre>

<p>It seems that it's only rotating to a new position if the current angle/degrees is lower than the start angle/degrees.</p>

<p>What can I do to make it work both ways around? I've tried something like if snappedDegrees == startingSnappedDegrees to add 90 to the snappedDegrees, but that doesn't really cut it.</p>
"
394,How to get the list of class that have a common S4 superclass in R,"<p>In R, how do I get the list of subclass of a S4 superclass?</p>

<p>I found showClass(""mySuperClass"",complete=FALSE) but it only prints the result. I would like to store it in a vector to use it.</p>
"
395,Best way to match social media data with CRM data in R?,"<p>I am having a social media data (First name, last name, gender, location). Now, i want to match this data with the CRM data to get to know the customer. I have tried couple of algorithms and it seems Jaor-Winkler is a better option then Levenshtein, soundex. But i am still looking for some better method/algorithm.
I am planning to match the strings based on variables and then create a equation to get the final record score by assigning wieghtage to the variables accordingly.
Let me know if there is a better approach for the same or if i am missing something useful.</p>

<p>Thanks,
Akash</p>
"
396,for loop on R function,"<p>I'm new to R (and programming generally), and confused about why the following bits of code yield different results:</p>

<pre><code>x &lt;- 100

for(i in 1:5){
  x &lt;- x + 1
  print(x)
}
</code></pre>

<p>This incrementally prints the sequence 101:105 as I would expect.</p>

<pre><code>x &lt;- 100

f &lt;- function(){
  x &lt;- x + 1
  print(x)
}

for(i in 1:5){
  f()
}
</code></pre>

<p>But this just prints 101 five times. </p>

<p>Why does packaging the logic into a function cause it to revert to the original value on each iteration rather than incrementing? And what can I do to make this work as a repeatedly-called function?</p>
"
397,xtable output is zero length when called from function,"<p>I want to output latex versions of my tables. I use xtable() to do so, with one major problem. When I try to generate latex tables from within a function that I call (which typically does lots of other things as well), the files get written as zero length! (I run Windows 7 x64, and use R 2.11).</p>

<p>Example:</p>

<pre><code>fnc &lt;- function (my.table) {
    sink(file=""paper/tables/output.tex"",caption=""my caption"")
    xtable(my.table)
    sink()
}
</code></pre>

<p>When I run the three lines inside the function directly, the file is written correctly. When I call fnc(my.table), the file is written as zero-length.</p>

<p>Why? Thanks!</p>
"
398,Which set of python libraries i should learn for AI and Data mining stuff,"<p>I am learning python and django. I need to do extensive stuff for data parsing , stats , AI and data mining. There are many libraries available. so I want to know which SET I should learn. Currently I have the following SET in My Mind.</p>

<ul>
<li>Parsing: Beautiful Soup</li>
<li>Search Engine: Whoosh  (Will it crawl and index the website or not??)</li>
<li>AI: PyBrain</li>
<li>Data Mining: orange (I am not sure bout this, Anything else)</li>
</ul>

<p>So I want to know is there anything else which I need to learn for that fields.
Or Some libraries which someone has tried. basically I want libraries which i can use from inside Django , I don't want different frameworks for those.</p>
"
399,Need code for Inverse Error Function,"<p>Does anyone know where I could find code for the ""Inverse Error Function?"" Freepascal/Delphi would be preferable but C/C++ would be fine too.</p>

<p>The TMath/DMath library did not have it :( </p>
"
400,Hough Transform question,"<p>I implemented Hough Transform in C#  this way:</p>

<pre><code>        List&lt;Point&gt; forme = new List&lt;Point&gt;();

        forme.Add(new Point(260, 307));
        forme.Add(new Point(268, 302));
        forme.Add(new Point(273, 299));
        forme.Add(new Point(279, 295));
        forme.Add(new Point(285, 292));
        forme.Add(new Point(291, 288));
        forme.Add(new Point(298, 283));
        forme.Add(new Point(305, 280));
        forme.Add(new Point(312, 277));
        forme.Add(new Point(319, 274));
        forme.Add(new Point(325, 271));
        forme.Add(new Point(333, 268));
        forme.Add(new Point(340, 264));
        forme.Add(new Point(350, 259));
        forme.Add(new Point(356, 256));

            int width =  Math.Abs(forme[0].X - forme[forme.Count - 1].X);
            int height =  Math.Abs(forme[0].Y - forme[forme.Count - 1].Y);

            int halfWidth = width / 2; int halfHeigh = height / 2;

            double pmax = Math.Sqrt((width * width) + (height * height));
            double tmax = Math.PI * 2;

            // step sizes
            double dp = pmax / (double)width;
            double dt = tmax / (double)height;

            int[,] A = new int[width , height]; // accumulator array

            foreach (Point p in forme)
            { 

               for (int Theta = 1; Theta &lt; height; Theta++)
                        {
                            double radius = ((double)(p.X) * Math.Cos(dt * (double)Theta)) + ((double)(p.Y) * Math.Sin(dt * (double)Theta)) ;

                            int k = (int)((radius / pmax) * width);
                            if (k &gt;= 0 &amp;&amp; k &lt; width) A[k, Theta]++;
                        }

            }
            int goodTheta = 0;
            int goodRadius = 0;

            // maxMapIntensity c'est l'intensité maximale
            int maxMapIntensity = 0;
            for (int radius = 0; radius &lt; width; radius++)
            {
                for (int theta = 0; theta &lt; height; theta++)
                {
                    if (A[radius, theta] &gt; maxMapIntensity)
                    {
                        maxMapIntensity = A[radius, theta];
                        goodRadius = radius;
                        goodTheta = theta;
                    }
                 }
            }
</code></pre>

<p>So, up to my understanding, i have now found the theta and radius of the intersecting point of all the curves. Then how can i find the real line ?</p>

<p>Some claim that I need to find the slope and a point, but it is really not clear to me what to do now.</p>

<p>Thanks for help, Jonathan</p>
"
401,Understandable documentation about R?,"<p>Does there exist understandable Free documentation for the R programming language, which is accessible to statistics-impaired people?<br />
The ideal documentation would include also motivation (an example) for using each statistical function.</p>
"
402,Animation problems with R,"<p>Using the animation package, I am trying to show fish movement over a jpeg of our study area. I want to show fish detection at each of 5 sites on each day they are detected.  Each of the five fish species will have their own color (not coded yet).  I am able to create the animation, though the data is not plotting correctly, and I feel that the problem lies with a loop that I can't seem to figure out.  I only want fish to ""show"" when they are detected, which is provided in the ""hits"" column, where 1 = hit, 0 = no hit.</p>

<p>The data(tagdata) looks like this:</p>

<pre><code>Day Tag Species Receiver    hits    x   y
1   1   xbow    10  1   105 350
1   2   cut 20  0   105 630
1   3   smilie  30  0   185 325
1   4   coho    40  0   280 250
1   5   mar 50  1   380 315
1   6   xbow    10  1   105 350
2   1   xbow    10  0   105 350
2   2   cut 20  1   105 630
2   3   smilie  30  1   185 325
2   4   coho    40  0   280 250
2   5   mar 50  0   380 315
3   1   xbow    10  0   105 350
3   2   cut 20  0   105 630
3   3   smilie  30  1   185 325
3   4   coho    40  1   280 250
3   5   mar 50  1   380 315
</code></pre>

<p>And my code:</p>

<pre><code>saveHTML({
      nmax &lt;- length(unique(tagdata$Day))
  # set the interval (secs) between panels
    oopt &lt;- ani.options(interval = 1, nmax = nmax)
  # plot the jpeg 
  for( i in 1 : nmax){
    plot(BCan, xlim = c(50, 800), ylim = c(200, 700))
  # to keep the map (jpeg) from disappearing at each iteration
    par(new = TRUE)
  # Plot the bubbles
    symbols(tagdata$x[i], tagdata$y[i], circles = tagdata$hits[i], inches = 0.35, xlim = c(50, 800), ylim = c(200, 700), text = i)
    text(tagdata$x, tagdata$y, tagdata$Species)
    }
  },

  # save the file
    img.name = ""Box.Canyon"", htmlfile = ""Box.Canyon.html"", 
  # set the height and width of the animation, and save it in the working directory
    ani.height = 500, ani.width = 600, outdir = getwd(),
    title = ""Box Canyon fish movement animation"",
    description = c(""Attempt to show fish movement over the Box Canyon map""))
</code></pre>
"
403,"R - where should I place RDA file - /R, /data, /inst/extdata?","<p>According to the ""Writing R Extensions"" manual, there are three directories where RDA files can be placed: <code>/R</code>, <code>/data</code>, <code>/inst/extdata</code></p>

<p>Its really hard to decipher the best-practice from the manual.  Could someone comment on when/why to place RDA files in each of these three directories.</p>

<p>Here's the specific cases I'm solving for:<br>
I have 1 RDA file that will be used in function examples as well as in my test_that tests (which all live in inst/tests)</p>
"
404,Likelihood Function,"<p>How do we write the likelihood function of a problem that consists of more than one x variable?  </p>

<p>(In my case it is x1, x2 and x3)</p>
"
405,What is the right way to round dividing to integers?,"<p>I am dividing 2 integers and looking to get an integer as a result.  I want the right <code>Math.Round()</code> method so when I divide, it always round to the next integer regardless of how.  Here are a bunch of examples with expectations below.</p>

<pre><code>int result = (6/4); // = 2
int result = (3/4);  // = 1
int result = (1/4);  // = 1
int result = (8/4);  // = 2
int result = (9/4);  // = 3
</code></pre>

<p>What is the correct way of doing this?</p>
"
406,How to extract tabular summary data from an lm command in R,"<p>I have data structured the following way:</p>

<pre><code>group_id, months_from_start, perc_total_downloads, experience_ratio
1             1                    1.2                4
1             2                    1.7                6
…
235           1                    6.7                3
235           2                   18                  8
…
</code></pre>

<p>There are about 300 groups, each of which have 70 or so consecutive data elements.  </p>

<p>I've issued the following script to estimate a second order polynomial for each of the groups.  </p>

<pre><code>s.1&lt;-lm(xts(s[s$group_id == 1,][,-2], order.by=as.Date(s[s$group_id == 1,][,2]))$perc_total_downloads ~ poly(xts(s[s$group_id == 1,][,-2], order.by=as.Date(s[s$group_id == 1,][,2]))$months_from_start, 2, raw=TRUE))
s.235&lt;-lm(xts(s[s$group_id == 235,][,-2], order.by=as.Date(s[s$group_id == 235,][,2]))$perc_total_downloads ~ poly(xts(s[s$group_id == 235,][,-2], order.by=as.Date(s[s$group_id == 235,][,2]))$months_from_start, 2, raw=TRUE))
s.599&lt;-lm(xts(s[s$group_id == 599,][,-2], order.by=as.Date(s[s$group_id == 599,][,2]))$perc_total_downloads ~ poly(xts(s[s$group_id == 599,][,-2], order.by=as.Date(s[s$group_id == 599,][,2]))$months_from_start, 2, raw=TRUE))
s.1111&lt;-lm(xts(s[s$group_id == 1111,][,-2], order.by=as.Date(s[s$group_id == 1111,][,2]))$perc_total_downloads ~ poly(xts(s[s$group_id == 1111,][,-2], order.by=as.Date(s[s$group_id == 1111,][,2]))$months_from_start, 2, raw=TRUE))
s.1537&lt;-lm(xts(s[s$group_id == 1537,][,-2], order.by=as.Date(s[s$group_id == 1537,][,2]))$perc_total_downloads ~ poly(xts(s[s$group_id == 1537,][,-2], order.by=as.Date(s[s$group_id == 1537,][,2]))$months_from_start, 2, raw=TRUE))
</code></pre>

<p>For each one of these new variables I can issue a summary statement to reveal interesting information:</p>

<pre><code>&gt; summary(s.44375)

Call:
lm(formula = xts(s[s$group_id == 44375, ][, -2], order.by = as.Date(s[s$group_id == 
44375, ][, 2]))$perc_total_downloads ~ poly(xts(s[s$group_id == 
44375, ][, -2], order.by = as.Date(s[s$group_id == 44375, 
][, 2]))$months_from_start, 2, raw = TRUE))


Residuals:
       Min         1Q     Median         3Q        Max 
-0.0064004 -0.0017315 -0.0002022  0.0012087  0.0078436 


Coefficients: (3 not defined because of singularities)
                                                                                                                                   Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                                                                                                                       1.993e-03  1.137e-03   1.753    0.084 .  
poly(xts(s[s$group_id == 44375, ][, -2], order.by = as.Date(s[s$group_id == 44375, ][, 2]))$months_from_start, 2, raw = TRUE)1.0  7.769e-04  6.707e-05  11.583   &lt;2e-16 ***
poly(xts(s[s$group_id == 44375, ][, -2], order.by = as.Date(s[s$group_id == 44375, ][, 2]))$months_from_start, 2, raw = TRUE)2.0 -9.258e-06  8.404e-07 -11.017   &lt;2e-16 ***
poly(xts(s[s$group_id == 44375, ][, -2], order.by = as.Date(s[s$group_id == 44375, ][, 2]))$months_from_start, 2, raw = TRUE)0.1         NA         NA      NA       NA    
poly(xts(s[s$group_id == 44375, ][, -2], order.by = as.Date(s[s$group_id == 44375, ][, 2]))$months_from_start, 2, raw = TRUE)1.1         NA         NA      NA       NA    
poly(xts(s[s$group_id == 44375, ][, -2], order.by = as.Date(s[s$group_id == 44375, ][, 2]))$months_from_start, 2, raw = TRUE)0.2         NA         NA      NA       NA    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 


Residual standard error: 0.002866 on 69 degrees of freedom
Multiple R-squared: 0.6619,Adjusted R-squared: 0.6521 
F-statistic: 67.53 on 2 and 69 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>For my purpose I need to transcribe this information into a table, which is incredibly tedious and time consuming cutting and pasting from this format:</p>

<pre><code>group_id   intercept est  intercept stnd err    intercept t value   …
44375         1.993e-03         1/137e-03           1.753          ...
…
</code></pre>

<p>It would also be convenient for me to have conventional notation rather than scientific notation, but I imagine I could live without that.</p>

<p>Is there any way for me to do this without cutting and pasting by hand?</p>

<p>Thanks  --sw</p>
"
407,Explain R tapply description,"<p>I understand what tapply() does in R.  However, I cannot parse this description of it from the documentaion:</p>

<pre>

Apply a Function Over a ""Ragged"" Array

Description:

     Apply a function to each cell of a ragged array, that is to each
     (non-empty) group of values given by a unique combination of the
     levels of certain factors.

Usage:

     tapply(X, INDEX, FUN = NULL, ..., simplify = TRUE)
</pre>

<p>When I think of tapply, I think of group by in sql.  You group values in X together by its parallel factor levels in INDEX and apply FUN to those groups.  I have read the description of tapply 100 times and still can't figure out how what it says maps to how I understand tapply.  Perhaps someone can help me parse it?</p>
"
408,How to analyze panel data using nested logit model in R,"<p>So after a week's research into R I still can't find a solution to my problem.. Here it goes:</p>

<p>I have a panel data set, which has 10 countries, and each country has 5 time series variables. I want to estimate the status for each country at each particular time. The status are nested.</p>

<p>Therefore I'm considering using mlogit() in R to adopt this analysis. However I can't convert my dataset into the mlogit.data() format...</p>

<p>So I'd like to ask if anyone knows how to do the conversion? Particularly how to deal with the time series variables??</p>

<p>Many thanks!!</p>
"
409,"Link between time and complex amplitude argument, DFT","<p>It's more math question than programming, but..
I have some discrete points, with time t_n and value u(t_n). i perform discrete fourier transormation using values u(t_n). now i have some complex values, as i understood absolute value is amplitude and argument is frequency, right? well my teacher said i need to compute SOMETHING using t_n's,u(t_n)'s, A_n's and w_n's(assuming A_n is absolute value, w_n is argument).
It's somehow related to statistics variance and uncertainity principle. As you see, i don know a damn about fourier transformations :D Now i've made a program that performs DFT and builds a plot using w_n's as Ox axis and A_n's as Oy axis. It looks like this: <a href=""http://yfrog.com/muscreenshot2otp"" rel=""nofollow"">screenshot</a></p>

<p>What my teacher wants me to compute and which values shud i expect as corresponding to theory?</p>
"
410,"in R get the difference between dates in terms of weeks, months, quarters, years","<p>I have two dates let´s say <code>14.01.2013</code> and <code>26.03.2014</code>. I would like to get the difference between those two dates in terms of weeks(?),months(in the example 14), quarters(4) and years(1). Do you know the best way to get this? Thanks for your help.</p>
"
411,interpolation of sparse grid using python (preferably scipy),"<p>I have a large (2000 x 2000) pixel grid that have values defined at only certain (x,y) coordinates. For example, a simplified version of this would look like this:</p>

<pre><code>-5-3--
---0--
-6--4-
-4-5--
---0--
-6--4-
</code></pre>

<p>How can I do linear interpolation or nearest neighbor interpolation so that I can have a defined value at every location in the grid.</p>
"
412,Creating a Unique Sequence of Dates,"<p>Let's say that I want to generate a data frame which contains a column with 
is structured in the following format.</p>

<pre><code>2011-08-01
2011-08-02
2011-08-03
2011-08-04
...
</code></pre>

<p>I want to know if it's possible to generate this data with the <code>seq()</code> command.</p>

<p>Something like the following: (obviously doesn't work)</p>

<pre><code>seq(2011-08-01:2011-08-31)
</code></pre>

<p>Would I instead have to use toDate and regex to generate this date in this
specific format.</p>
"
413,simple R project,"<p>Hi I'm new to R and i'm building off of two guides from the web, I figured out how to automate a script for data mining but instead of appending the data is then over written each time the code is run. I would like to have it appended can any one point me in the right direction.</p>

<p>here is the script as such</p>

<pre><code># loading the package is required once each session
require(XML)

# initialize a storage variable for Twitter tweets
mydata.vectors &lt;- character(0)

# paginate to get more tweets
for (page in c(1:15))
{
    # search parameter
    twitter_q &lt;- URLencode('#google OR #apple')
    # construct a URL
    twitter_url = paste('http://search.twitter.com/search.atom?q=',twitter_q,'&amp;rpp=100&amp;page=', page, sep='')
    # fetch remote URL and parse
    mydata.xml &lt;- xmlParseDoc(twitter_url, asText=F)
    # extract the titles
    mydata.vector &lt;- xpathSApply(mydata.xml, '//s:entry/s:title', xmlValue, namespaces =c('s'='http://www.w3.org/2005/Atom'))
    # aggregate new tweets with previous tweets
    mydata.vectors &lt;- c(mydata.vector, mydata.vectors)
}

# how many tweets did we get?
length(mydata.vectors)
</code></pre>
"
414,Any R function to translate amino acid sequences to nucleotide sequences?,"<p>Wondering if there is any R function available, since I'm using the package Phangorn and want to convert the amino acid sequences back in order to calculate pairwise distances at the nucleotide level. </p>
"
415,How can I calculate the probality of to get a number with 15 digits lenght repeat,"<p>A number with nineteen digits is generated randomically by a particular system and it's guaranteed that every number generated is unique (by the system provider). </p>

<p>If I chunk this number and get the last fifteen digits, from right to the left, for example:</p>

<p>0123456789012345678 -> <strong>456789012345678</strong></p>

<p>Assuming that this procedure will be repeat for undefined and unlimited times it's possible to verify if will be possible to get repeated number?  How would be the chance or the probability to get a repeated number in this new sub selection or subset? </p>

<p>How can I calculate this or explain mathematically this probability?</p>

<p>Thanks </p>
"
416,Computing complex math equations in python,"<p>Are there any libraries or techniques that simplify computing equations ?</p>

<p>Take the following two examples:</p>

<ul>
<li><p>F = B * { [ a * b * sumOf (A / B  ''' for all i ''' ) ] / [ sumOf(c * d * j) ] }</p>

<p>where:</p>

<p>F = cost from i to j</p>

<p>B, a, b, c, d, j are all vectors in the format [ [zone_i, zone_j, cost_of_i_to_j], [..]]</p>

<p>This should produce a vector F [ [1,2, F_1_2], ...,  [i,j, F_i_j] ]</p></li>
<li><p>T_ij = [ P_i * A_i * F_i_j]  / [ SumOf [ Aj * F_i_j ] // j = 1 to j = n ]</p>

<p>where:</p>

<p>n is the number of zones</p>

<p>T = vector [ [1, 2, A_1_2, P_1_2], ...,  [i, j, A_i_j, P_i_j] ]</p>

<p>F = vector [1, 2, F_1_2], ..., [i, j, F_i_j] </p>

<p>so P_i would be the sum of all P_i_j for all j and Aj would be sum of all P_j for all i</p></li>
</ul>

<p>I'm not sure what I'm looking for, but perhaps a parser for these equations or methods to deal with multiple multiplications and products between vectors?</p>

<p>To calculate some of the factors, for example A_j, this is what i use</p>

<pre><code>from collections import defaultdict

A_j_dict = defaultdict(float)
for A_item in TG: A_j_dict[A_item[1]] += A_item[3]
</code></pre>

<p>Although this works fine, I really feel that it is a brute force / hacking method and unmaintainable in the case we want to add more variables or parameters. Are there any math equation parsers you'd recommend?</p>

<p><strong>Side Note:</strong> These equations are used to model travel. Currently I use excel to solve a lot of these equations; and I find that process to be daunting. I'd rather move to python where it pulls the data directly from our database (postgres) and outputs the results into the database. All that is figured out. I'm just struggling with evaluating the equations themselves.</p>

<p>Thanks :)</p>
"
417,Making a Game in JavaScript,"<p>This doesn't really have to just be in JavaScript but in all Languages where games can be made. I just don't get it. I just don't get how something like:</p>

<pre><code>B.Gd = function () {
    var a = this.na,
        b = this.Hb(),
        c = this.ba || 1,
        d = c / a,
        e;
    if (this.b) {
        this.xb &amp;&amp; this.xb.contains(b) &amp;&amp; (e = Ka(this.xb.size()) / Ka(b.size())) &amp;&amp; 1.6 &gt; e &amp;&amp; 0.5 &lt; e ? b = this.xb : 1 != this.He &amp;&amp; 0 != this.g.length &amp;&amp; (this instanceof C || b.expand(12, 12, 12, 12));
        this.xb = b;
        var f = b.size();
        e = f.e()
            .scale(c)
            .ceil();
        if (this.b.width != e.width || this.b.height != e.height) this.b.width = e.width, this.b.height = e.height, this.Lb = 1;
        var g = this.C.e();
        this.m[D] &amp;&amp; (g = this.m[D]);
        0 != e.width ? g.scale(f.width * d / e.width) : g.scale(1 / a);
        a = E(this);
        this.Vb = (a.left - b.left) * c;
        this.Wb = (a.top - b.top) * c;
        b = this.f()
            .e();
        a = this.Y;
        b.width *= a.x;
        b.height *= a.y;
        b = b.scale(c);
        c = this.d.e();
        this.m[F] &amp;&amp; (c = this.m[F]);
        c.x *= d;
        c.y *= d;
        c.x -= b.width + this.Vb;
        c.y -= b.height + this.Wb;
        Oa(this.b, 100 * ((this.Vb + b.width) / e.width), 100 * ((this.Wb + b.height) / e.height), k);
        !this.T[F] &amp;&amp; (!this.T[D] &amp;&amp; !this.T[Pa]) &amp;&amp; (d = -(this.J %= 360), s(this.m[Pa]) &amp;&amp; (d = -this.m[Pa]), Qa(this.b, Ra(new Sa, 0.1)
            .translate(c.x, c.y)
            .scale(g.x, g.y)
            .rotate(d)));
        this.Lb &amp;&amp; (d = this.b.getContext(""2d""), c = this.ba || 1, d.clearRect(0,
        0, e.width, e.height), d.save(), d.translate(this.Vb, this.Wb), d.scale(c, c), e = this.f(), g = this.Y, d.translate(e.width * g.x, e.height * g.y), this.q.Zc.call(this, d), d.restore(), this.Lb = 0)
    }
};
</code></pre>

<p>In the end will come out to be a game. I'm always amazed when I look at source codes of games such as this one: ""http://www.limejs.com/static/zlizer/index.html"" which is where I got that little snippet of code from (http://www.limejs.com/static/zlizer/zlizer.js). I just want to know, can anyone make sense of this? Can anyone explain to me how people write this kinda stuff? If I were to make a game I wouldn't know how to start. Or end.</p>

<p>This is more a question for people who actually have made a game that required Math like the one above.</p>

<p>How do you guys do it? It's so amazing and it seems so difficult. I'm not planning on making a game, this is just a question I was wondering and I wanted some kind of answer.</p>
"
418,R: Comparing oddly indexed vectors,"<p>I have some data with some hard to deal with properties. There are two vectors that are taking a measure of quality (from 0-1) at points along a physical object. These measurements are indexed according to the distance the measurement was taken from the bottom of the object. Then, a quality improving transformation is applied to the object, and measurements are taken again. However, the number of measurements are not the same, nor are the points on which they are taken.</p>

<p>In R, the data looks something like this (but with many more points)</p>

<p>Before transformation:</p>

<pre><code>     value index
[1,]   0.3     6
[2,]   0.6    16
[3,]   0.1    25
[4,]   0.8    37
[5,]   0.2    46
[6,]   0.4    58
[7,]   0.4    64
[8,]   0.2    76
</code></pre>

<p>After transformation:</p>

<pre><code>      value index
 [1,]   0.3     1
 [2,]   0.5     9
 [3,]   0.7    18
 [4,]   0.4    30
 [5,]   0.9    44
 [6,]   0.3    48
 [7,]   0.4    61
 [8,]   0.5    66
 [9,]   0.3    76
[10,]   0.1    85
</code></pre>

<p>Under the assumption that quality along the object is continuous (if not observed at every point), and that the ammount of improvement during the tranformation is dependent on the point along the object, I would like to be able to show the distribution of quality improvement.</p>

<p>Since there are different numbers of measurements, and different indexes, I don't think</p>

<pre><code>plot(density(after$value - before$value)) 
</code></pre>

<p>is what I'm looking for. My question is, is there a sane way to take that difference such that I have a number of observations for how much quality improved? Or am I going to be stuck looking at the difference in means?</p>
"
419,"Using Pylab, Do matrix operations!","<p>Here's this question I have to solve..</p>

<p>Read the image <a href=""http://www.imagemagick.org/Usage/img_photos/taj_mahal_orig.jpg"" rel=""nofollow"">taj mahal orig</a> into (2-D image matrix) variable A.<br>
What is the size of the image matrix ? Display this image.</p>

<p>I  want to do this using pylab in python . But just can't understand how to do it. Can anyone help me with this? </p>
"
420,Generating data.frame objects with a for loop,"<p>Is there a way to dynamically create a data.frame (or any object for that matter) without it being initialized as a variable? For example: </p>

<pre><code>for(x in 1:26) { 
paste(letters[x]) &lt;- df[x]
}
</code></pre>

<p>Such that you have new variables a,b,c,d,...,z? </p>
"
421,Weird behaviour of np.sqrt for large integers,"<pre><code>&gt;&gt;&gt; np.__version__
'1.7.0'
&gt;&gt;&gt; np.sqrt(10000000000000000000)
3162277660.1683793
&gt;&gt;&gt; np.sqrt(100000000000000000000.)
10000000000.0
&gt;&gt;&gt; np.sqrt(100000000000000000000)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
AttributeError: sqrt
</code></pre>

<p>Huh... <code>AttributeError: sqrt</code> what's going on here then?  <code>math.sqrt</code> doesn't seem to have the same problem.  </p>
"
422,"make a list of lm objects, retain their class","<p>Apologies for such a rudimentary question--I must be missing something obvious.</p>

<p>I want to build a list of <code>lm</code> objects, which I'm then going to use in an <code>llply</code> call to perform mediation analysis on this list. But this is immaterial--I just first want to make a list of length m (where m is the set of models) and each element within m will itself contain n <code>lm</code> objects. </p>

<p>So in this simple example</p>

<pre><code>d1 &lt;- data.frame(x1 = runif(100, 0, 1),
             x2 = runif(100, 0, 1),
             x3 = runif(100, 0, 1),
             y1 = runif(100, 0, 1),
             y2 = runif(100, 0, 1),
             y3 = runif(100, 0, 1))

m1 &lt;- lm(y1 ~ x1 + x2 + x3, data = d1)
m2 &lt;- lm(x1 ~ x2 + x3, data = d1)
m3 &lt;- lm(y2 ~ x1 + x2 + x3, data = d1)
m4 &lt;- lm(x2 ~ x1 + x3, data = d1)
m5 &lt;- lm(y3 ~ y1 + y2 + x3, data = d1)
m6 &lt;- lm(x3 ~ x1 + x2, data = d1)
</code></pre>

<p>I want a list containing 3 elements, and the first element will contain <code>m1</code> and <code>m2</code>, the second will contain <code>m3</code> and <code>m4</code>, etc. My initial attempt is sort of right, but the lmm objects don't retain their class.</p>

<pre><code>mlist &lt;- list(c(m1,m2),
              c(m3,m4),
              c(m5,m6))
</code></pre>

<p>It has the right length (ie <code>length(mlist)</code> equals 3), but I thought I could access the <code>lm</code> object itself with</p>

<pre><code>class(mlist[1][[1]])
</code></pre>

<p>but this element is apparently a list.</p>

<p>Am I screwing up how I build the list in the first step, or is this something more fundamental regarding lm objects?</p>
"
423,Make a function which draw a plot and save it,"<p>I have the following dataset.</p>

<pre><code>structure(list(Rf = c(60.105, 62.205, 64.305, 64.305, 66.405, 
66.405), Es = c(0, -0.07, -0.36, -0.47, -0.39, -1.54), H = c(32.3, 
-6.9, -5.59, -14.4, -6.5, -21), S = c(267, 136, 151, 114, 143, 
90.4), G = c(-46.8, -47.3, -50.7, -48.5, -49, -47.8)), .Names = c(""Rf"", 
""Es"", ""H"", ""S"", ""G""), class = ""data.frame"", row.names = c(""Me"", 
""Et"", ""Pr"", ""iPr"", ""Bu"", ""tBu""))
</code></pre>

<p>I need to plot Rf vs H, S and G, and the same for Es column. There is six plots in total.</p>

<p>To draw single plot I use the following function:</p>

<pre><code>ggplot(data=df, aes(x=Rf, y=H, label=row.names(df))) + 
  geom_point(size=4) +
  geom_text(vjust=2) + 
  ylab(expression(list(Delta*H^o,~kJ/mol))) + 
  xlab(""Molecular refraction"") + 
  ylim((min(df$H) - 0.2*(abs(min(df$H)))), max(df$H)) + 
  opts(axis.line = theme_segment(size=1),
       axis.text.x = theme_text(colour=""black"", size=15),
       axis.text.y = theme_text(colour=""black"", size=15),
       axis.title.x = theme_text(colour=""black"", size=15),
       axis.title.y = theme_text(colour=""black"", size=15, angle=90),
       panel.background=theme_rect(colour=""white""),
       panel.grid.minor = theme_blank(), 
       panel.grid.major = theme_blank())
</code></pre>

<p><img src=""http://i.stack.imgur.com/2Sr9t.jpg"" alt=""enter image description here""></p>

<p>I want to create a function like</p>

<pre><code>f1 &lt;- function(x.label, y.label, df, xtitle, filename) {
  g &lt;- ggplot(data=df, aes(x=x.label, y=y.label, label=row.names(df))) + 
    geom_point(size=4) +
    geom_text(vjust=2) + 
    ylab(expression(list(Delta*y.label^o,~kJ/mol))) + 
    xlab(xtitle) + 
    ylim((min(df[,y.label]) - 0.2*(abs(min(df[,y.label])))), max(df[,y.label])) + 
    opts(axis.line = theme_segment(size=1),
         axis.text.x = theme_text(colour=""black"", size=15),
         axis.text.y = theme_text(colour=""black"", size=15),
         axis.title.x = theme_text(colour=""black"", size=15),
         axis.title.y = theme_text(colour=""black"", size=15, angle=90),
         panel.background=theme_rect(colour=""white""),
         panel.grid.minor = theme_blank(), 
         panel.grid.major = theme_blank())
  ggsave(filename, g)
}
</code></pre>

<p>To call it for the example above like</p>

<pre><code>f1(Rf, H, df, ""Molecular refraction"", ""D:/temp/1.jpg"")
</code></pre>

<p>The difficulty is how to transfer and correctly handle <code>x.label</code> and <code>y.label</code>, since they are used in <code>aes</code> option of <code>ggplot</code>, <code>ylab(expression())</code> and <code>ylim</code> calls. Curent function return an error <code>Error in eval(expr, envir, enclos) : object 'x.label' not found</code></p>

<p><strong>ANSWER:</strong></p>

<p>The following function works correctly. <code>aes_string</code> should be used insted of <code>aes</code> in <code>ggplot</code> option, and <code>substitute</code> instead of <code>expression</code> in <code>ylab</code>.</p>

<pre><code>f1 &lt;- function(x.label, y.label, df, x.title, filename) {
  g &lt;- ggplot(data=df, aes_string(x=x.label, y=y.label)) + 
    geom_point(size=4) +
    geom_text(aes(label=row.names(df)), vjust=2) + 
    ylab(substitute(list(Delta*y.label^o,~kJ/mol), list(y.label=y.label))) + 
    xlab(x.title) + 
    ylim((min(df[,y.label]) - 0.2*(abs(min(df[,y.label])))), max(df[,y.label])) + 
    opts(axis.line = theme_segment(size=1),
         axis.text.x = theme_text(colour=""black"", size=15),
         axis.text.y = theme_text(colour=""black"", size=15),
         axis.title.x = theme_text(colour=""black"", size=15),
         axis.title.y = theme_text(colour=""black"", size=15, angle=90),
         panel.background = theme_rect(colour=""white""),
         panel.grid.minor = theme_blank(), 
         panel.grid.major = theme_blank())
  ggsave(filename=filename, plot=g)
}
</code></pre>
"
424,"A tourist in France wants to visit 12 different cities, find the probability.","<p>The Question :</p>

<blockquote>
  <p>A tourist in France wants to visit 12 different cities. If the route is randomly selected, what is the probability that she will
  visit the cities in alphabetical order?</p>
</blockquote>

<p>I thought about this in two ways and I got two different answers, both of them look correct to me ! but only one is correct.</p>

<p><code>1/144</code> (12 cities times 12 times to make the arrangement) </p>

<p><code>1/479001600</code> (which is generated by the permutation 12 P 12)</p>

<p>Which one is correct? and is there a better answer?</p>

<p>Please explain how do you know if the problem is permutation or combination or just simple multiplication.</p>
"
425,sample sizes.. which formulas to use?,"<p>Find the minimum sample size needed to estimate, within two percent, the percentage of US voters who intend to vote Republican in the next election. Use 90% confidence and assume a previous poll indicates 42% intend to vote Republican.</p>

<p>90 percent confidence = 1.645
thats all i get then im stumped.  which formula can i plug any info into?</p>
"
426,R - World Contour Map with Correlation between two data sets,"<p>I have two data sets A and B and i wanna to find the correlation and plot the contour map.</p>

<p>A is just a simple vector with 230 stream flow data.</p>

<p>B is a complicated sea surface temperature(SST) data under a series date. On each date, the SST has a matrix of 360row *180columns of recorded temperatures.</p>

<p>The vector A (230 data) is :</p>

<pre><code>Houlgrave_flow_1981_2000 = window(Houlgrave_flow_average, start = as.Date(""1981-11-15""),end = as.Date(""2000-12-15"")) 
Houlgrave_SF_1981_2000 = coredata(Houlgrave_flow_1981_2000)
</code></pre>

<p>The dimension of matrix B  is shown below and i only use from 1 to 230.</p>

<pre><code>&gt; dim(ssta_sst)
[1] 360 180 362
</code></pre>

<p>My idea for finding correlation is below. </p>

<pre><code>z_correlation = cor(Houlgrave_SF_SST_1981_2000,ssta_sst[c(181:360, 1:180),,i])
</code></pre>

<p>Try, i=1. However, it doesn't work.The error message says:</p>

<pre><code>""Error in cor(Houlgrave_SF_SST_1981_2000, ssta_sst[c(181:360, 1:180), ,  : 
  incompatible dimensions."".
</code></pre>

<p>Also, this is my contour map code, </p>

<pre><code>require(maps)
par(ask=TRUE)
for (i in 1:230) {
    maps::map(database=""world"", fill=TRUE, col=""light blue"")
    maps::map.axes()
    contour(x=lon_sst, y=lat_sst, z=cor(Houlgrave_SF_1981_2000,ssta_sst[c(181:360, 1:180),,i]), zlim=c(-3,3), add=TRUE)
    title(paste(""Year: "", year_sst[i], "", Month: "", month_sst[i]))
}
</code></pre>

<p>I think i just need to modify z under contour code. Is it necessary to redefine each A's data as a 360*180 data matrix?</p>
"
427,converting one column of data into two columns in R,"<p>I have a text file that has one colunm of 1800 entries that are actually two sets of data (size,age) that have been combined. The first number (row1,column1) is size 1, the second number (row2,column1) is age1, the third column (row3,column1) is size 2, the fourth number (row4,column1) is age2, and so on. Does anyone know how to split this into a matrix of two columns of 900 entries each?</p>
"
428,List coordinates between a set of coordinates,"<p>This should be fairly easy, but I'm getting a headache from trying to figure it out. I want to list all the coordinates between two points. Like so:</p>

<p>1: (1,1)
2: (1,3)
In between: (1,2)</p>

<p>or</p>

<p>1: (1,1)
2: (5,1)
In between: (2,1), (3,1), (4,1)</p>

<p>It does not need to work with diagonals.</p>
"
429,how to have x-axis labels in multicolumn ggplot with facet_wrap?,"<p>When trying to plot something like this:</p>

<pre><code>library(ggplot2)
d &lt;- ggplot(diamonds, aes(carat, price)) +
  xlim(0, 2) + geom_point()
d + facet_wrap(~ color)
</code></pre>

<p>You will notice that the x-axis labels only show up for the first column. I would like them to repeate on the second and third column. Is this possible?</p>

<p>If on the facet_wrap I use the option scales=""free"", </p>

<pre><code>d + facet_wrap(~ color, scales=""free"")
</code></pre>

<p>then i get x-axis labels on all plots, which I also don't want. I want only labels in the bottom row repeating across columns</p>

<p>If the number of panels to be plot is such, that all columns have the same number of plots, the axis gets repeated in the way I want. But I can't always have the right number of panels for that.</p>
"
430,Math.random() and precision loss curiosity,"<p>The following does not compile:</p>

<pre><code>int result = Math.random() + 1;

error: possible loss of precision
    int result = Math.random() + 1;
                               ^
    required: int
    found:    double
</code></pre>

<p>but the following <strong>does</strong> compile:</p>

<pre><code>int result = 0;
result += Math.random() + 1;
</code></pre>

<p>Why?</p>

<p>Putting the compilable code into a nested loop, one would expect result to increment by 1 with each iteration because Math.random() always returns a double whose value is less than 1 and when added to an integer the fractional part would be lost due to precision loss.  Run the following code and see the unexpected result:</p>

<pre><code>public class MathRandomCuriosity
{
  public static void main(String[] args)
  {
    int result = 0;
    for (int i = 0; i &lt; 10; i++)
    {
      // System.out.println(result);
      for (int j = 0; j &lt; 20; j++)
      {
        // System.out.println(result);
        for (int k = 0; k &lt; 300; k++)
        {
          // System.out.println(result);
          for (int m = 0; m &lt; 7000; m++)
          {
            result += Math.random() + 1;
          }
        }
      }
    }
    System.out.println(result);
  }
}
</code></pre>

<p>With 10*20*300*7000 = 42,000,000 iterations the result should be 42,000,000.  But it's not!  The result varies i.e. 42,000,007 vs. 42,000,006 vs. 42,000,010 etc.</p>

<p>Why?</p>

<p>By the way...this is not code that is being used anywhere, it comes from a quiz I received in a newsletter.  The reason for the nested loops is so that I can view the value of result at intervals.</p>
"
431,How to hash a large object (dataset) in Python?,"<p>I would like to calculate a hash of a Python class containing a dataset for Machine Learning. The hash is meant to be used for caching, so I was thinking of <code>md5</code> or <code>sha1</code>.
The problem is that most of the data is stored in NumPy arrays; these do not provide a <code>__hash__()</code> member. Currently I do a <code>pickle.dumps()</code> for each member and calculate a hash based on these strings. However, I found the following links indicating that the same object could lead to different serialization strings:</p>

<ul>
<li><a href=""http://www.gossamer-threads.com/lists/python/python/735907"">Hash of None varies per machine</a></li>
<li><a href=""http://www.aminus.org/blogs/index.php/2007/11/03/pickle%5Fdumps%5Fnot%5Fsuitable%5Ffor%5Fhashing?blog=2"">Pickle.dumps not suitable for hashing</a></li>
</ul>

<p>What would be the best method to calculate a hash for a Python class containing Numpy arrays?</p>
"
432,Saving a data frame as a binary file,"<p>I would like to save a whole bunch of relatively large data frames while minimizing the space that the files take up. When opening the files, I need to be able to control what names they are given in the workspace. </p>

<p>Basically I'm looking for the symantics of dput and dget but with binary files.</p>

<p>Example:</p>

<pre><code>n&lt;-10000

for(i in 1:100){
    dat&lt;-data.frame(a=rep(c(""Item 1"",""Item 2""),n/2),b=rnorm(n),
     c=rnorm(n),d=rnorm(n),e=rnorm(n))
    dput(dat,paste(""data"",i,sep=""""))
}


##much later


##extract 3 random data sets and bind them
for(i in 1:10){
    nums&lt;-sample(1:100,3)
    comb&lt;-rbind(dget(paste(""data"",nums[1],sep="""")),
      dget(paste(""data"",nums[2],sep="""")),
      dget(paste(""data"",nums[3],sep="""")))
    ##do stuff here
}
</code></pre>
"
433,Naive Bayes row classification,"<p>How do you classify a row of seperate cells in MATLAB?</p>

<p>At the moment I can classify single coloums like so:</p>

<pre><code>training = [1;0;-1;-2;4;0;1]; % this is the sample data.
target_class = ['posi';'zero';'negi';'negi';'posi';'zero';'posi'];
% target_class are the different target classes for the training data; here 'positive' and 'negetive' are the two classes for the given training data

% Training and Testing the classifier (between positive and negative)
test = 10*randn(25, 1); % this is for testing. I am generating random numbers.
class  = classify(test,training, target_class, 'diaglinear')  % This command classifies the test data depening on the given training data using a Naive Bayes classifier
</code></pre>

<p>Unlike the above, I want to classify:</p>

<pre><code>        A   B   C
Row A | 1 | 1 | 1 = a house

Row B | 1 | 2 | 1 = a garden
</code></pre>

<p>Here's a code example from the MATLAB site:</p>

<pre><code>nb = NaiveBayes.fit(training, class)
nb = NaiveBayes.fit(..., 'param1', val1, 'param2', val2, ...)
</code></pre>

<p>I don't understand what <code>param1</code>, <code>val1</code>, etc. are. Can anyone help?</p>
"
434,How to remove a part of a string prior to a symbol in PHP,"<p>This should be simple to answer, I'm just very new to this all...</p>

<p>If I have the following (the 89 is NOT constant — it could be 2, sometimes 3 numbers — but the /100 is):</p>

<blockquote>
  <p>89/100</p>
</blockquote>

<p>how can I get just 89 saved as an integer?</p>
"
435,weka mac installation,"<p>Recently, I bought a new macbook and installed weka. When I double click on the weka icon, it opens and after around 2 seconds, it closes.</p>

<p>I've tried re-installing it several times and also tried different versions. However nothing seems to work.</p>

<p>I've also searched the internet for a solution but can't seem to find one.</p>

<p>Any help will be greatly appreciated!</p>

<p>Thanks a lot!</p>
"
436,Correlating word proximity,"<p>Let's say I have a text transcript of a dialogue over a period of aprox. 1 hour.  I want to know what words happen in close proximatey to one another.  What type of statistical technique would I use to determine what words are clustered together and how close their proximatey to one another is?</p>

<p>I'm suspecting some sort of cluster analysis or PCA.</p>
"
437,Solve logical and mathematical Equation,"<p>I would like to solve following equation:</p>

<pre><code>x^2-bitxor(2,x)=0
</code></pre>

<p>Is it possible ?</p>
"
438,How to select not only the maximum of a `numpy.ndarray` but the top 3 maximal values in python?,"<p>I have a list of float values (positive and negative ones) stored in a variable <code>row</code> of type <code>&lt;type 'numpy.ndarray'&gt;</code>.</p>

<pre><code>max_value = max(row)
</code></pre>

<p>gives me the maximal value of <code>row</code>. Is there an elegant way to select the top 3 (5, 10,...) values?</p>

<p>I came up with </p>

<ol>
<li>selecting the maximum value from <code>row</code></li>
<li>deleting the maximal value in <code>row</code></li>
<li>selecting the maximum value from <code>row</code></li>
<li>deleting the maximal value in <code>row</code></li>
<li>and so on</li>
</ol>

<p>But that's certainly an ugly style and not pythonic at all. What do the pythonistas say to that? :)</p>

<hr>

<h2>Edit</h2>

<p>I need not only the maximal three values, bit there position (index in <code>row</code>), too. Sorry, I forgot to mention that...</p>
"
439,Keeping a numpy ndarray subclass as the return value during a transformation. Safe to set __array_priority__?,"<p>I'm trying to subclass <code>numpy</code>'s <code>ndarray</code> class, and have had some luck. The behavior that I would like is nearly exactly the same as the <a href=""http://docs.scipy.org/doc/numpy/user/basics.subclassing.html#slightly-more-realistic-example-attribute-added-to-existing-array"" rel=""nofollow"">example</a> given in the documentation. I want to add a parameter <code>name</code> to the array (which I use to keep track of where the data originally came from).</p>

<pre><code>class Template(np.ndarray):
    """"""A subclass of numpy's n dimensional array that allows for a
    reference back to the name of the template it came from.
    """"""
    def __new__(cls, input_array, name=None):
        obj = np.asarray(input_array).view(cls)
        obj.name = name
        return obj

    def __array_finalize__(self, obj):
        if obj is None: return
        self.name = getattr(obj, 'name', None)
</code></pre>

<p>This works, except that, like <a href=""http://stackoverflow.com/questions/6190859/some-numpy-functions-return-ndarray-instead-of-my-subclass"">this question</a>, <strong>I want any transformation involving my subclass to return another instance of my subclass</strong>.</p>

<p>Sometimes numpy functions do return an instance of <code>Template</code>:</p>

<pre><code>&gt;&gt;&gt; a = Template(np.array([[1,2,3], [2,4,6]], name='from here')
&gt;&gt;&gt; np.dot(a, np.array([[1,0,0],[0,1,0],[0,0,1]]))
Template([[1, 2, 3],
       [2, 4, 6]])
</code></pre>

<p>However, sometimes they don't:</p>

<pre><code>&gt;&gt;&gt; np.dot(np.array([[1,0],[0,1]]), a)
array([[1, 2, 3],
       [2, 4, 6]])
</code></pre>

<p>In the question I linked to above, it was suggested that the OP should override the <code>__wrap_array__</code> method for the subclass. However, I don't see any justification in this. In some situations, I'm getting my expected behavior with the default <code>__array_wrap__</code>. <a href=""http://docs.scipy.org/doc/numpy/user/basics.subclassing.html#array-wrap-for-ufuncs"" rel=""nofollow"">The docs</a> seem to suggest that I'm running into a situation where it's the other argument's <code>__array_wrap__</code> method being called because of a higher <code>__array_priority__</code> value:</p>

<blockquote>
  <p>Note that the ufunc (<code>np.add</code>) has called the <code>__array_wrap__</code> method of
  the input with the highest <code>__array_priority__</code> value</p>
</blockquote>

<p>So my question has a couple related parts. First: <strong>can I set the <code>__array_priority__</code> attribute of my subclass such that its <code>__array_wrap__</code> will always be called?</strong> Second: <strong>Is this the best/easiest way to go about achieving my desired behavior?</strong> </p>
"
440,Vectorising a for loop,"<p>Is it possible to vectorise the following function, (<code>f</code>)?</p>

<p>I have a vector <code>x</code> for which I want to maximise the output value of the function <code>f</code> by altering <code>p</code>.</p>

<p>But the function is quite slow as it is not vectorised in anyway and was wondering if there was a good way to do so. The idea is to parallelise this in the future, and also potentially use <code>data.table</code> to speed it up</p>

<p>my real data is significantly larger...so I'm providing a mock example....</p>

<pre><code># My mock data 
x &lt;- data.frame(x=rep(c(rep(c(0.2,-0.2),4),0.2,0.2,-0.2,0.2),20))

# The function to optimise for
f &lt;- function(p,x){
    # Generate columns before filling
    x$multiplier &lt;- NA
    x$cumulative &lt;- NA

    for(i in 1:nrow(x)){
        # Going through each row systematically
        if(i==1){
            # If first row do a slightly different set of commands
            x[i,'multiplier'] &lt;- 1 * p
            x[i,'cumulative'] &lt;- (x[i,'multiplier'] * x[i,'x']) + 1
        } else {
            # For the rest of the rows carry out these commands
            x[i,'multiplier'] &lt;- x[i-1,'cumulative'] * p
            x[i,'cumulative'] &lt;- (x[i,'multiplier'] * x[i,'x']) + x[i-1,'cumulative']
        }
    }

# output the final row's output for the cumulative column
as.numeric(x[nrow(x),'cumulative'])
}

# Checking the function works by putting in a test value of p = 0.5
f(0.5,x)

# Now optimise the function between the interval of p between 0 and 1
optim.p &lt;- optimise(f=f, interval=c(0,1),x, maximum=TRUE)

# Viewing the output of optim.p
optim.p
</code></pre>
"
441,How to aggregate data in timeSequence?,"<p>I have a problem and would really need your help? My data (let's name it ""date"") looks like this:</p>

<pre><code>location       date  value
       1 2010-01-01    6.4
       1 2010-01-02    5.7
       .
       .  
       2 2010-01-01    0.8
       2 2010-01-02    2.5
       2 2010-01-03    5.5
</code></pre>

<p>I would like to aggregate data (value) on location and on 3 weeks period? I have already try to use package timeSeries but it is not working?</p>

<pre><code>by1 &lt;- timeSequence(from = ""2009-12-30"", to = ""2010-12-29"", by= ""4 week"")
by1
aggregate(date, by=list(by1, date$location), sum)
</code></pre>
"
442,Calc period of function in R,"<p>There is any R packages for calculation period of some function? I am looking for r-function like this:</p>

<pre><code>x &lt;- seq(0, 50, by = 0.05)
y &lt;- sin(x)
p &lt;- calcPeriod(x, y) # result = 2pi
</code></pre>
"
443,grid: How to specify limits to avoid grid.points() to plot beyond viewport?,"<p>I just tried my first steps with <code>grid</code>. I would like to set up a 2 x 2 matrix of square scatter plots, with some space in between. To get the space, I actually use a 3 x 3 layout (Question 1: is there an easier way?). As you can see from the example below, the points are plotted outside the bounding rectangle. I somehow have to specify the limits in the plot. Question 2: How can this be done? Finally, can I use base graphics to create the plots? [I know <code>lattice</code> graphics will work and also a standard <code>layout</code> could be used (or <code>ggplot2</code>), but I am interested if that's possible with <code>grid.layout</code>, too]</p>

<pre><code>require(grid)

## generate data to be plotted in the top left plot
X &lt;- matrix(rexp(2000), ncol=2)

## plot device
file &lt;- ""foo.pdf""
pdf(file=file, width=10, height=10)

## set up grid.layout
gl &lt;- grid.layout(3, 3, respect=rbind(c(0,1,0), c(1,1,1), c(0,1,0)),
                  widths=unit(c(3,1,3), ""inches""), heights=unit(c(3,1,3), ""inches"")) # define grid layout

pushViewport(viewport(layout=gl)) # use this layout in a viewport

## (1,1) plot
pushViewport(viewport(layout.pos.row=1, layout.pos.col=1, name=""11""))
grid.points(X[,1], X[,2], pch=1) # points
grid.rect() # bounding rectangle
grid.xaxis() # x-axis
grid.yaxis() # y-axis
grid.text(expression(italic(X[1])), y=unit(-3, ""lines"")) # x-axis label
grid.text(expression(italic(X[2])), x=unit(-3, ""lines""), rot=90) # y-axis label
grid.text(""Plot 1"", x=0.86, y=0.9, gp=gpar(fontface=""bold"", cex=1.6)) # add label
upViewport()

## (1,2) plot
pushViewport(viewport(layout.pos.row=1, layout.pos.col=3, name=""13""))
grid.rect()
upViewport()

## (2,1) plot
pushViewport(viewport(layout.pos.row=3, layout.pos.col=1, name=""31""))
grid.rect()
upViewport()

## (2,2) plot
pushViewport(viewport(layout.pos.row=3, layout.pos.col=3, name=""33""))
grid.rect()
upViewport()

## plot device
dev.off()
</code></pre>
"
444,Numpy out of memory,"<p>I have two variables, x and y.  x is </p>

<p><code>type(x) = &lt;class 'numpy.matrixlib.defmatrix.matrix'&gt;</code></p>

<p><code>type(y) = &lt;type 'numpy.ndarray'&gt;</code></p>

<p><code>x.shape = (869250, 1)</code></p>

<p><code>y.shape = (869250,)</code></p>

<p>x+y gives a MemoryError, despite the fact that I have around 5 gb free.  This seems rather odd - does anyone have an idea as to what might be going on?</p>

<p>This is numpy 1.5.1, python 2.7, on 64 bit Linux.</p>
"
445,Algorithm for modeling expanding gases on a 2D grid,"<p>I have a simple program, at it's heart is a two dimensional array of floats, supposedly representing gas concentrations, I have been trying to come up with a simple algorithm that will model the gas expanding outwards, like a cloud, eventually ending up with the same concentration of the gas everywhere across the grid.</p>

<p>For example a given state progression could be: 
(using ints for simplicity)</p>

<p>starting state</p>

<p>00000<br />
00000<br />
00900<br />
00000<br />
00000  </p>

<p>state after 1 pass of algorithm</p>

<p>00000<br />
01110<br />
01110<br />
01110<br />
00000  </p>

<p>one more pas should give a 5x5 grid all containing the value 0.36 (9/25).<br />
I've tried it out on paper but no matter how I try, I cant get my head around the algorithm to do this.</p>

<p>So my question is, how should I set about trying to code this algorithm? I've tried a few things, applying a convolution, trying to take each grid cell in turn and distributing it to its neighbours, but they all end up having undesirable effects, such as ending up eventually with less gas than I originally started with, or all of gas movement being in one direction instead of expanding outwards from the centre. I really can't get my head around it at all and would appreciate any help at all. </p>
"
446,Unbiased estimator questions,"<ol>
<li>If $X_1,X_2,\ldots,X_n$ are i.i.d. $\mathrm{B}(1,p)$, find the best unbiased estimator of $p^n$.</li>
</ol>

<p>Attempt: Use indicator functions to show every observation has mean equal to 1 so this is the same as the summation of the xi's = n. So this is a sufficient statistic and best unbiased estimator.</p>

<ol>
<li>Let $X_1,X_2,\ldots,X_n$ be i.i.d. $\mathcal{N}(\mu,1)$. If $\bar{x}$ attains the lower bound as an unbiased estimator of $\mu$, find the Fisher Information in $(X_1,X_2,\ldots,X_n)$.</li>
</ol>

<p>Fisher information = 1/CR-lower bound. </p>

<ol>
<li>If $X_1,X_2,\ldots,X_n$ are i.i.d. Uniform on $(\theta-\frac{1}{4},\theta + \frac{1}{4})$, find the sufficient statistic for theta and find an unbiased estimator of $\theta$ based on $\bar{x}$ and determine if you can improve it.</li>
</ol>

<p>So x_bar is a sufficent statistic and E[Unbiased Estimator|Sufficent Statistic] is the best unbiased estimator.</p>
"
447,dynamically changing list name in loop using assign?,"<p>Best give an example: this code should, the way I see it, produce 10 lists with 100 elements each. </p>

<pre><code>for (i in 1:10){
assign(paste(""List"", i, sep="".""), vector(""list"", length=100))
  for (j in 1:100){
    assign(paste(""List"", i, sep=""."")[[j]], j+1)
  }
}
</code></pre>

<p>But it doesn't.. The first assign works fine, creates an empty list with 100 elements, but how can I fill element [[j]] in it? Thanks!</p>
"
448,Numpy 1d Array CumSquare of the values,"<p>I am looking to emulate the functionality of <code>numpy.cumsum()</code>, except I need to capture the cumulative squares of the values.</p>

<p>For example: I have an array that is [1,2,3,4].</p>

<p>I can use <code>numpy.cumsum(array)</code> to return an <code>array([1,3,6,10])</code>.  My goal is to use some fast numpy trick to get the cumulative squares of the values.</p>

<p>In pure Python using a list:</p>

<pre><code>&gt;&gt;&gt; y = [1,2,3,4]
&gt;&gt;&gt; sqVal = 0
&gt;&gt;&gt; for val in y:
...     sqVal += val*val
...     print sqVal
... 
1
5
14
30
</code></pre>

<p>I tried <code>numpy.cumprod()</code>, but that is cumulative product, not the sum of the cumulative squares of the values.  My desire to use NumPy is purely based on speed.  Using <code>cumsum()</code> is substantially faster than using for loops (which makes sense).</p>
"
449,Return variable length array in Numpy C-extension?,"<p>I have made some Numpy C-extensions before with great help from this <a href=""http://www.scipy.org/Cookbook/C_Extensions/NumPy_arrays"" rel=""nofollow"">site</a>, but as far as I can see the returned parameters are all fixed length.</p>

<p>Is there any way to have a Numpy C-extension return a variable length numpy array instead?</p>
"
450,Converting two columns of date and time data to one,"<p>I have a simple data frame:</p>

<pre><code>D &lt;- c(""2012/12/14"", ""2012/12/14"")
Time &lt;- c(""18:40:37"", ""18:40:48"")
df1 &lt;- data.frame(D, Time)
</code></pre>

<p>I wish to combine the two columns of date and time information into one, preferable in the format of day, month, year, time.</p>

<p>How would I go about doing this?</p>
"
451,How to write a data-frame with one column a list to a file?,"<p>Here is my dummy dataset:</p>

<pre><code>dataset&lt;-data.frame(a=c(1,2,3,4),b=c('a','b','c','d'), c=c(""HI"",""DD"",""gg"",""ff""))
g=list(c(""a"",""b""),c(2,3,4), c(44,33,11,22),c(""chr"",""ID"",""i"",""II""))
dataset$l&lt;-g
dataset

 a b  c              l
1 1 a HI           a, b
2 2 b DD        2, 3, 4
3 3 c gg 44, 33, 11, 22
4 4 d ff chr, ID, i, II

&gt; mode(dataset$l)
[1] ""list""
</code></pre>

<p>when I try to write the dataset to a file:</p>

<pre><code>&gt; write.table(dataset, ""dataset.txt"", quote=F, sep=""\t"")
Error in write.table(x, file, nrow(x), p, rnames, sep, eol, na, dec, as.integer(quote),  : 
  unimplemented type 'list' in 'EncodeElement'
</code></pre>

<p>How can i solve this problem? </p>
"
452,Correlation between selection of columns in df using a for-loop,"<p>I have a dataframe (<code>df</code>) with 8 columns.  I'd like to use a <code>for</code> loop to calculate Pearson correlation for a selection of columns the following way: </p>

<pre><code>cor1=cor(df[,1], df[,2])
cor2=cor(df[,3], df[,4])
</code></pre>

<p>and so on. What is the best way to do this? </p>
"
453,Image to numpy-array: JPG vs. PNG,"<p>My system: Mac OS X 10.7<br>
Python 2.7.1<br>
NumPy version 1.5.1</p>

<p>My code: </p>

<pre><code>image = openImage(fileDir)  
print image  
image = np.asarray(image)  
print image
</code></pre>

<p>If I run this:</p>

<pre><code>python main.py 1010.png
</code></pre>

<p>the output is:</p>

<pre><code>&lt;PIL.PngImagePlugin.PngImageFile image mode=RGB size=10x10 at 0x10A835368&gt;  
[[[226 226 226] ...    `
</code></pre>

<p>If I run this:</p>

<pre><code>python main.py google.jpg 
</code></pre>

<p>the output is:</p>

<pre><code>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=488x640 at 0x10140B368&gt;  
&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=488x640 at 0x10140B368&gt;
</code></pre>

<p>I cannot figure out why <code>numpy</code> can converts png's but can't do it on jpg's, why? How can I solve this?</p>

<p><strong>edit:</strong></p>

<p>ok tried with</p>

<pre><code>image.show()
</code></pre>

<p>and got following error-message:</p>

<pre><code>IOError: decoder jpeg not available
</code></pre>

<p>i solved the problem with the information i got from this page:<br>
<a href=""http://mariz.org/blog/2007/01/26/mac-os-x-decoder-jpeg-not-available/"" rel=""nofollow"">http://mariz.org/blog/2007/01/26/mac-os-x-decoder-jpeg-not-available/</a></p>
"
454,Solve Physics exercise by brute force approach,"<p>Being unable to reproduce a given result. (either because it's wrong or because I was doing something wrong) I was asking myself if it would be easy to just write a small program which takes all the constants and given number and permutes it with a possible operators (* / - + exp(..)) etc) until the result is found.</p>

<p>Permutations of n distinct objects with repetition allowed is n^r. At least as long as r is small I think you should be able to do this. I wonder if anybody did something similar here..</p>
"
455,Print a data.frame in the white space of a plot,"<p>I'd like to have the output of <code>print(df.to.print)</code> to be printed on a plot. It would be great if I could place the location with <code>topleft</code> or <code>topright</code> like in a call to `legend(...), but that's just a bonus. </p>

<p>Some code: </p>

<pre><code># A data.frame to play with
df.to.print &lt;- structure(list(p.val = c(0.05, 0.1), self = c(0.0498, 0.0997), 
    H2007.REML = c(0, 0.01), H2007.ref = c(0, 0)), .Names = c(""p.val"", 
""self"", ""H2007.REML"", ""H2007.ref""), row.names = c(NA, -2L), class = ""data.frame"")

# A simple plot
plot(1)
text(1,1, df.to.print )
# All of the entries are overlapping

# I can insert newlines easily enough
plot(1)
text(1,1, paste(as.character(df.to.print), collapse='\n'))

# But that looses all of the nice formatting in the data.frame.
# Which is easy enough to get on the console with: 
print(df.to.print)

# Bonus points for 'topleft', 'topright', etc. like in legend().
</code></pre>

<p>Any help would be appreciated. </p>
"
456,List all possible combinations of k integers between 1...n (n choose k),"<p>Out of no particular reason I decided to look for an algorithm that produces all possible choices of k integers between 1...n, where the order amongst the k integer doesn't matter (the n choose k thingy). </p>

<p>From the exact same reason, which is no reason at all, I also implemented it in C#. My question is:</p>

<p><em>Do you see any mistake in my algorithm or code? And, more importantly, <strong>can you suggest a better algorithm?</em></strong></p>

<p>Please pay more attention to the algorithm than the code itself. It's not the prettiest code I've ever written, although do tell if you see an error.</p>

<p><strong>EDIT:</strong> Alogirthm explained - <br/></p>

<ul>
<li>We hold k indices.</li>
<li>This creates k nested <em>for</em> loops, where loop i's index is indices[i]. </li>
<li>It simulates k <em>for</em> loops where indices[i+1] belongs to a loop nested within the loop of indices[i].</li>
<li>indices[i] runs from indices[i - 1] + 1 to n - k + i + 1.</li>
</ul>

<p>CODE:</p>

<pre><code>public class AllPossibleCombination
{
    int n, k;
    int[] indices;
    List&lt;int[]&gt; combinations = null;

    public AllPossibleCombination(int n_, int k_)
    {
        if (n_ &lt;= 0)
        {
            throw new ArgumentException(""n_ must be in N+"");
        }
        if (k_ &lt;= 0)
        {
            throw new ArgumentException(""k_ must be in N+"");
        }
        if (k_ &gt; n_)
        {
            throw new ArgumentException(""k_ can be at most n_"");
        }

        n = n_;
        k = k_;
        indices = new int[k];
        indices[0] = 1;
    }

    /// &lt;summary&gt;
    /// Returns all possible k combination of 0..n-1
    /// &lt;/summary&gt;
    /// &lt;returns&gt;&lt;/returns&gt;
    public List&lt;int[]&gt; GetCombinations()
    {
        if (combinations == null)
        {
            combinations = new List&lt;int[]&gt;();
            Iterate(0);
        }
        return combinations;
    }

    private void Iterate(int ii)
    {
        //
        // Initialize
        //
        if (ii &gt; 0)
        {
            indices[ii] = indices[ii - 1] + 1;
        }

        for (; indices[ii] &lt;= (n - k + ii + 1); indices[ii]++)
        {
            if (ii &lt; k - 1)
            {
                Iterate(ii + 1);
            }
            else
            {
                int[] combination = new int[k];
                indices.CopyTo(combination, 0);
                combinations.Add(combination);
            }
        }
    }
}
</code></pre>

<p>I apologize for the long question, it might be fit for a blog post, but I do want the community's opinion here.</p>

<p>Thanks, <br/>
Asaf</p>
"
457,"C++ Error, adding even numbers","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stackoverflow.com/questions/2574611/sum-of-even-fibonacci-numbers"">Sum of even fibonacci numbers</a>  </p>
</blockquote>



<p>At the moment, I am doing some Project Euler problems and I am up to the second problem.</p>

<blockquote>
  <p>By considering the terms in the Fibonacci sequence whose values do not exceed four million, find the sum of the even-valued terms.</p>
</blockquote>

<p>I am doing the problem in C++, and I think my code is perfectly fine, because up to a point, everything is correct, but then, everything spirals out of control...
Code: </p>

<pre><code>#include &lt;iostream&gt;
using namespace::std;
int main() {
    int a = 1;
    int b = 1;
    int evsum = 0;
    while (a &lt; 4000000 and b &lt; 4000000) {
        a = a + b;
        b = a + b;
        if (a % 2 == 0)
            evsum += evsum + a;
        if (b % 2 == 0)
            evsum = evsum + b;
        cout &lt;&lt; a &lt;&lt; endl;
        cout &lt;&lt; b &lt;&lt; endl;
        cout &lt;&lt; evsum &lt;&lt; ""evensum"" &lt;&lt; endl;
    }
}
</code></pre>

<p>The xCode compiler prints everything out correctly until...</p>

<pre><code>2
3
2evensum

5
8
10evensum

13
21
10evensum

...

...

233
377
198evensum

610
987
1006evensum
</code></pre>

<p>For some reason, the compiler adds 610 to 198 and thinks it is equal to 1006!!! How do I fix this???</p>
"
458,numpy: is there any driver to load data from mongodb?,"<ul>
<li>I have a large collection in mongo  </li>
<li>I want to load the data in numpy ndarray  </li>
<li>is there a way to load data from mongodb without iterating through pymongo. something like <a href=""https://github.com/tc/RMongo"" rel=""nofollow"">R-Mongo</a></li>
</ul>
"
459,simple Feed forward (newff) network in MATLAB,"<p>I used <code>ffnew</code> functions many times but when I am trying to create a simple feed forward network such that the input vector is <code>P=[1;2;3;4]</code> and the desired output is <code>T=[1 ;0;0;1]</code>. So i only have one sample input vector</p>

<p>The code is</p>

<pre><code>net = newff(P,T,[4 1],{'tansig','tansig'});
net=train (net,P,T);
</code></pre>

<p>When I write the last line I got:</p>

<pre><code>??? Error using ==&gt; plus
Matrix dimensions must agree.

Error in ==&gt; calcperf2 at 163
        N{i,ts} = N{i,ts} + Z{k};

Error in ==&gt; trainlm at 253
[perf,El,trainV.Y,Ac,N,Zb,Zi,Zl] = calcperf2(net,X,trainV.Pd,trainV.Tl,trainV.Ai,Q,TS);

Error in ==&gt; network.train at 216
  [net,tr] = feval(net.trainFcn,net,tr,trainV,valV,testV);
</code></pre>
"
460,Has the Urban Heat Island effect on the US temperature record been underestimated?,"<p>One of the early climate-skeptic arguments against the consensus view that the world is warming to a dangerous extent, was that the instrumental record of temperature was corrupted by the Urban Heat Island (UHI) effect. Wikpedia has a <a href=""http://en.wikipedia.org/wiki/Urban_heat_island"">good summary</a> and even <a href=""http://www.realclimate.org/index.php/archives/2004/11/urban-heat-island-uhie/"">RealClimate</a> refers to their summary.</p>

<p>The argument was that a failure to correct for the (well-known) effect where urbanisation increases local temperatures led to exaggeration of the temperature trend measured by local weather stations and this, in turn, led to an overestimation of the average degree of warming across the world. </p>

<p>The climate-skeptic argument has been widely criticised. <a href=""http://www.skepticalscience.com/urban-heat-island-effect.htm"">SkepticalScience</a> dismisses it like this (and the link has further detail):</p>

<blockquote>
  <p>Scientists have been very careful to ensure that UHI is not influencing the temperature trends. To address this concern, they have compared the data from remote stations (sites that are nowhere near human activity) to more urban sites. Likewise, investigators have also looked at sites across rural and urban China, which has experienced rapid growth in urbanisation over the past 30 years and is therefore very likely to show UHI. The difference between ideal rural sites compared to urban sites in temperature trends has been very small...</p>
</blockquote>

<p>But a recent paper by the climate-skeptic Anthony Watts and others (<a href=""http://wattsupwiththat.com/2012/07/29/press-release-2/"">press release here</a>, <a href=""http://wattsupwiththat.files.wordpress.com/2012/07/watts-et-al_2012_discussion_paper_webrelease.pdf"">full draft paper pdf here</a>) argues that, when properly classified by site-quality, there is a significant difference in the degree of warming reported between good and bad sites which leads to a significant exaggeration of the reported US temperature trend.</p>

<p>The results are summarised well by this picture showing the apparent differences between the reported warming of good versus bad sites:
<img src=""http://i.stack.imgur.com/nSFds.png"" alt=""figure from watts et. al. paper""></p>

<p>The paper argues:</p>

<blockquote>
  <p>A reanalysis of U.S. surface station temperatures has been performed using the recently WMO-approved Siting Classification System devised by METEO-France’s Michel Leroy. The new siting classification more accurately characterizes the quality of the location in terms of monitoring long-term spatially representative surface temperature trends. The new analysis demonstrates that reported 1979-2008 U.S. temperature trends are spuriously doubled, with 92% of that over-estimation resulting from erroneous NOAA adjustments of well-sited stations upward.</p>
</blockquote>

<p>So the question is: does the new paper from Watts et. al. convincingly suggest that, when sites recording actual temperatures are properly classified, the warming trend is lower in sites where the UHI is less significant? Is Watts' observation-based argument credible? Does it undermine previous rebuttals of the influence of UHI on the temperature record?</p>

<p>NB. Watts doesn't claim there is no warming, just that the extent of warming is lower when only good-quality temperature-recording sites are used. So the question is about the quality of the instrumental record not about whether warming is happening. </p>
"
461,How to Find A Path of Length X Between Two Nodes in A Weighted (Undirected) Graph,"<p>Given a graph, how can I find a path of length X between two nodes in the graph. The path should ideally visit an edge no more than once.</p>
"
462,Ordering date/time in descending order in R,"<p>I have a data frame DF with one of the columns being <strong>date/Time</strong> and I would like to order the data frame in descending order of this column. </p>

<pre><code>DF &lt;- data.frame(ID=c('ID3', 'ID2','ID1'), end=c('4/1/10 12:00', '6/1/11 14:20', '1/1/09 11:10'), age=c(40,30,20));
</code></pre>

<p>I first converted the <code>end</code> column into <code>et</code> using <code>et = as.POSIXct(DF$end,format='%m/%d/%Y %H:%M')</code>, and used the following, but got the error that unary operator '-' is not accepted for the argument :</p>

<pre><code>out &lt;- DF[order(-DF$et),];
</code></pre>

<p>I also tried used the descending flag but again got an error about arguments not being of same length.</p>

<pre><code>out &lt;- DF[order(DF$et, descending=TRUE),];
</code></pre>

<p>However, the ascending order seems to work: <code>out &lt;- DF[order(DF$et),]</code>.</p>

<p>How can I order in descending order (most recent time first)? Thank you.</p>
"
463,Cannot read text file using csv?,"<p>I have a text data separated ny ""commas"" i.e."","". The sample of the data is given below (first row indicates the column names):</p>

<pre><code>userID,appName,startTime,endTime,endResult
chhieut,gms.mos.test,2012-07-01 02:47:16,2012-07-01 02:47:46,1
chhieut,gms.mos.test,2012-07-01 03:11:46,2012-07-01 03:12:25,2
chhieut,gms.mos.test,2012-07-01 03:13:36,2012-07-01 03:14:03,2
chhieut,gms.mos.test,2012-07-01 03:18:26,2012-07-01 03:18:58,2
chhieut,gms.mos.test,2012-07-01 04:10:36,2012-07-01 04:10:54,2
chhieut,gms.mos.test,2012-07-01 04:38:26,2012-07-01 04:38:48,2
chhieut,gms.mos.test,2012-07-01 04:48:56,2012-07-01 04:49:04,3
chhieut,gms.mos.test,2012-07-01 05:49:46,2012-07-01 05:50:14,2
chhieut,gms.mos.test,2012-07-01 06:19:07,2012-07-01 06:19:25,2
chhieut,gms.mos.test,2012-07-01 07:09:17,2012-07-01 07:09:47,2
</code></pre>

<p>I am using the following syntax:</p>

<pre><code>appsession &lt;- read.table(""C:/.../AppSession.txt"", sep = "","", 
  col.names = c(""userID"",""appName"",""startTime"",""endTime"",""endResult""), 
  fill = FALSE, strip.white = TRUE)
</code></pre>

<p>I am getting this error:</p>

<pre><code>Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,  : 
  line 1 did not have 5 elements
</code></pre>
"
464,Animate quadratic grid changes (matshow),"<p>I have a NxN grid with some values, which change every time step. I have found a way to plot a single grid configuration of this with <code>matshow</code> function, but I don't know how do I update the status with every time step. Here is a simple example:<br></p>

<pre><code>from pylab import *
from matplotlib import pyplot

a = arange(25)
a = a.reshape(5,5)
b = 10*rand(5,5)
matshow(a-b, cmap = cm.jet)
colorbar()
show()
</code></pre>

<p>This code produces the following picture:
<img src=""http://i.stack.imgur.com/Ea20X.png"" alt=""enter image description here""><br>
Now imagine that the next time step some values change, so should this picture. This is the logic I had in mind:</p>

<pre><code>from pylab import *
from matplotlib import pyplot

a = arange(25)
a = a.reshape(5,5)
time=10
for t in range(time):
    b = 10*rand(5,5)
    print b
    matshow(a-b, cmap=cm.jet)
    colorbar()
show()
</code></pre>

<p>This produces 10 pictures. I'd like to animate this instead producing individual pictures, and for example I'd like to choose a time step between changes (that is, frame rate).<br>
Also, I'm open to suggestions for a different functions, if <code>matshow</code> is not the way to go, but please keep it simple, I'm relatively inexperienced.</p>
"
465,Is there a function which evaluates to the same value for all elements of an interval?,"<p>Is there a function which evaluates to the same value for all elements of an interval?</p>

<p>For example I have an interval [15, 20], would it be possible to have the same result after passing through a function for all values ​​of the interval?</p>

<p>Merci</p>
"
466,"How to pass ""..."" arguments to a function called from foreach()?","<p>The following code produces the warning:</p>

<pre><code>Warning message:
&lt;anonymous&gt; : &lt;anonymous&gt;: ... may be used in an incorrect context: ‘mean(x[l], ...)’

doForeach &lt;- function(x, ...)
{
    require(doSNOW)
    require(foreach)
    cl &lt;- snow::makeCluster(2, type=""MPI"")
    on.exit(snow::stopCluster(cl))
    registerDoSNOW(cl)
    do.i &lt;- function(i) lapply(seq_len(length(x)), function(l) mean(x[l], ...))
    foreach(i=seq_len(10)) %dopar% { do.i(i) }
}
x &lt;- rnorm(20)
r &lt;- doForeach(x, trim=1)
</code></pre>

<p>I'd guess that it comes from the fact that the workers/slaves do not see the <code>...</code> anymore. Formal arguments are typically passed as character vectors via <code>.export=c(""&lt;arg&gt;"")</code>, but that does not seem to work for <code>...</code> arguments.</p>

<p>What's the correct way of dealing with <code>...</code> arguments in this example?</p>
"
467,recoding variables,"<p>I want to recoded a variable so that, for example, we can get transforme this vector in the following way:</p>

<pre><code>&gt; a &lt;- c(0,0,0,0,0,1,1,1,1,1) # original 
&gt; b &lt;- c(-5,-4,-3,-2,-1,0,1,2,3,4) # transformed
&gt; cbind(a,b)
  a  b
 [1,] 0 -5
 [2,] 0 -4
 [3,] 0 -3
 [4,] 0 -2
 [5,] 0 -1
 [6,] 1  0
 [7,] 1  1
 [8,] 1  2
 [9,] 1  3
[10,] 1  4
&gt;
</code></pre>

<p>These variables follow an order, which happen to be a time order. In the original data set, I have a variable which is coded ""0"" or ""1"", such as ""a"" in the example here. It is a categorical indicator for each year. At some point, there is a transition from ""0"" to ""1"", like in the row number 6 in these example. Then I would like to recoded the original variable, creating a new variable that actually tells me how many years before and after the change from ""0"" to ""1"". Thus ""-5"" means five year before the transition, ""0"" means the year of the transition and, say, ""4"" means four years after the transitions. Any suggestions the best way to do it? Thanks! Antonio. </p>
"
468,"Python: Memory-efficient matrix creation for sets of 1's, -1's, and 0s to be optimized by scipy least squares","<p>I'm iterating through a list of strings and translating them into arrays of 1's, -1's, and 0's. For example - I may have the following list:</p>

<pre><code>A,B,-C
A,-D
B,C,-D
</code></pre>

<p>Which will become a ""biglist"" equal to:</p>

<pre><code>[
 [1  1 -1  0],
 [1  0  0 -1],
 [0  1  1 -1]
]
</code></pre>

<p>At the moment, I'm simply looping through every line of strings, assigning values of 1 or -1 to the string if it is unique, and zeroing out the ones that do not exist (for example, D is not present in the first line, so it is 0). The silly way I'm doing the above is basically:</p>

<pre><code>for line_of_strings in all_strings:
    for the_string in line_of_strings:
        entry[string_index] = (1 or -1)

    biglist.append(entry)
</code></pre>

<p>Eventually, I have a nice set of lists on which I run:</p>

<pre><code>scipy.optimize.nnls(biglist)
</code></pre>

<p>This works, but winds up taking up a truckload of memory and time. Is there a more efficient way to go about this? Using numpy or scipy arrays/matrices, perhaps?</p>
"
469,Plotting series with some no data in xyplot in R,"<p>I tried this question on the R-Help list with no success <a href=""https://stat.ethz.ch/pipermail/r-help/2011-November/295792.html"" rel=""nofollow"">CrossPostHere</a>...</p>

<p>I am working on some xyplots using the Lattice Library.  My X-axis is 
the date and I am reproducing charts similar to those found in the R 
Gallery (see here:<a href=""http://www.sr.bham.ac.uk/~ajrs/R/gallery/plot_midday_weather_profiles.txt"" rel=""nofollow"">R-Gallery Trellis</a>)</p>

<p>However, the key difference is that some of my data is missing (not 
collected at that time).  For instance, I might have a whole month that 
I do not have data.  The problem is that xyplot connects the data points.</p>

<p>To continue with the R Gallery plot example, if your Outside Temperature 
data stopped at 20 degrees in March 2007 and picked back up in July 2007 
at 20 degrees you would have a straight line connecting the two data 
points (at 20 degrees).  I would rather have no line which would (in my 
opinion) better  represent that no data was collected for that time period.</p>

<p>I am curious to know if anyone has a idea of how to alter this behavior 
so that the lines are not connected when data are missing in the 
series.  I am hoping there is an easy or slick solution but as I started 
to think about it, it might be rather complex because you'd need to tell 
xyplot what granularity you'd like to not connect (e.g. 5 hours in a 
row, 4 days in a row, etc).</p>

<p>Any help would be much appreciated !</p>
"
470,How does python numpy.where() works?,"<p>I am playing with <code>numpy</code> and digging thruogh documentation and I have come across some magic. Namely I am talking about <code>numpy.where()</code>:</p>

<pre><code>&gt;&gt;&gt; x = np.arange(9.).reshape(3, 3)
&gt;&gt;&gt; np.where( x &gt; 5 )
(array([2, 2, 2]), array([0, 1, 2]))
</code></pre>

<p>How do they achieve internally that you are able to pass something like <code>x &gt; 5</code> into a method? I guess it has something to do with <code>__gt__</code> but I am looking for detailed explanation.</p>
"
471,shading within xy curve plot in R,"<p>I have data similar to following: </p>

<pre><code>X &lt;- 1:20
B &lt;- c(1,4,6,3,1, 4, 5,8,8,6,3,2,1, 1,5,7,8,6,4,2)
C &lt;- B + 4
myd &lt;- data.frame (X, B, C)
</code></pre>

<p>I want to shade with different color within the curve. Please note the boundries color filling in x </p>

<pre><code>region 1 = 1 to 6
 region 2 =  6 to 16
 region 3 =  16 to 20 
</code></pre>

<p><img src=""http://i.stack.imgur.com/DmqD8.jpg"" alt=""enter image description here""></p>
"
472,How do I round a set of numbers while ensuring the total adds to 1,"<p>I'm looking for a way to round a set of numbers to the nearest rational number while still preserving the total of the set.</p>

<p>I want to distribute the total value of '1' amongst a variable number of fields without allowing irrational numbers.</p>

<p>So say I want to distribute the total across three fields. I don't want each value to be 0.33333333333333333333'. I would prefer 0.33, 0.33 &amp; 0.34.</p>

<p>I would like to implement this using Jquery/javascript. I have a form where fields are added dynamically. By default the total is evenly distributed amongst each field, however my problem is further complicated because often the value will not be shared equally amongst all fields. Some values might be more heavily weighted. So if I were to change one of the three values to 0.5, the other two values would be adjusted to 0.25.</p>

<p>Can anyone help with a suitable algorithm?</p>
"
473,The right steps to enter a value in the arguments x and y in Adjusted Rand Index?,"<p>Im attempting to use the <strong>Adjusted Rand Index</strong> to compare clustering results. Here, I use Iris data set as an example. These are the code:</p>

<pre><code>iris.data=subset(iris, select=-Species)
iris.eucdist &lt;- dist(iris.data, method=""euclidean"")
iris.sqeucdist &lt;- iris.eucdist^2
iris.hc &lt;- hclust(iris.sqeucdist, ""ward"")
plot(iris.hc, main=""Dendrogram of Ward's Method"", label=iris$Species)
table(cutree(iris.hc, 3), iris$Species)

##        setosa versicolor virginica
##   1     50          0         0
##   2      0         49        15
##   3      0          1        35
</code></pre>

<p>Firstly, I compute the ARI(Hubert and Arabie, 1985) manually, by using value in the table above. The answer is 0.7311986. However, when I using R, I cannot get the same answer.</p>

<pre><code>library(mclust)
U=c(50,0,0,50,0,49,1,50,0,15,35,50)
V=c(50,0,0,50,0,49,15,64,0,1,35,36)
adjustedRandIndex(U,V)
## [1] 0.6961326
</code></pre>

<p>Perhaps, the way I put in the value is wrong. Is there a way to implement this so that the answer from R is same with manual computation?</p>
"
474,How to measure the distribution of single data point from a set?,"<p>I have 100 data points. I calculate their standard deviation. Then, I get a new data point, number 101. How can I measure the deviation of the new data point compared to the deviation of the other 100 data points?</p>

<p>In order to answer a question like this: The new data point deviates by X more / less than the other 100 data points combined.</p>
"
475,blind convex hull code in java,"<p>Hi
I know that blind <a href=""http://en.wikipedia.org/wiki/Convex_hull"" rel=""nofollow"">convex hull</a> has o(n^4)
but I have never seen its code! Is there any site that has its code?
thanks</p>
"
476,Mixture of two mirror-image Gaussians,"<p>Suppose we are given a set of points $(x_i, y_i)\in\mathbb{R}^2$ and are told that they are drawn from a normal (Gaussian) distribution.  It is a simple matter in that case to find the mean $(\mu_x,\mu_y)$ of the distribution: $(\langle x \rangle, \langle y \rangle)$, where $\langle x \rangle=\frac{1}{N}\sum_i x_i$ and similarly for $\langle y \rangle$, is an unbiased estimator.  </p>

<p>Now suppose, instead, that each point is randomly presented as either $(x_i,y_i)$ or $(y_i, x_i)$, with equal probability.  Is there still a comparably simple way to estimate the mean of the original distribution?  (Assume $\mu_x \le \mu_y$.)  This is equivalent to the case where the points are drawn from a mixture of two Gaussians, where one is constrained to be the reflection of the other across the line $y=x$.  However, one might hope that the symmetry of the problem leads to some simplification from the general case of two Gaussians.</p>

<p>Note, by the way, that taking $\mu_x=\langle \min(x, y)\rangle$ and $\mu_y = \langle \max (x,y) \rangle$ does not work: this is adequate only when the individual Gaussian distributions are well-separated by the line $y=x$.  Otherwise, it introduces a systematic bias away from that line.</p>
"
477,Sharing the x-axis,"<p>Consider the output of the following R code:</p>

<pre><code>par( mar=c(5,4,4,5)+.1 )
boxplot( t( matrix( rnorm( 100 ), nrow=10 )), xlab=""Var1"", ylab=""Var2"")
par( new=T )
plot( 1:10, runif(10,min=-2, max=3), xaxt='n', yaxt='n', xlab='', ylab='', type='b', col='blue', pch=15 )
axis(4)
mtext(""Var3"", side=4, line=3 )
</code></pre>

<p>The blue points are not aligned with the x-axis. I would like to get both plots to share the x-axis correctly. I've tried using the <code>at</code> argument to <code>boxplot</code> but that gives me weird output.</p>

<p>Thanks in advance!</p>

<p>PK
^_^</p>
"
478,Cannot view gvisMotionChart from printed html file,"<p>When I save the code for a Motion Chart into an html file and try to open it, I get the footer and the ""initializing"" progress bar blinks for a split second, but nothing else shows up.</p>

<p>so </p>

<pre><code>M1 &lt;- gvisMotionChart(Fruits, idvar=""Fruit"", timevar=""Year"")
plot(M1)
</code></pre>

<p>works great, but</p>

<pre><code>print(M1, file='d:/delete/fruit.html')
</code></pre>

<p>and then opening the resulting file in a browser comes up blank (except for the footer). Is there a way I can save this visualization and circulate it around?</p>
"
479,Complete statistic: Poisson Distribution,"<p><strong>Context</strong></p>

<p>I am having difficulty trying to understand a step of a proof which relies on a property of series. </p>

<p><strong>Proof</strong></p>

<p>Suppose that $X_1, X_2, \ldots , X_n$ is a random sample of size $n$ from a Poisson distribution with parameter $\lambda &gt; 0$. The goal is to show that $T = \sum_{i=1}^n X_i$ is a complete statistic.</p>

<p>Since we know that $T =  \sum_{i=1}^n X_i \sim \mathrm{Poisson}(n\lambda)$:</p>

<p>$$
\mathbb{E}(h(T)) = \sum_{k=0}^{\infty} h(k) \, e^{-n\lambda} \, \frac{(n\lambda)^k}{k!} = 0\Longrightarrow \sum_{k=0}^{\infty} h(k)  \, \frac{(n\lambda)^k}{k!} = 0 
$$</p>

<p>The textbook I am using and some others sources I've found argue that: </p>

<p>$$
\boxed{\displaystyle\sum_{k=0}^{\infty} h(k)  \, \frac{(n\lambda)^k}{k!} = 0 \Longrightarrow h(k)  \, \frac{(n\lambda)^k}{k!}  = 0 \qquad \forall k}
$$</p>

<p>It probably is an obvious result from calculus, but I am unable to prove it. </p>

<p>If $ h(k) \, (n\lambda)^k/k!  = 0$ for all $k$ then $T$ is a complete statistic because $\lambda$ is nonnegative and then $h(k) = 0$ for all $k$ . </p>
"
480,Multi-output vs. multiple single-output neural networks for classification,"<p>Is there a theoretical argument in favor of or against using a single multi-output NN to do multi-class classification vs. using several one-vs-all NNs ?</p>

<p>In both cases once all output values are obtained, the same decision rule is used: the output with highest activation ""wins"" and decides which class the prediction returns.</p>

<p>But I wonder whether -and why- it's better or worse to have all outputs calculated on the same NN rather than separately.</p>
"
481,How can I neatly clean my R workspace while preserving certain objects?,"<p>Suppose I'm messing about with some data by binding vectors together, as I'm wont to do on a lazy sunday afternoon.</p>

<pre><code>    x &lt;- rnorm(25, mean = 65, sd = 10)
    y &lt;- rnorm(25, mean = 75, sd = 7)
    z &lt;- 1:25

    dd &lt;- data.frame(mscore = x, vscore = y, caseid = z)
</code></pre>

<p>I've now got my new dataframe <code>dd</code>, which is wonderful. But there's also still the detritus from my prior slicings and dicings:</p>

<pre><code>    &gt; ls()
    [1] ""dd""        ""x""          ""y""          ""z""         
</code></pre>

<p>What's a simple way to clean up my workspace if I no longer need my ""source"" columns, but I want to keep the dataframe? That is, now that I'm done manipulating data I'd like to just have <code>dd</code> and none of the smaller variables that might inadvertently mask further analysis:</p>

<pre><code>    &gt; ls()
    [1] ""dd""
</code></pre>

<p>I feel like the solution must be of the form <code>rm(ls[ -(dd) ])</code> or something, but I can't quite figure out how to say ""please clean up everything BUT the following objects.""</p>
"
482,How to Insert into Latex a Text File whose name is a variable?,"<p>I need to insert a text file into LaTeX. But text file names are different for different projects. So I want to define text file name as a variable and refer to the variable when inserting the text</p>

<p>I tried the code as below. But </p>

<pre><code>\VerbatimInput[baselinestretch=1,fontsize=\footnotesize]{{\myfile}}
</code></pre>

<p>doesn't work. Only</p>

<pre><code>\VerbatimInput[baselinestretch=1,fontsize=\footnotesize]{abc.txt} 
</code></pre>

<p>Could anyone give me some guide?</p>

<pre><code>\usepackage{fancyvrb}
&lt;&lt;text, echo=F&gt;&gt;=
targetvar&lt;-'abc'
textfile&lt;-paste(targetvar,'.txt',sep='') 
@
\newcommand{\myfile}{\Sexpr{textfile}}
\VerbatimInput[baselinestretch=1,fontsize=\footnotesize]{{\myfile}}
</code></pre>
"
483,Specifying Data Frame as Time Series,"<pre><code>data=read.csv(""filelocation"",header=T,colClasses=c(""Date"",""numeric"")

  date   weight
2010-10-04 52495    
2010-10-01 53000    
2010-09-30 52916    
2010-09-29 52785    
2010-09-28 53348    
2010-09-27 52885    
2010-09-24 52174    
2010-09-23 51461    
2010-09-22 51286    
2010-09-21 50968    
2010-09-20 49250

data=data[order(data$date),]
diffweight1=weight-lag(weight,1)    
</code></pre>

<p>Hi guys,</p>

<p>I am am loading in time-series data into R for analysis. I am trying to lag one of the variables in order to difference the series. Unfortunately, the values of the differences variables all equal 0, because R wasn't successful at lagging the weight variable. I know I am supposed to use the as.ts(data$date) to specify that that ""date"" variable is a time series but every time I do so it changes the ""date"" variable into numeric numbers.Not to mention I thought I specified that the ""date"" column in the dataset was a time/date variable when I initially loaded it. How can I specify the data.frame as a time series? I appreciate any help. Thank you!!</p>
"
484,How to create a sequence of intervals for allocating a value,"<p>I'm trying to find the most efficient way to split seconds between session start and end time into 15 minute intervals to I can show the seconds and multiple of the bitrate in each interval.</p>

<p>Here is some sample data:</p>

<p><code>df &lt;- structure(list(username = structure(c(1L, 2L, 3L, 4L, 5L, 6L, 
7L, 8L, 9L, 9L), .Label = c(""user1"", ""user2"", ""user3"", 
""user4"", ""user5"", ""user6"", ""user7"", ""user8"", 
""user9""), class = ""factor""), bitrate = structure(c(3500000, 7000000, 
3500000, 3500000, 3500000, 7000000, 3500000, 7000000, 3500000, 7000000), class = ""numeric""), 
    start = structure(c(1322700567, 1322700984, 1322700646, 1322700883, 
    1322700042, 1322700073, 1322700547, 1322700794, 1322700694, 
    1322700934), tzone = """", class = c(""POSIXct"", ""POSIXt"")), 
    end = structure(c(1322700766, 1322701250, 1322700945, 1322701270, 
    1322701284, 1322706303, 1322701781, 1322702307, 1322701600, 
    1322701224), tzone = """", class = c(""POSIXct"", ""POSIXt""))), .Names = c(""username"", 
""birate"", ""start"", ""end""), row.names = c(NA, 10L), class = ""data.frame"")</code></p>

<pre><code>      username  birate               start                 end
1     user1 3500000 2011-12-01 01:49:27 2011-12-01 01:52:46
2     user2 7000000 2011-12-01 01:56:24 2011-12-01 02:00:50
3     user3 3500000 2011-12-01 01:50:46 2011-12-01 01:55:45
4     user4 3500000 2011-12-01 01:54:43 2011-12-01 02:01:10
5     user5 3500000 2011-12-01 01:40:42 2011-12-01 02:01:24
6     user6 7000000 2011-12-01 01:41:13 2011-12-01 03:25:03
7     user7 3500000 2011-12-01 01:49:07 2011-12-01 02:09:41
8     user8 7000000 2011-12-01 01:53:14 2011-12-01 02:18:27
9     user9 3500000 2011-12-01 01:51:34 2011-12-01 02:06:40
10    user9 7000000 2011-12-01 01:55:34 2011-12-01 02:00:24
</code></pre>

<p>></p>

<p>Ideally I want to do this in R if possible, only needs to be for 1 calendar day, with either showing the seconds in a vector or allocating the bitrate vector as a multiple of the seconds, so for example with seconds:</p>

<pre><code>session 01:30   01:45   02:00   02:15   02:30  etc.
   1        0      199      0       0       0  etc.
   2        0      266      0       0       0  etc.
  10        0      306      24      0       0  etc.
</code></pre>

<p>I was thinking either a sequence by minute or perhaps using xts with align time might be the best approach.</p>
"
485,Multiplication algorithm for abritrary precision (bignum) integers,"<p>I'm writing a small bignum library for a homework project. I am to implement Karatsuba multiplication, but before that I would like to write a naive multiplication routine.</p>

<p>I'm following a guide written by Paul Zimmerman titled ""Modern Computer Arithmetic"" which is <a href=""http://www.loria.fr/~zimmerma/mca/mca-cup-0.5.1.pdf"" rel=""nofollow"">freely available online</a>.</p>

<p>On page 4, there is a description of an algorithm titled BasecaseMultiply which performs gradeschool multiplication.</p>

<p>I understand step 2, 3, where B^j is a digit shift of 1, j times.
But I don't understand step 1 and 3, where we have A*b_j. How is this multiplication meant to be carried out if the bignum multiplication hasn't been defined yet?</p>

<p>Would the operation ""*"" in this algorithm just be the repeated addition method?</p>

<p>Here is the parts I have written thus far. I have unit tested them so they appear to be correct for the most part:</p>

<p>The structure I use for my bignum is as follows:</p>

<pre><code>#define BIGNUM_DIGITS 2048
typedef uint32_t u_hw; // halfword
typedef uint64_t u_w; // word

typedef struct {
    unsigned int sign; // 0 or 1
    unsigned int n_digits;
    u_hw digits[BIGNUM_DIGITS];
} bn;
</code></pre>

<p>Currently available routines:</p>

<pre><code>bn *bn_add(bn *a, bn *b); // returns a+b as a newly allocated bn
void bn_lshift(bn *b, int d); // shifts d digits to the left, retains sign
int bn_cmp(bn *a, bn *b); // returns 1 if a&gt;b, 0 if a=b, -1 if a&lt;b
</code></pre>
"
486,Max of Brownian motion with drift is finite almost surely,"<p>For $B_t$ Brownian Motion with drift $\mu&lt;0$, I need to prove that the max value, $X = \max_{0&lt;t&lt;\infty}B_t$ is finite almost surely, ie $P(X&lt;\infty)=1$.</p>

<p>Now, I know that because the mean is negative, it will go more and more negative, and it is also a supermartingale. But I don't know how to prove almost surely...</p>

<p>Appreciate any hints.</p>
"
487,Tutorial for libsvm c++,"<p>I would like to know, if you have any code or tutorial for using LIBSVM.. I tried the site but I didn't find what I am looking for! </p>

<p>So, can you give me any written code in C++ for classifying features extracted by using SIFT or SURF... or even general one ""Basically, I want a code to understand what is happening and how to write my own code""</p>

<p>thanks </p>
"
488,Is it worth to take another sample?,"<p>There is a random normal variable, with unknown mean and std. I want to estimate the std by sampling. After several samples, I have an estimate, which I can make  more precise by taking more samples, however each additional sample has a cost. </p>

<p>How can I estimate, from the samples I currently have, the effect on an additional sample on the accuracy of my estimate of the mean?</p>
"
489,Arbitrary precision arithmetic with Ruby,"<p>How the heck does Ruby do this? Does Jörg or anyone else know what's happening behind the scenes? </p>

<p>Unfortunately I don't know C very well so <a href=""https://GitHub.Com/Ruby/Ruby/tree/trunk/bignum.c#L1816-1848"" rel=""nofollow""><code>bignum.c</code></a> is of little help to me. I was just kind of curious it someone could explain (in plain English) the theory behind whatever miracle algorithm its using.</p>

<pre><code>irb(main):001:0&gt; 999**999
</code></pre>

<blockquote>
  <p>368063488259223267894700840060521865838338232037353204655959621437025609300472231530103873614505175218691345257589896391130393189447969771645832382192366076536631132001776175977932178658703660778465765811830827876982014124022948671975678131724958064427949902810498973271030787716781467419524180040734398996952930832508934116945966120176735120823151959779536852290090377452502236990839453416790640456116471139751546750048602189291028640970574762600185950226138244530187489211615864021135312077912018844630780307462205252807737757672094320692373101032517459518497524015120165166724189816766397247824175394802028228160027100623998873667435799073054618906855460488351426611310634023489044291860510352301912426608488807462312126590206830413782664554260411266378866626653755763627796569082931785645600816236891168141774993267488171702172191072731069216881668294625679492696148976999868715671440874206427212056717373099639711168901197440416590226524192782842896415414611688187391232048327738965820265934093108172054875188246591760877131657895633586576611857277011782497943522945011248430439201297015119468730712364007639373910811953430309476832453230123996750235710787086641070310288725389595138936784715274150426495416196669832679980253436807864187160054589045664027158817958549374490512399055448819148487049363674611664609890030088549591992466360050042566270348330911795487647045949301286614658650071299695652245266080672989921799342509291635330827874264789587306974472327718704306352445925996155619153783913237212716010410294999877569745287353422903443387562746452522860420416689019732913798073773281533570910205207767157128174184873357050830752777900041943256738499067821488421053870869022738698816059810579221002560882999884763252161747566893835178558961142349304466506402373556318707175710866983035313122068321102457824112014969387225476259342872866363550383840720010832906695360553556647545295849966279980830561242960013654529514995113584909050813015198928283202189194615501403435553060147713139766323195743324848047347575473228198492343231496580885057330510949058490527738662697480293583612233134502078182014347192522391449087738579081585795613547198599661273567662441490401862839817822686573112998663038868314974259766039340894024308383451039874674061160538242392803580758232755749310843694194787991556647907091849600704712003371103926967137408125713631396699343733288014254084819379380555174777020843568689927348949484201042595271932630685747613835385434424807024615161848223715989797178155169951121052285149157137697718850449708843330475301440373094611119631361702936342263219382793996895988331701890693689862459020775599439506870005130750427949747071390095256759203426671803377068109744629909769176319526837824364926844730545524646494321826241925107158040561607706364484910978348669388142016838792902926158979355432483611517588605967745393958061959024834251565197963477521095821435651996730128376734574843289089682710350244222290017891280419782767803785277960834729869249991658417000499998999</p>
</blockquote>
"
490,An error with... operator precedence?? (.Net Compact Framework),"<p>I have this operation:</p>

<pre><code>t = (x - (y * (z + w)) - w) / 2;
</code></pre>

<p>where:</p>

<p>x = 268;
y = 4;
z = 20;
w = 30;</p>

<p>As far as I know the result will be 49, but I getting 19.</p>

<p>Where is my error? (In using this code on a .Net Compact Framework 2.0 SP2 WinForm app).</p>

<p>Thank you.</p>
"
491,grep subset index referencing,"<p>Say I have a list in R</p>

<pre><code>alist&lt;-c(""a"",""a"",""b"",""c"")
</code></pre>

<p>now I want to take a subset of that list <code>alist[c(2,3,4)]</code></p>

<p>if I apply the <code>grep</code> function to the subset i.e. </p>

<pre><code>grep(""b"",alist[c(2,3,4)]) 
</code></pre>

<p>I will get 2. Is there anyway to use the <code>grep</code> function so that I get the position of <code>b</code> in the original list and not the subset? So instead of 2 the <code>grep</code> function will return 3?</p>
"
492,Matrix multiplications for 3d scene,"<p>I am trying to write a patch for webgl for ie through creating a series of prototypes and covering functions, much like excanvas. (because I felt like it).</p>

<p>Everything is mostly working that I expect to but I am struggling with some matrix manipulation in trying to allow for 'looking up/down' -the x&amp;z rotations. I can't seem to modify my rotation matrix correctly though. I can translate the scene, rotate around the y axis fine but I not sure how to combine the x and z rotation correctly. At the moment the scene will only translate around the x axis and I can't seem to figure out how to change this to comps and include the z rotation.</p>

<p>(Apologies for the poor description but I find it hard to explain - probably why Im stuck!)</p>

<p>The easiest way to understand what I mean is by going to: <a href=""http://www.placidcow.hostoi.com/scripts/Nano%20Doom.htm"" rel=""nofollow"">http://www.placidcow.hostoi.com/scripts/Nano%20Doom.htm</a>
and use page up/down (ensure num lock is off) to look up and down, and then turn right by pressing the right arrow key, and then try to look up and down again. (The demo is the 'Doom' tutorial from learningwebgl.com).</p>

<p>Currently the matrix manipulation in the script is:</p>

<pre><code>function Multiply (pMatrix, mat2draw, c, r) {
    var R = [[pMatrix[0],pMatrix[1],pMatrix[2]],[pMatrix[3],pMatrix[4],pMatrix[5]],[pMatrix[6],pMatrix[7],pMatrix[8]]];

    if (R[2][0] != 1 &amp;&amp; R[2][0] != -1) {
        var t1 = -Math.asin(R[2][0]);
        var t2 = Math.PI - t1;
        var rho1 = Math.atan2(R[2][1]/Math.cos(t1), R[2][2]/Math.cos(t1));
        var rho2 = Math.atan2(R[2][1]/Math.cos(t2), R[2][2]/Math.cos(t2));
        var phi1 = Math.atan2(R[1][0]/Math.cos(t1), R[0][0]/Math.cos(t1));
        var phi2 = Math.atan2(R[1][0]/Math.cos(t2), R[0][0]/Math.cos(t2));
        var Angles = [[t1*180/Math.PI,rho1*180/Math.PI,phi1*180/Math.PI],[t2*180/Math.PI,rho2*180/Math.PI,phi2*180/Math.PI]];
    }
    else {
        var phi = 0;
        if (R[2][0] == -1) {
            var t = Math.PI/2;
            var rho = phi + Math.atan2(R[0][1],R[0][2]);
        }
        else {
            var t = -Math.PI/2;
            var rho = -phi + Math.atan2(-R[0][1],-R[0][2]);
        }
        var Angles =[[t*180/Math.PI,rho*180/Math.PI,phi*180/Math.PI]];
    }

    var yAngle = Angles[0][0];
    var cos_y = Math.cos(yAngle*Math.PI/180);
    var sin_y = Math.sin(yAngle*Math.PI/180);

    var zAngle = Angles[0][2];
    var cos_z = Math.cos(zAngle*Math.PI/180);
    var sin_z = Math.sin(zAngle*Math.PI/180);

    var xAngle = Angles[0][1];
    var cos_x = Math.cos(xAngle*Math.PI/180);
    var sin_x = Math.sin(xAngle*Math.PI/180);

    //Need to fix here. Take components of 2 depending on rotation of 3rd.
    //eg. if rotated around y-axis. up/down = xcos0 + zsin0??

    //rotation matrix
    var P = [[(cos_y*cos_z),(cos_x*sin_z)+(sin_x*sin_y*cos_z),(sin_x*sin_z)-(cos_x*sin_y*cos_z)],[-1*cos_y*sin_z,(cos_x*cos_z)-(sin_x*sin_y*sin_z),(sin_x*cos_z)+(cos_x*sin_y*sin_z)],[sin_y,-1*sin_x*cos_y,cos_x*cos_y]];


    var M = new BaseMatrix(c, r, mat2draw);
    var Q = new BaseMatrix(c, r, mat2draw);
    var tmp = 0;

    for (var r = 0; r &lt; P.length; r++) {
      for (var c = 0; c &lt; M.length; c++) {
        for (var i = 0; i &lt; P.length; i++) {
          tmp = parseFloat(tmp) + (parseFloat(P[r][i]) * parseFloat(M[c][i]));
        }
        Q[c][r] = tmp;
        tmp = 0;
      }
    }

    var R = new Array();
    var count=0;
    for (var i = 0; i &lt; Q.length; i++) {
          for (var j = 0; j &lt; Q[0].length; j++) {
        R[count] = Q[i][j];
            count++;
          }
        }
        return R;
}

  function BaseMatrix(r, c, mat2draw) {
    var M = new Array(c);
    var count = 0;
      for (var i = 0; i &lt; M.length; i++) {
        M[i] = new Array (r);
        for (var j = 0; j &lt; M[i].length; j++) {
          M[i][j] = mat2draw[count];
          count++;
        }
      }
    count = 0;
    return M;
}
</code></pre>

<p>I can provide more code, or clarification on request if needed.</p>

<p>Ultimately, how would I modify the above to correctly perform the multiplication and hence modify the scene correctly. Thanks in advance.</p>
"
493,can not read the whole image in numpy array through Python Imaging Library (PIL),"<p>Following the last question: <a href=""http://stackoverflow.com/questions/11571216/read-16-bit-or-32-bit-images-in-python"">read 16 bit or 32 bit images in python</a></p>

<p>I still do not know why I can not read the whole tif image in. I used Python Imaging Library (PIL) to load image and converted to numpy array. The issue is that the array appear lots of negative values, and only 2/3 image has correct values. I used ArcGIS to read in, and the values of the original images are correct, so the issue is not from the image data. 
Therefore, I changed the negative values to 0 following the last question and exported as a new tif file. I found that the two images of original image and new image are similar except the right side with negative values.</p>

<p>Does anyone try to load 16 or 32 bit image through Python Imaging Library (PIL) successfully? Do you get the correct values for the whole image?
Except this image, I also tried other images (16 and 32 bit float, 8 bit is correct) , but they also showed the similar situation. </p>

<p>The following codes show the above process and the first row of image.</p>

<pre><code>import Image
import numpy as num
im=Image.open('srtm.tif')
imarray=num.array(im)
imarray
arr[arr &lt; 0] = 0
arr[0]
array([   0.,    0.,  400.,  400.,  402.,  402.,  408.,  408.,  408.,
    416.,  416.,  426.,  426.,  426.,  437.,  437.,  450.,  450.,
    462.,  462.,  462.,  470.,  470.,  477.,  477.,  477.,  481.,
    481.,  477.,  477.,  477.,  467.,  467.,  465.,  465.,  465.,
    472.,  472.,  483.,  483.,  483.,  498.,  498.,  505.,  505.,
    505.,  507.,  507.,  507.,  507.,  507.,  508.,  508.,  517.,
    517.,  526.,  526.,  526.,  528.,  528.,  526.,  526.,  526.,
    535.,  535.,  537.,  537.,  537.,  548.,  548.,  543.,  543.,
    543.,  539.,  539.,  541.,  541.,  541.,  551.,  551.,  558.,
    558.,  558.,  558.,  558.,  549.,  549.,  533.,  533.,  533.,
    515.,  515.,  503.,  503.,  503.,  494.,  494.,  481.,  481.,
    481.,  468.,  468.,  459.,  459.,  459.,  451.,  451.,  446.,
    446.,  446.,  434.,  434.,  426.,  426.,  426.,  422.,  422.,
    421.,  421.,  419.,  419.,  419.,  410.,  410.,  398.,  398.,
    398.,  390.,  390.,  384.,  384.,  384.,  369.,  369.,  354.,
    354.,  354.,  355.,  355.,  358.,  358.,  358.,  360.,  360.,
    359.,  359.,  359.,  360.,  360.,  367.,  367.,  371.,  371.,
    371.,  381.,  381.,  390.,  390.,  390.,  399.,  399.,  404.,
    404.,  404.,  412.,  412.,  418.,  418.,  418.,  422.,  422.,
    426.,  426.,  426.,  436.,  436.,  448.,  448.,  448.,  464.,
    464.,  480.,  480.,  491.,  491.,  491.,  498.,  498.,  493.,
    493.,  493.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,
      0.,    0.,    0.], dtype=float32)
</code></pre>
"
494,Message within function (for status) not showing immediatly in console,"<p>I have written a function, which takes some time to run (due to a 1000+ loop on a huge dataset in combination with forecasting model testing).</p>

<p>To have any idea on the status, while the function is called, I use the <code>message</code> command inside the for-loop in the function. The problem is that all the messages are shown in the console after the function is finished, instead of showing immediately. So it doesn't help me :)</p>

<p>I tried to find a solution on Stackoverflow, but didn't found one. I looked for instance on the question ""<a href=""http://stackoverflow.com/q/4754996/1281181"">showing a status message in R</a>"". All answers and example codes in that topic still give me only text in the console after a function is processed instead of immediately.</p>

<p>How to solve this? Is there maybe a setting in R which prevents immediate printing of <code>message</code> text in the console?</p>

<p>note: examples I tried below, which give the same results as my function; showing text after processing the function.</p>

<p><em>example1 (Joshua Ulrich):</em></p>

<pre><code>for(i in 1:10) {
  Sys.sleep(0.2)
  # Dirk says using cat() like this is naughty ;-)
  #cat(i,""\r"")
  # So you can use message() like this, thanks to Sharpie's
  # comment to use appendLF=FALSE.
  message(i,""\r"",appendLF=FALSE)
  flush.console()
}
</code></pre>

<p><em>example2 (Tyler):</em></p>

<pre><code>test.message &lt;- function() {
  for (i in 1:9){
    cat(i)
    Sys.sleep(1)
    cat(""\b"")
  }
}
</code></pre>

<p>edit: the first example does work ('flush console' was the problem)...but when I tested it, I commented out flush console for some reason :S</p>
"
495,Compute Salary Increments,"<blockquote>
  <p>In a company, there are three categories: A,B,C.</p>
  
  <p>They want to give an increment. So if category C gets N% as increment. category B gets 2N% as increment and category A gets 3N% as increment. But the increment should be atleast 1% and The total updated salary should not exceed $50,000.</p>
  
  <p>Print the increment and the total updated salary for a particular employee.</p>
  
  <p>Assume all the required variables. </p>
</blockquote>

<p>How do I solve the above, there seem to be many unknown parameters like <code>SALARY A</code>, <code>SALARY B</code>, <code>SALARY C</code>, and increment <code>N</code>.
looking for the maximum possible value of N within the restriction</p>
"
496,Template Matching (Image Search) function in Python Imaging Library,"<p>I had a problem where I need to search for a pattern (present as a numpy ndarray) within another image (also present as a numpy ndarray) and compute a template match (minimum difference position in the image). My question is... is there any in-built image that I can possibly use in the Python Imaging Library or Numpy or anything possible that can do this without me manually writing a function to do so???</p>

<p>Thank you....</p>
"
497,Direction to angle,"<p>I'm not very good at math( and i never was ) and I've a problem: I'm trying to make sprite always rotated to mouse cursor. That's what i have:</p>

<pre><code> // mx is mouse X and my is mouse y
 double rotate = (atan2(my, mx) * PI);

 // Set rotation takes rotation in degrees
 ship.SetRotation( rad2deg(rotate) );
</code></pre>

<p>Where <code>deg2rad</code> function is:</p>

<pre><code>double rad2deg(double rad)
{
    double deg = 0;
    deg = rad * (180/M_PI);
    return deg;
}
</code></pre>

<p>Unfortunately it is not working. The ship is rotating very weird ( really hard to define that  ). And I don't have any idea to solve this problem.</p>

<p>I'm working on SFML and <a href=""http://www.sfml-dev.org/documentation/1.6/classsf_1_1Drawable.php#a3e7d558d0ef488485a2d3f885ff2b419"" rel=""nofollow""><code>SetRotation</code></a> takes degrees.</p>

<p>Thanks in advance.</p>
"
498,More efficent way of generating this random distribution?,"<p>Is there a more efficent, possibly more mathematical and less algorithmic way of achieving a similar random number distribution to this?</p>

<pre><code>unsigned int weighted_random_UINT()
{
    float r2 = 1;
    while(rand() % 4 != 0) // 3/4 chance
    {
        r2 *= fmod(
            ((float)rand()/RAND_MAX)+1, // random float between 1 and 2
            (float)UINT_MAX
        );
    }
    return (unsigned int)r2 - 1;
}
</code></pre>

<p>Below is a less safe but more easily readable version of the inside of the while.</p>

<pre><code>r2 *= ((float)rand()/RAND_MAX)+1;
</code></pre>

<p><hr>
The distribution visualized:
<img src=""http://i.stack.imgur.com/Cs41e.png"" alt=""the distribution visualized"">
<hr>
Comparison between the smoother solution in the question (1st graph) and the faster solution in the best answer (2nd graph):
<img src=""http://with-logic.co.uk/a/graph.png"" alt=""comparison""></p>
"
499,Euclidean distance with weights,"<p>I am currently using <code>SciPy</code> to calculate the euclidean distance</p>

<pre><code>dis = scipy.spatial.distance.euclidean(A,B)
</code></pre>

<p>where; A, B are 5-dimension bit vectors. It works fine now, but if I add weights for each dimension then, is it still possible to use scipy?</p>

<p>What I have now: <code>sqrt((a1-b1)^2 + (a2-b2)^2 +...+ (a5-b5)^2)</code>  </p>

<p>What I want: <code>sqrt(w1(a1-b1)^2 + w2(a2-b2)^2 +...+ w5(a5-b5)^2)</code> using scipy or numpy or any other efficient way to do this.</p>

<p>Thanks</p>
"
