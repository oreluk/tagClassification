Id,Title,Body
1,Why Does Delphi's Math.Payment Function Return A Negative Number,"<p>I'm trying to calculate the monthly payment for the following scenario:</p>

<p>$5,000 is borrowed for 3 years at 8.00% compounded monthly with $1,000 due at the end of the term.</p>

<pre><code>/*
From Math.pas 
function Payment(Rate: Extended;
 NPeriods: Integer;
 const PresentValue: Extended;
 const FutureValue: Extended;
 PaymentTime: TPaymentTime): Extended;
*/

var
  Pmt : Extended;
begin
  Pmt := Payment(0.08/12,36,5000,1000,ptEndOfPeriod);
  Edit1.Text := FloatToStr(Pmt);
end
</code></pre>

<p>Result =  <b>-181.351526101918</b><br><br>
The result is correct except it is negative.<br><br>
Why does the result of the Payment function return a negative number?</p>
"
2,"Let $X$ be the total number of heads and $Y$ be the difference between the total number of heads and the total number of tails. Find $\rho(X, Y )$","<p>A coin (with probability of getting head equal to $p$) is tossed twice. Let $X$ be the total number of heads and $Y$ be the difference between the total number of heads and the total number of tails. Find $\rho(X, Y)$.</p>
"
3,Creating a new array having logarithmic values of elements in another array in Python,"<p>Hey.. I have run into a bit of a problem with my python code.. I have a set of values each for frequency and power-spectrum. I need to plot frequency v/s power-spectrum on Log scale. I was trying to store the logarithmic values of frequency and power-spectrum in 2 other variables and then plot them.. Any idea how that can be done?</p>
"
4,Convert for loop to apply,"<p>In R, how do you replace the following code using functions like <code>apply</code>, <code>lapply</code>, <code>rapply</code>, <code>do.call</code>, etc.? </p>

<pre><code>u &lt;- 10:12
slist &lt;- list()

for (i in 1:length(u)) {
  p &lt;- combn(u, i) 
  for (j in 1:ncol(p)) {
    s &lt;- paste(p[,j], collapse="","")
    slist[[s]] &lt;- 0
  }
}
</code></pre>

<p><br></p>

<p>For this part: </p>

<pre><code>  for (j in 1:ncol(p)) {
    s &lt;- paste(p[,j], collapse="","")
</code></pre>

<p>I tried something like:</p>

<pre><code>  s &lt;- apply(p, 2, function(x) paste(x, collapse="",""))
</code></pre>

<p>Which works. But then for that <code>slist[[s]] &lt;- 0</code> part inside that same for-loop, I don't know what to do. </p>

<p><strong>Edit:</strong> This is what I'm trying to do. For the vector <code>u</code>, I'm producing a list of all the subsets in that vector. Then for each subset, I'm assigning it to <code>s</code>, then using the string <code>s</code> as the name of an element in <code>slist</code>. Kind of strange, I know, but it's for a homework assignment. For the code above, this would be the first 5 elements of slist: </p>

<pre><code> &gt; slist
 $`10`
 [1] 0

 $`11`
 [1] 0

 $`12`
 [1] 0

 $`10,11`
 [1] 0

 $`10,12`
 [1] 0
</code></pre>

<p>Yeah, I'm just trying to learn how to use apply and stuff properly. </p>
"
5,Large scale Machine Learning,"<p>I need to run various machine learning techniques on a big dataset (10-100 billions records)
The problems are mostly around text mining/information extraction and include various kernel techniques but are not restricted to them (we use some bayesian methods, bootstrapping, gradient boosting, regression trees -- many different problems and ways to solve them)</p>

<p>What would be the best implementation? I'm experienced in ML but do not have much experience how to do it for huge datasets
Is there any extendable and customizable Machine Learning libraries utilizing MapReduce infrastructure 
Strong preference to c++, but Java and python are ok
Amazon Azure or own datacenter (we can afford it)?</p>
"
6,How to produce HTML tables and accompanying CSS using R Markdown or HTML Sweave?,"<p>I previously asked a question about <a href=""http://stackoverflow.com/q/10774285/180892"">how to export a HTML table in R and have control over line borders</a>. </p>

<p>I'm used to LaTeX where when you create a table, the formatting of the table is largely determined by the text and markup that appears at that point. This works well for Sweave, because your R code chunk can output LaTeX table markup at that point. I also understand that there are tools like <code>xtable</code> that can produce HTML markup for a table.</p>

<p>However, control over HTML tables seems to rely on style sheets, which are meant to appear in the header of the document and not in the location where the R code chunk is placed. Of course, I could just put content in the style sheet, but in scientific applications often there can be some quite specific table formatting that varies in some respects from table to table.</p>

<p>Thus, my question:</p>

<ul>
<li>In general, how do you format an HTML table with literate programming like R Markdown or even from raw HTML if formatting of the output requires output to be created in a separate place in the document (i.e., CSS for the table in the header) to where the R code chunk is placed (i.e.,the table itself in the body)?</li>
</ul>
"
7,Avoiding underflow for joint probabilities using numpy,"<p>I face a problem of estimating a joint probability for independent variables in a simple setup. Currently I have an array of 100 random variables and I would like to obtain their joint probability without failing into the underflow problem. Any ideas how to achieve this goal in numpy? if possible? If not could someone please explain me further the role of numpy routine (logaddexp) as I thought it might be a help for me in such situation.</p>

<p>Thanks</p>
"
8,how do I replace numeric codes with value labels from a lookup table?,"<p>This question is related to <a href=""http://stackoverflow.com/questions/8433523/creating-a-new-variable-from-a-lookup-table"">this question</a>, but not quite the same.</p>

<p>Say I have this data frame,</p>

<pre><code>df &lt;- data.frame(
                id = c(1:6),
                profession = c(1, 5, 4, NA, 0, 5))
</code></pre>

<p>and a string with human readable information about the profession codes. Say,</p>

<pre><code>profession.code &lt;- c(
                     Optometrists=1, Accountants=2, Veterinarians=3, 
                     `Financial analysts`=4,  Nurses=5)
</code></pre>

<p>Now, I'm looking for the easiest way to replace the values in  <code>df$profession</code> with the text found in <code>profession.code</code>. Preferably without use of special libraries, unless it shortens the code significantly.</p>

<p>I would like my end result to be</p>

<pre><code>df &lt;- data.frame(
                id = c(1:6),
                profession = c(""Optometrists"", ""Nurses"", 
                ""Financial analysts"", NA, 0, ""Nurses""))
</code></pre>

<p>Any help would be greatly appreciated.</p>

<p>Thanks,
Eric </p>
"
9,"Let $X$ be the total number of heads and $Y$ be the difference between the total number of heads and the total number of tails. Find $\rho(X, Y )$","<p>A coin (with probability of getting head equal to $p$) is tossed twice. Let $X$ be the total number of heads and $Y$ be the difference between the total number of heads and the total number of tails. Find $\rho(X, Y)$.</p>
"
10,Comparing reference classes objects in R,"<p>i recently started using <a href=""http://www.inside-r.org/r-doc/methods/ReferenceClasses"" rel=""nofollow"">reference classes</a>. At some point i would like to test whether two variables point at the same instance, or at different ones. </p>

<p>however, could not find any appropriate operator or function for this purpose:</p>

<ul>
<li><code>==</code> doesn't apply to objects, </li>
<li><code>identical</code> and <code>all.equal</code> just consider values, so won't be able to discriminate between two identical but distinct instances,</li>
<li>same thing for the package <a href=""http://cran.r-project.org/web/packages/compare/"" rel=""nofollow""><code>compare</code></a></li>
</ul>

<p>if anyone knows how to do that, i'd be much obliged!</p>

<p>thanks</p>
"
11,Parsing math equations,"<p>Just for kicks, I'm trying to create an application that can simplify, factor, and expand algebra equations. Programming the rules seems as if it will be straight forward if I can get the equations into a good workable format. Parsing the equations is proving to be a hassle, Currently working with Python but I'm not against having to learn something new. </p>

<p>Are there any libraries (for any language) that would make this project pretty simple, or is that a pipe dream? </p>

<p>[Tagging this with Haskell because I have a feeling that's where the 'simple' is]</p>
"
12,What book to use to learn Algorithms and Data Structures?,"<p>I want to learn Algorithms and become fluent in them. My goal is to land a job as a Software Development Engineer at either Google or Microsoft. I've been recommended to use the book ""Introduction to Algorithms"" by Cormen.</p>

<p>But I feel like the book is not for beginners.</p>

<p>Can anyone recommend a book on Algoritms that I can find easier ? one that after reading can help me read and understand Intro to Alg. ?</p>
"
13,"Using pytables, which is more efficient: scipy.sparse or numpy dense matrix?","<p>When using <code>pytables</code>, there's no support (as far as I can tell) for the <code>scipy.sparse</code> matrix formats, so to store a matrix I have to do some conversion, e.g.</p>

<pre><code>def store_sparse_matrix(self):
    grp1 = self.getFileHandle().createGroup(self.getGroup(), 'M')
    self.getFileHandle().createArray(grp1, 'data', M.tocsr().data)
    self.getFileHandle().createArray(grp1, 'indptr', M.tocsr().indptr)
    self.getFileHandle().createArray(grp1, 'indices', M.tocsr().indices)

def get_sparse_matrix(self):
    return sparse.csr_matrix((self.getGroup().M.data, self.getGroup().M.indices, self.getGroup().M.indptr))
</code></pre>

<p>The trouble is that the <code>get_sparse</code> function takes some time (reading from disk), and if I understand it correctly also requires the data to fit into memory.</p>

<p>The only other option seems to convert the matrix to dense format (<code>numpy array</code>) and then use <code>pytables</code> normally. However this seems to be rather inefficient, although I suppose perhaps <code>pytables</code> will deal with the compression itself?</p>
"
14,Getting plot boundaries in R,"<p><strong>My question: How do I find the boundaries for the plot area?</strong></p>

<p>I'm using the <a href=""http://rforge.org/plothr/"" rel=""nofollow"">plotHR</a> function for plotting splines from cox regressions. I've done some adaptations found <a href=""http://pastebin.com/v0LZje5F"" rel=""nofollow"">here</a> and the density plot annoys me so I want to be able to change it to a simple regular polygon without the need for using a second plot. The problem is that the second plot needs repositioning as soon as the medium size changes.</p>

<p>Now I know I can use the plot_boundaries.y and plot_boundaries.x together with the limits and adding the xaxs/yaxs information but this seems like not the most intuitive way of doing this. Is there an easy way of getting my plots bottom left x,y and top right x,y?</p>

<p>Here's an example of how to use the plotHR:</p>

<pre><code>library(survival)

hmohiv&lt;-read.table(""http://www.ats.ucla.edu/stat/R/examples/asa/hmohiv.csv"", sep="","", header = TRUE)

surv &lt;- with(hmohiv, Surv(time, censor))
fit &lt;- coxph(surv~ pspline(age), data=hmohiv)
par(xaxs=""i"", yaxs=""i"")
plotHR(fit, bty=""l"", ylim=c(.4, 5), y.ticks=c(.5, 1, 1.5, 2, 3, 4), xlim=c(25, 55))
</code></pre>
"
15,Defining controlling parameters for graphs globally in knitr,"<p>To control the size of graph in <code>knitr</code> I use the following code</p>

<pre><code>\begin{figure}[H]
   \centering
&lt;&lt; label = Plot1, fig.width=5, fig.height=5, out.width=.7\linewidth, fig.keep = all&gt;&gt;=
...
@
    \caption{Plot}
    \label{fig:figPlot1}
\end{figure}
</code></pre>

<p>I wonder how to define these controlling parameters for graphs globally instead for defining each graph. Thanks for your help and time.</p>
"
16,selecting certain rows in a data frame,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stackoverflow.com/questions/3971006/r-get-qualifying-rows-from-data"">R: get qualifying rows from data?</a>  </p>
</blockquote>



<p>I like to retrieve rows that meet some criteria. for example, from the below set up numbers, I like to retrieve rows Percent_free &lt; 10. How would I do this in R?</p>

<pre><code> Date             file                 Percent_free
2011-12-06            /tmp             2
2011-12-06            /var             9
2011-12-06            /crash           11
</code></pre>
"
17,Is there any difference between DATE_SUB() and using arithmetic operators for datetime calculation?,"<p>After I have seen a lot of questions here using the <code>DATE_SUB()</code> or <code>DATE_ADD()</code> functions instead of the arithmetic operators <code>+</code> or <code>-</code>, I was wondering if there was any difference:</p>

<p>Quote from the <a href=""http://dev.mysql.com/doc/refman/5.1/en/date-and-time-functions.html#function_date-add"" rel=""nofollow"">MySQL-manual</a>:</p>

<blockquote>
  <p>Date arithmetic also can be performed using INTERVAL together with the + or
  - operator:</p>

<pre><code>date + INTERVAL expr unit
date - INTERVAL expr unit
</code></pre>
</blockquote>

<p>So basically, these two statements return the same result:</p>

<pre><code>SELECT DATE_ADD(NOW(), INTERVAL 7 DAY);
</code></pre>

<p>and</p>

<pre><code>SELECT NOW() + INTERVAL 7 DAY;
</code></pre>

<p>Now my question:</p>

<p><strong>Is there any difference between <code>DATE_SUB()</code> and using the <code>-</code> operator in MySQL?</strong> (besides readability?)</p>
"
18,"Equivalent of ""throw"" in R","<p>How does one ""throw"" an error in R?  I have a function that takes a data frame and some column names and does stuff with them.  If the columns don't exist, I want the function to stop and to stop all functions depending on it. </p>

<p>I have looked at <code>recover</code> and <code>browse</code> and <code>traceback</code> but, well, they seemed to be close but not what I am looking for.</p>
"
19,"I have imported the data in R, how to make a scatter plot?","<p>I'm new to <code>R</code>, and have imported my dataset as follows (dots mean that there is remaining data):</p>

<pre><code>&gt; num.csv &lt;- read.csv(""c:/num.csv"", header=T)
&gt; print(num.csv)
            X.Y
1     22500;482
2       25842;1
3       27221;1
4       32757;1
5       40152;1
.       .
.       .
.       .
</code></pre>

<p>How can I make a scatter plot for this data?</p>

<p>Thanks.</p>
"
20,strsplit one column with exact information into two column,"<p>I have data looking like this:</p>

<pre><code>    SNP Geno Allele
marker1   G1    AA
marker2   G1    TT
marker3   G1    TT
marker1   G2    CC
marker2   G2    AA
marker3   G2    TT
marker1   G3    GG
marker2   G3    AA
marker3   G3    TT
</code></pre>

<p>And I want it to look like this:</p>

<pre><code>    SNP Geno Allele1 Allele2
marker1   G1       A       A
marker2   G1       T       T
marker3   G1       T       T
marker1   G2       C       C
marker2   G2       A       A
marker3   G2       T       T
marker1   G3       G       G
marker2   G3       A       A
marker3   G3       T       T
</code></pre>

<p>I am using this:</p>

<pre><code>strsplit(Allele, split extended = TRUE)
</code></pre>

<p>But this is not working. Do I need additional commands?</p>
"
21,Writing a 36 bit random number generator,"<p>I'm writing an application in an environment that features</p>

<ul>
<li>36 bit one's complement integers</li>
<li>arithmetic limited to <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code> and remainder</li>
<li>no bit operations like <code>AND</code> or <code>OR</code>. But because of one's complement, <code>XOR</code> is equivalent to subtraction and <code>NOT</code> to negation.</li>
<li>numeric overflow is fatal, so can't be used for silent truncation</li>
<li>Yes, there are conditionals: <code>IF/THEN/ELSEIF/ELSE/IF</code>.</li>
</ul>

<p>Ideally, I'd like 35 or 36 bit random integers, but 25 bits would suffice too.</p>

<p>My naive implementations of the <a href=""http://en.wikipedia.org/wiki/Linear_congruential_generator"">linear congruential generator</a> run into overflow if based on sufficiently large numbers and produce only a small number of bits when I use smaller numbers.</p>

<p>So I'm looking either for a set of numbers <em>a,c,m</em> that will yield the maximum number of bits in the constraints, or a sensible adaptation of the LCG to combine 2 or more numbers.</p>

<p>As a starting point, here's what I'm using so far:</p>

<pre><code>*DEFINE NextRandom . min,max resultVar
* . This is a very shitty RNG. The numbers were never checked
* . for suitability for a long-period linear congruential generator.
* . But it yields numbers that look vaguely random.
*CLEAR rq
*CLEAR rr
*SET RandZ = RandZ * 169687 + 347011      . RandZ is a global var.
*DIVIDE RandZ BY 131072 GIVING rq, RandZ  . Division yields a remainder
*DIVIDE RandZ BY 4 GIVING rq
*SET r0 = +[#,[#],1,1]                    . r0 = argument 'min'
*SET r9 = +[#,[#],1,2]                    . r9 = 'max'
*SET rd = r9 - r0 + 1
*DIVIDE rq BY rd GIVING rq, rr
*SET [#,[#],2,1] TO r0 + rr               . return in 'resultVar'
*ENDDEFINE
</code></pre>

<hr>

<p>In case anyone cares, the scripting language is SSG (Symbolic Stream Generator) in a UNISYS 2200 mainframe operating system called EXEC 8.</p>

<hr>

<p>On criticality: The app this RNG works in generates test data. It's not encrypting state secrets or the nuclear missile codes. So we're talking about ""nice to have"" and ""best effort"" rather than ""mission critical."" I'd be happy about an improvement but am not looking for the ultimate solution.</p>
"
22,ggplot - change line width,"<p>I can plot each series in a different colour in ggplot2 by doing something like this ...</p>

<pre><code>colours &lt;- c('red', 'blue')
p &lt;- ggplot(data=m, mapping=aes_string(x='Date', y='value'))
p &lt;- p + geom_line(mapping=aes_string(group='variable', colour='variable'), size=0.8)
p &lt;- p + scale_colour_manual(values=colours)
</code></pre>

<p>Is there something comparable I can do to set different line widths for each series? (Ie. I want to use a thick red line to plot the trend and a thin blue line to plot the seasonally adjusted series.)</p>
"
23,Is it possible to open raster layer or stack in RGUI from the directory?,"<p>Hi i want to open an image file which is in tiff format and i am creating a GUI with the help of packages RGtk2 and gwidgets, i am able to browse tiff format file but the problem is it is not displaying anywhere, for that i need a window or frame which display that layer or any raster brick, please anyone tell me how is it possible to do with RGUI.</p>
"
24,How can I remove repeated characters in a string with R?,"<p>I would like to implement a function with <code>R</code> that removes repeated characters in a string. For instance, say my function is named <code>removeRS</code>, so it is supposed to work this way:</p>

<pre><code>  removeRS('Buenaaaaaaaaa Suerrrrte')
  Buena Suerte
  removeRS('Hoy estoy tristeeeeeee')
  Hoy estoy triste
</code></pre>

<p>My function is going to be used with strings written in spanish, so it is not that common (or at least correct) to find words that have more than three successive vowels. No bother about the possible sentiment behind them. Nonetheless, there are words that can have two successive consonants (especially ll and rr), but we could skip this from our function.</p>

<p>So, to sum up, this function should replace the letters that appear at least three times in a row with just that letter. In one of the examples above, <code>aaaaaaaaa</code> is replaced with <code>a</code>.</p>

<p>Could you give me any hints to carry out this task with <code>R</code>?</p>
"
25,Making multiple plots in R from one textfile,"<p>I'm new to R and trying to generate a lot of graphs from one file, with headers between different data sets.
I have a tab-delimited plaintext file, formatted like this:</p>

<pre><code>Header: Boston city data
Month    Data1    Data2    Data3
1        1.5      9.1342   8.1231
2        12.3     12.31    1.129
3        (etc...)  

Header: Chicago city data
Month    Data1    Data2    Data3
1        1.5      9.1342   8.1231
2        12.3     12.31    1.129
...
</code></pre>

<p>I would like to create a graph of month vs Data1, month vs Data2, and month vs Data2, for each city.</p>

<p>I know in python, I could iterate through each line, do something different if the line starts with 'Header', and then somehow process the numbers. I would like to simply do this:</p>

<pre><code>for (data block starting with header) in inf:
    data = read.delim()
    barplot(data, main=header, ylab=""Data1"", xlab=""Month"")
    # repeat for Data2, Data3
</code></pre>

<p>but I'm not sure how to actually iterate through the file, or if I should just split up my file by city into lots of small files, then run through a list of small files to read.</p>
"
26,R - logic manipulation via vector operations,"<p>I would like to convert this piece of logic with purely matrix operations instead of for loops. The logic is that in my binary value vector, I want to note every transition point (i.e. where 0 turns to 1 and where 1 turns to 0). Otherwise I want to retain the original values. While a simple loop is fast enough for small vector, I'll need to perform this operation several times over large datasets hence the need for efficiency via matrics. </p>

<pre><code>x &lt;- c(1,0,0,0,0,1,1,0,0,1,0,1,0,1,0,0,0,1);
y &lt;- rep(-2, length(x));
y[1] &lt;- x[1];
for(i in 2:length(x)){
     if((x[i]==1 &amp;&amp; x[i-1]==0) || (x[i]==0 &amp;&amp; x[i-1]==1)){
       y[i] = -1;
    }
    else{
       y[i] = x[i];
     }
}
</code></pre>

<p>final value of y is
    1 -1 0 0 0 -1 1 -1 0 -1 -1 -1 -1 -1 -1 0 0 -1</p>

<p>I'm a new convert to R, many thanks in advance</p>
"
27,How to replace a value in multiple columns based on criteria?,"<p>I would like to replace all values <code>99</code> with <code>NA</code> for all records where <code>ART == '999'</code> only in columns <code>L1:L8</code>. I know how to do this for once column at a time, but I would like to do this more efficiently for all columns in one command.</p>

<p>Sample data:</p>

<pre><code>df &lt;- structure(list(KARTA = c(""02C2H"", ""02C2H"", ""02C2H"", ""02C2H"", 
""02C2H"", ""02C2H"", ""02C2H"", ""02C2H"", ""02C2H"", ""02C2H"", ""02C2H"", 
""02C2H"", ""02C2H"", ""02C7H"", ""02C7H"", ""02C7H"", ""02C7H"", ""02C7H"", 
""02C7H"", ""02C7H"", ""02C7H"", ""02C7H"", ""02C7H"", ""02C7H"", ""02C7H""
), YEAR = c(1997L, 1999L, 2000L, 2001L, 2002L, 2003L, 2005L, 
2006L, 2007L, 2008L, 2009L, 2010L, 2011L, 1997L, 1998L, 2000L, 
2001L, 2002L, 2003L, 2004L, 2006L, 2008L, 2009L, 2010L, 2011L
), ART = c(""999"", ""999"", ""100"", ""100"", ""100"", ""999"", ""999"", ""999"", 
""999"", ""999"", ""999"", ""999"", ""999"", ""999"", ""999"", ""999"", ""999"", 
""999"", ""999"", ""999"", ""999"", ""999"", ""999"", ""999"", ""999""), L1 = c(99, 
99, 99, 99, 99, 10, 10, 10, 10, 10, 10, 10, 10, 99, 99, 99, 99, 
99, 10, 10, 10, 10, 10, 10, 10), L2 = c(99, 99, 99, 99, 99, 10, 
10, 10, 10, 10, 10, 10, 10, 99, 99, 99, 99, 99, 10, 9, 10, 10, 
10, 10, 10), L3 = c(99, 99, 99, 99, 99, 7, 10, 10, 10, 10, 10, 
10, 10, 99, 99, 99, 99, 99, 10, 10, 10, 10, 10, 10, 10), L4 = c(99, 
99, 99, 99, 99, 10, 10, 10, 10, 10, 10, 10, 10, 99, 99, 99, 99, 
99, 10, 10, 8, 7, 7, 10, 8), L5 = c(99, 99, 99, 99, 99, 5, 8, 
10, 10, 10, 10, 10, 10, 99, NA, 99, 99, 99, 10, 10, 7, 7, 0, 
10, 8), L6 = c(99, 99, 99, 99, 99, 8, 10, 10, 10, 10, 10, 10, 
10, 99, 99, 99, 99, 99, 10, 9, 10, 10, 10, 10, 10), L7 = c(99, 
99, 99, 99, 99, 10, 10, 10, 10, 10, 10, 8, 10, 99, 99, 99, 99, 
99, 10, 10, 10, 10, 10, 10, 10), L8 = c(99, 99, 99, 99, 99, 10, 
10, 10, 10, 10, 10, 10, 10, 99, 99, 99, 99, 99, 10, 10, 6, 10, 
10, 10, 10)), .Names = c(""KARTA"", ""YEAR"", ""ART"", ""L1"", ""L2"", 
""L3"", ""L4"", ""L5"", ""L6"", ""L7"", ""L8""), row.names = c(161008L, 161009L, 
161010L, 161011L, 161012L, 87055L, 106223L, 128072L, 160909L, 
172583L, 208774L, 45L, 227972L, 161013L, 161014L, 161015L, 161016L, 
161017L, 71813L, 89034L, 139633L, 181266L, 208838L, 97L, 225989L
), class = ""data.frame"")
</code></pre>

<p>Example of how to replace values in a single column ('L1')</p>

<pre><code>df[which(df$ART == '999' &amp; df$L1 == '99'), ] &lt;- NA
</code></pre>
"
28,Efficient way of finding distance between two 3D points,"<p>I am writing a code in C++ and want to compute distance between two points. 
<strong>Question 1:</strong> </p>

<p>I have two points P(x1, y1, z1) and Q(x2, y2, z2)  , where x, y and z are floats/doubles. </p>

<p>I want to find the distance between these two points. One way to do it is :</p>

<p>square_root(x_diff*x_diff + y_diff*y_diff + z_diff*z_diff) </p>

<p>But this is probably not the most efficient way . (e.g. a better formula or a ready made utility in <code>math.h</code> etc ) </p>

<p><strong>Question 2:</strong> </p>

<p>Is there a better way if I just want to determine if P and Q are in fact the same points? </p>

<p>My inputs are x, y and z coordinates of both the points. </p>

<p>Thank you</p>
"
29,NumPy k-th diagonal indices,"<p>I'd like to do arithmetics with k-th diagonal of a numpy.array. I need those indices.
For example, something like:</p>

<pre><code>&gt;&gt;&gt; a = numpy.eye(2)
&gt;&gt;&gt; a[numpy.diag_indices(a, k=-1)] = 5
&gt;&gt;&gt; a
array([[ 1.,  0.],
       [ 5.,  1.]])
</code></pre>

<p>Unfortunately, diag_indices only returns the indices comprising the main diagonal, so at the moment I am doing:</p>

<pre><code>a += numpy.diag([5], -1)
</code></pre>

<p>But that doesn't seem as nice or robust. :-)</p>

<p>Is there a way in numpy to get indices to other than the main diagonal?</p>
"
30,Why does the logarithm algorithm for Divide Two Integers fail some special test cases?,"<p>I tried to used the formula <code>a/b = e^(ln a - ln b)</code> to solve the infamous <code>Divide 2 Integers</code> without using / % * question, however for some test cases <code>(dividend=Integer.MAX_VALUE or MIN_VALUE and divisor=1)</code> my solution fails.</p>

<p>Why does it fail?</p>

<p>[EDIT]: The answers I get for that test case are <code>(MAX-1 or MIN+1)</code>. I'd like to know why this happens.</p>

<pre><code>public int divide(int dividend, int divisor) {
    boolean neg = false;
    if ((dividend &lt; 0 &amp;&amp; divisor &gt; 0) || (dividend &gt; 0 &amp;&amp; divisor &lt; 0))
        neg = true;

    long a = dividend;
    a = Math.abs(a);
    long b = divisor;
    b = Math.abs(b);

    double res = Math.pow(Math.E, Math.log(a) - Math.log(b));
    int ans = Math.floor(res);
    return neg ? -ans : ans;
}
</code></pre>
"
31,Splitting Strings and Generating Frequency Tables in R,"<p>I have a column of firm names in an R dataframe that goes something like this:</p>

<pre><code>""ABC Industries""  
""ABC Enterprises""  
""123 and 456 Corporation""  
""XYZ Company""
</code></pre>

<p>And so on.  I'm trying to generate frequency tables of every word that appears in this column, so for example, something like this:</p>

<pre><code>Industries   10  
Corporation  31  
Enterprise   40  
ABC          30  
XYZ          40  
</code></pre>

<p>I'm relatively new to R, so I was wondering of a good way to approach this.  Should I be splitting the strings and placing every distinct word into a new column?  Is there a way to split up a multi-word row into multiple rows with one word?   </p>

<p>Thanks.</p>
"
32,classify cell array in matlab,"<p>I want to do text categorization on a dataset of news. I have a lot of features like <code>subject</code>, <code>keyword</code>, <code>summary</code>, etc... all of these features are stored in one cell array of structs, each struct looking like this:</p>

<pre><code>       label: 'misc.forsale'
        subj: ' Motorcycle wanted.'
     keyword: [1x190 char]
   reference: []
organization: ' Worcester Polytechnic Institute'
        from: ' kedz@bigwpi.WPI.EDU (John Kedziora)'
     summary: []
       lines: ' 11'
       vocab: [4x2 double]
</code></pre>

<p>I want to classify them with <code>class = classify(test, train, target, 'diaglinear');</code><br>but these functions only receive arrays as input, and do not accept cells or structs.</p>

<p>I can't convert this cell array to <em>one</em> multidimensional array because the amount of features varies (for example, one subject has two words and other has three words).</p>

<p>What can I do?</p>
"
33,Why do I get an error when I call int(input()) where input is decimal (eg. '87.94')?,"<p>I'm a programming newbie having difficulty with Python multiplication. I have code like this:</p>

<pre><code>def getPriceDiscount():
    price = int(input())
    if price &gt; 3000:
        priceDiscount = price * 0.6
        return priceDiscount
    else:
        return price
</code></pre>

<p>But when I execute it and type an input which is a decimal number like 87.94, I get the following error:</p>

<pre><code>ValueError: invalid literal for int() with base 10: '87.94'
</code></pre>

<p>Isn't the <code>int()</code> method able to convert the string '87.94' into a number allowing me to multiply it by 0.6? What should I do to perform that conversion?</p>

<p>I'm using Python 3.2.2.</p>
"
34,Detect conflicts between packages in R,"<p>I've recently found out that errors can be caused due to conflicts between packages, that is, two (or more) packages might have functions named similarly. I know that the code <code>search ()</code> produces the list of packages ordered in the way R reads them. There is also the <code>args</code> code which gives the function read by R.  <br/> What I would like to know firstly is how to detect if an error is being produced because of conflicts between packages and secondly how to find out which packages are conflicting? Finally, after the conflicts have been detected, how can we force R to use specifically the function from  one of the packages?</p>
"
35,ggplot2 : Adding two errorbars to each point in scatterplot,"<p>I need to plot two error-bars on each point in a scatterplot. The usual is vertical error-bars that corresponds to the error on the points y-value, but I need to add the error-bar associated with the X-axis (horizontal) as well. I could probably do this with some abline command, but thought there might be a more clever way to do it with ggplot2?</p>
"
36,Changing Tor identity in R,"<p>I am using Tor in combination with R and would like to change my IP for each new request. The code I have is as follows:</p>

<pre><code>library(RCurl)
opts &lt;- list(proxy=""127.0.0.1"", proxyport=8118)
for (i in 1:10)
  {
  con &lt;- socketConnection(host=""127.0.0.1"",port=9051)  # DOES NOT WORK
  writeLines(""signal newnym"", con=con)                 # DOES NOT WORK
  ip &lt;- getURL(""http://ifconfig.me/ip"", .opts = opts)  
  print(ip)
  Sys.sleep(1)
  }  
</code></pre>

<p>I am able to connect via Tor, however the two lines marked as 'DOES NOT WORK' don't seem to get the proper signal across to Tor, so the IP stays the same.</p>

<p>Regards!</p>
"
37,How to perform single factor ANOVA in R with samples organized by column?,"<p>I have a data set where the samples are grouped by column.  The following sample dataset is similar to my data's format:</p>

<pre><code>a = c(1,3,4,6,8)
b = c(3,6,8,3,6)
c = c(2,1,4,3,6)
d = c(2,2,3,3,4)

mydata = data.frame(cbind(a,b,c,d))
</code></pre>

<p>When I perform a single factor ANOVA in Excel using the above dataset, I get the following results:</p>

<p><img src=""http://i.stack.imgur.com/aBdfH.jpg"" alt=""enter image description here""></p>

<p>I know a typical format in R is as follows:</p>

<pre><code>group  measurement
a      1
a      3
a      4
.      .
.      .
.      .
d      4
</code></pre>

<p>And the command to perform ANOVA in R would be to use <code>aov(group~measurement, data = mydata)</code>.  <strong>How do I perform single factor ANOVA in R with samples organized by column rather than by row?</strong>  In other words, how do I duplicate the excel results using R?  Many thanks for the help.</p>
"
38,Dot product of every element with every other element of an array,"<p>Is there an easy way to take the dot product of one element of an array with every other?
So given:</p>

<pre><code>array([[1, 2, 3],
       [4, 5, 6],
       [7, 8, 9]])
</code></pre>

<p>I would like to get the result:</p>

<pre><code>array([  32.,   50.,  122.])
</code></pre>

<p>I.e. a[0] dot a[1], a[0] dot a[2], a[1] dot a[2].</p>

<p>The array I am working with will NOT be square; that's just an example.</p>

<p>Thanks!</p>
"
39,How to convert wave data into Complex numbers,"<p>I'm reading raw data from a mic and feeding into FFT. Two of the FFT libraries I'm trying (<a href=""http://code.google.com/p/aforge/"" rel=""nofollow"">AForge</a> and <a href=""http://www.exocortex.org/dsp/"" rel=""nofollow"">Exocortex.DSP</a>) takes Complex numbers as input and gives Complex numbers as output.</p>

<p>I'm trying to understand what complex numbers are.</p>

<p>More specifically - how do I convert the raw audio data obtained from a microphone into complex numbers for processing in FFT?<br />
And how do I plot the output to a nice spectrogram (that is; reading the frequencies and amplitudes from the output)?</p>

<p>Added bonus: What FFT libs are there for .Net other than the two mentioned?</p>
"
40,Insert a non-character if a criteria is TRUE,"<p>Is it possible to insert a non-character, in this case -, if a particular criteria is met? </p>

<p>For example:
If there are five numeric characters (12345), then insert a - after the 2nd numeric character (12-345).</p>

<p>I am trying to fix street addresses.</p>

<p>Thanks!</p>
"
41,Statistics on RAM malfunction,"<p>Does anyone know about any statistics or studies about how often computers have malfunctioning RAM?</p>

<p>Update: <strong>My computer is fine!</strong> I don't have RAM problems, I'm interested in the statistics. I get bug reports for my software for which one cause could be malfunctioning RAM on the user's computer, and I would like to know how likely that is.</p>

<p>Thanks!</p>

<p>Carl</p>
"
42,"Plot Issues - Start always in (0,0)","<p>I am working with a huge data set where all columns look something like this:
0
10
12
30
10
0
20
30
0
40
50
10
0</p>

<p>The idea is to make a simple plot in R where every time it reads a 0 the plot will begin in (0,0).
Do you have any idea of how I can do this?</p>

<p>Thanks in advance,
J</p>

<p>UPDATE:
I am a new user so I can't post any images!
Here's an example of the column I want to plot:</p>

<p>0<br>
10<br>
20<br>
12<br>
5<br>
6<br>
9<br>
0<br>
20<br>
24<br>
40<br>
14<br>
0<br>
20<br>
59<br>
50<br>
12<br>
0<br>
20<br>
23<br>
49<br>
45<br>
23<br>
12<br>
(...)</p>

<p>Image a line plot.
Instead of plotting a long line with all the values I want to plot several shorter lines with the first line plotting (0,10,20,12,5,6,9), the second line plotting (0,20,24,40,14) etc...</p>
"
43,What should I notice when I design the database schema for data-mining use?,"<p>I'm going to design a database. The data in the database will be used in data-mining purpose later. I want to know if there is a better practice or way to design the database so that it will be useful for data-mining use. Particularly what should I notice when I design the database schema for data-mining use?</p>
"
44,Elegant calculation of array elements,"<p>I would like to do some calculations and comparisons inside my array</p>

<p>assuming my array is simple as:</p>

<pre><code>NSMutableArray *array;   
array = [[NSMutableArray alloc] init]; 
[array addObject:[NSNumber numberWithInt:1]]; 
[array addObject:[NSNumber numberWithInt:2]]; 
[array addObject:[NSNumber numberWithInt:2]]; 
[array addObject:[NSNumber numberWithInt:3]]; 
[array addObject:[NSNumber numberWithInt:2]]; 
</code></pre>

<p>What would be an <strong>elegant and clever</strong> way to check if a number repeats itself 2,3,4 or 5 times inside this array.
(looking for something smart and dynamic....)</p>
"
45,mysql php date math (dateandtimeA - dateandtimeB),"<p>I am working on a time system where I need to take the time someone punches in to when some on punches out</p>

<pre><code>i.e 2011-05-20 08:30:00 (punch in time) - 2011-05-20 17:30:00 = X hrs
or  2011-05-20 09:45:00                 - 2011-05-25 17:57:00 = X days Y hrs
</code></pre>

<p>Any help would be awesome!</p>
"
46,Continuous-time stochastic process that is left-continuous predictable process - why?,"<p>Predictable processes are basically deterministic processes - and I am wondering why continuous-time processes that are left-continuous are automatically predictable processes. To my eyes, right-continuous and left-continuous seem not much different in determining whether a process is predictable process or not.. Can anyone explain why? (I do understand what left-continuous and right-continuous are.)</p>
"
47,Efficiency of floating point division and checking values beforehand if eq,"<p>I have a situation where I <em>might</em> need to apply a multiplier to a value in order to get the correct results.  This involves computing the value using floating point division.</p>

<p>I'm thinking it would be a good idea to check the values before I perform floating point logic on them to save processor time, however I'm not sure how efficient it will be at run-time either way.</p>

<p>I'm assuming that the <code>if</code> check is 1 or 2 instructions (been a while since assembly class), and that the floating point operation is going to be many more than that.</p>

<pre><code>//Check 
if (a != 10) {              //1 or 2 instructions?
    b *= (float) a / 10;    //Many instructions?
}
</code></pre>

<p>Value <code>a</code> is going to be '10' most of the time, however there are a few instances where it wont be.  Is the floating point division going to take very many cycles even if <code>a</code> is equal to the divisor?</p>

<p>Will the previous code with the <code>if</code> statement execute more efficiently than simply the next one without?</p>

<pre><code>//Don't check
b *= (float) a / 10;    //Many instructions?
</code></pre>

<p>Granted there wont be any noticable difference either way, however I'm curious as to the behavior of the floating point multiplication when the divisor is equal to the dividend in case things get processor heavy.</p>
"
48,What machine learning algorithm is appropriate for predicting one time-series from another?,"<p>You are a plane tracking an enemy ship that travels across the ocean, so you have collected a series of (x,y,time) coordinates of the ship. You know that a hidden submarine travels with the ship to protect it, but while there is a correlation between their positions, the submarine often wanders off from the ship, so while it's often near it, it can also be on the other side of the world occasionally. You want to predict the path of the submarine, but unfortunately it is hidden from you.</p>

<p>But one month in April you notice the submarine forgets to hide itself, so you have a series of coordinates for both the submarine and the ship throughout 1,000 trips. Using this data, you'd like to build a model to predict the hidden submarine's path given just the ship's movements. The naive baseline would be to say ""submarine position guess = ""ship's current position"" but from the April data where the submarine was visible, you notice there is a tendency for the submarine to be ahead of the ship a bit, so ""submarine position guess = ship's position in 1 minute"" is an even better estimate. Furthermore, the April data shows that when the ship pauses in the water for an extended period, the submarine is likely to be far away patrolling the coastal waters. There are other patterns of course.</p>

<p>How would you build this model, given the April data as training data, to predict the submarine's path? My current solution is an ad-hoc linear regression where the factors are ""trip time"", ""cargo ship's x coordinate"", ""was cargo ship idle for 1 day"", etc. and then having R figure out the weights and doing a cross-validation. But I would really love a way to generate these factors automatically from the April data. Also, a model that uses sequence or time would be nice, since the linear regression doesn't and I think it's relevant.</p>

<p>Edit: I've reformulated the problem with a made-up story so it's a less confusing. The original problem I posted is:</p>

<p>I have eye-tracking data on two subjects -- a teacher, and a student. It's in the form (x, y, time), so there is a series of these for each subject. What the teacher looks at influences what the student looks at. What method would I use to predict what the student is looking at, using only teacher data? Lets say I can train some learning algorithm using a gold standard set of student and teacher data.</p>

<p>I was thinking hidden markov model would be appropriate, given the definition in Wikipedia, but I am not sure how to put this into practice over my dataset.</p>

<p>More detail: I have data about how a teacher and student each look at a map and some readings. I have 40 of these datasets, which look like [(366,234,0), (386,234,5), ...] which means the teacher looked at point (366,234) at time 0 and then 5 seconds later moved up to look at coordinate (386, 234). I can to learn a model to understand the relationship between how a teacher looks at content, to predict how a student will look at the same content. So maybe the student looks at the content in the same order as the teacher but slower. Or perhaps the student doesn't look around as much but the teacher scans more of the content. I have both sets of data and want to see how accurate of a model I can get -- would I be able to predict the student's looking behavior within 50px of the teacher's looking behavior?</p>
"
49,How to batch select and calculate arrays in Numpy?,"<p>How to (1) batch select all arrays under a hdf5 file, then (2) apply calculations on those arrays and finally (3) batch create new arrays in another hdf5 file?</p>

<p>for example:</p>

<pre><code>import numpy
import tables

file = openFile('file1',""r"")

array1 = file.root.array1
array1_cal = (array1 &lt;= 1)
newfile.createArray('/','array1_cal',array1_cal)

array2 = file.root.array2
array2_cal = (array2 &lt;= 1)
newfile.createArray('/','array2_cal',array2_cal)
</code></pre>

<p>I have 100+ arrays under a single hdf5 file and several hdf5 files, how can I batch process them? Thanks a lot.</p>
"
50,Using Common Lisp for new Machine Learning project,"<p>We've been looking into languages for a ML project at work. A colleague of mine is a big Common Lisp fan, however I have some concerns. Are there any good/modern ML libraries for Common Lisp that people know of (something comparable to Weka)? Also, does anyone know of a <em>good</em> statistics library for CLisp? </p>
"
51,Adding/Combining Standard Deviations,"<p><strong>Short Version:</strong><br>
Can StdDevs be added/combined? i.e.</p>

<pre><code>if StdDev(11,14,16,17)=X and StdDev(21,34,43,12)=Y  
can we calculate StdDev(11,14,16,17,21,34,43,12) from X &amp; Y
</code></pre>

<p><strong>Long Version:</strong><br>
I am designing a star schema. The schema has a fact_table (grain=transaction) which stores individual transaction response_time. The schema also has an aggregate_table (grain=day) which stores the response_time_sum per day.<br>
In my report I need to calculate standard deviations of the response time for a given timedimension, say day, week, month etc. How can I calculate the StandardDeviation using the aggregate_table instead of touching the huge fact_table?</p>
"
52,What is the PHP equivalent to these lines in R?,"<p>I'm trying to port a script from R into PHP but not sure what lines 3 and 4 (<a href=""http://stackoverflow.com/questions/8708048/position-of-the-sun-given-time-of-day-latitude-and-longitude/8764866#8764866"">taken from the larger function discussed here</a>) are doing. Looks like logical operations and array definition at the same time. Can someone please give me the equivalent in PHP?</p>

<pre><code>cosAzPos &lt;- (0 &lt;= sin(dec) - sin(el) * sin(lat))
sinAzNeg &lt;- (sin(az) &lt; 0)
az[cosAzPos &amp; sinAzNeg] &lt;- az[cosAzPos &amp; sinAzNeg] + twopi
az[!cosAzPos] &lt;- pi - az[!cosAzPos]
</code></pre>
"
53,Adding key legend to multi-histogram plot in R,"<p>How do I add a key legend to the below plot</p>

<p><img src=""http://i.stack.imgur.com/WWinE.jpg"" alt=""enter image description here""></p>

<p>I whish to have a key legend somewhere in the upper right corner with two short horizontal color bars, where the red one should say ""Plastic surgery gone wrong"" and the blue one should say ""Germany"".</p>

<p>I used the following code to produce the plot:</p>

<pre><code>bar2 &lt;- read.table(""div/ana-mut[...]/barriers-set-2.dat"", sep="" "")
bar2val &lt;- c(bar2$V1, bar2$V2)
bar3 &lt;- read.table(""div/ana-mut[...]/barriers-set-3.dat"", sep="" "")
bar3val &lt;- c(bar3$V1, bar3$V2)
p1 &lt;- hist(subset(bar2val, bar2val &lt; 30), breaks=30)
p2 &lt;- hist(subset(bar3val, bar3val &lt; 30), breaks=30)
plot(p1, col=rgb(1,0,0,8/9), main=""Barrier distribution"", xlab=""Barrier [kcal/mol]"", ylab=""Mutant count"")
plot(p2, col=rgb(0,0,1,8/9), add=T)
</code></pre>

<p>Any hints would be greatly appreciated.</p>
"
54,Neural network in MATLAB,"<p>I have trained xor neural network in Matlab and got these weights:</p>

<pre><code>iw: [-2.162 2.1706; 2.1565 -2.1688]

lw: [-3.9174 -3.9183]

b{1} [2.001; 2.0033]

b{2} [3.8093]
</code></pre>

<p>Just from curiosity I have tried to write MATLAB code which computes the output of this network (2 neurons in hidden layer, and 1 in output, TANSIG activation function). </p>

<p>Code that I got:</p>

<pre><code>l1w = [-2.162 2.1706; 2.1565 -2.1688];
l2w = [-3.9174 -3.9183];
b1w = [2.001 2.0033];
b2w = [3.8093];

input = [1, 0];

out1 = tansig (input(1)*l1w(1,1) + input(2)*l1w(1,2) + b1w(1));
out2 = tansig (input(1)*l1w(2,1) + input(2)*l1w(2,2) + b1w(2));
out3 = tansig (out1*l2w(1) + out2*l2w(2) + b2w(1))
</code></pre>

<p>The problem is when input is lets say [1,1], it outputs <strong>-0.9989</strong>, when [0,1] <strong>0.4902</strong>. While simulating network generated with MATLAB outputs adequately are <strong>0.00055875</strong> and <strong>0.99943</strong>. </p>

<p>What I'm doing wrong?</p>
"
55,What's the best trick to speed up a monte carlo simulation?,"<p>Whenever I run large scale monte carlo simulations in S-Plus, I always end up growing a beard while I wait for it to complete.  </p>

<p>What are the best tricks for running monte carlo simulations in R?  Any good examples of running processes in a distributed fashion?</p>
"
56,knitr - How to align code and plot side by side,"<p>is there a simple way (e.g., via a chunk option) to get a chunk's source code and the plot it produces side by side, as in <a href=""http://www.scribd.com/doc/93360631/Presentation"">http://www.scribd.com/doc/93360631/Presentation</a> (page 8)?</p>

<p>I tried using <code>out.width=""0.5\\textwidth"", fig.align='right'</code>, which makes the plot correctly occupy only half the page and be aligned to the right, but the source code is displayed on top of it, which is the normal behaviour.
I would like to have it on the left side of the plot.</p>

<p>Thanks</p>

<p>Sample code:</p>

<pre><code>&lt;&lt;someplot, out.width=""0.5\\textwidth"", fig.align='right'&gt;&gt;=
plot(1:10)
@
</code></pre>
"
57,Creating cx_Freeze exe with Numpy for Python,"<p>Im trying to create a basic exe using cx_Freeze. It works for .py programs that don't have numpy but I can't get one made correctly with numpy.   </p>

<p>*Any ideas on how to fix this? is there something i need to include in my setup.py?  </p>

<p>When I go to run the exe it says:       </p>

<pre><code>           c:\Python32\Scripts\dist&gt;Assignment4_5.exe
           Traceback (most recent call last):
     File ""C:\Python32\lib\site-packages\cx_Freeze\initscripts\Console3.py"", line 2
     7, in &lt;module&gt;
     exec(code, m.__dict__)
     File ""c:\Python32\Assignment4_5.py"", line 6, in &lt;module&gt;
     import numpy as np
     File ""C:\Python32\lib\site-packages\numpy\__init__.py"", line 137, in &lt;module&gt;
     from . import add_newdocs
     File ""C:\Python32\lib\site-packages\numpy\add_newdocs.py"", line 9, in &lt;module&gt;

     from numpy.lib import add_newdoc
     File ""C:\Python32\lib\site-packages\numpy\lib\__init__.py"", line 17, in &lt;modul
     e&gt;
    from .npyio import *
    File ""C:\Python32\lib\site-packages\numpy\lib\npyio.py"", line 6, in &lt;module&gt;
    from . import format
    ImportError: cannot import name format

   c:\Python32\Scripts\dist&gt;
</code></pre>

<p>Setup.py:</p>

<pre><code>   from cx_Freeze import setup, Executable

   includeDependencies = []

   setup(
        name = ""Assignment4_5PythonExe"",
        version = ""0.1"",
        description = ""Sort Methods"",
        executables = [Executable(""Assignment4_5.py"")]
        )
</code></pre>
"
58,non parametric vs parametric regression,"<p>Would appreciate your take on this. What is the essential difference between parametric and non parametric regression? Is it true that non parametric regression basically relies on local models or is a sophisticated form of a ""look-up"" table? </p>

<p>Thanks</p>
"
59,Calculating the square of BigInteger,"<p>I'm using .NET 4's <a href=""http://msdn.microsoft.com/en-us/library/dd268287.aspx"" rel=""nofollow"">System.Numerics.BigInteger structure</a>.</p>

<p>I need to calculate the square (x<sup>2</sup>) of very large numbers - <strong>millions of decimal digits</strong>.</p>

<p>If <code>x</code> is a <code>BigInteger</code>, What is the time complexity of:</p>

<pre><code>x*x;
</code></pre>

<p>or</p>

<pre><code>BigInteger.Pow(x,2);
</code></pre>

<p>?</p>

<p>How can multiply such big numbers in the fastest way using .NET 4 BigInteger? Is there an implementation for <a href=""http://en.wikipedia.org/wiki/Sch%C3%B6nhage%E2%80%93Strassen_algorithm"" rel=""nofollow"">SchnhageStrassen algorithm</a>?</p>
"
60,R - how to add cases of one variable to other variable (stack variables),"<p>var1 var2 var3 <br />
1    2    3 <br />
1     2    3 <br />
1     2    3 <br /></p>

<p>I want to stack var2 and var3 underneath var1 to get:</p>

<p>var1 <br />
1<br />
1<br />
1<br />
2<br />
2<br />
2<br />
3<br />
3<br />
3<br /></p>

<p>I tried: 
<code>data$var &lt;- append(data$var1,data$var2)</code></p>

<p>Then I get an error that my replacement has more rows. How do I solve this?</p>

<p>Thanks</p>
"
61,Adding up prices in foreach loop (PHP) - Not adding up,"<p>... It's not as silly as it sounds...</p>

<p>I have the following code, which is used by my ajax table script to display database stuff on the page in a table.</p>

<pre><code>foreach($ct-&gt;data as $key =&gt; $value){
    $ct-&gt;data[$key][2]='&lt;a href=""quantity.php?partno='.$ct-&gt;data[$key][0].'&amp;description='.$ct-&gt;data[$key][1].'&amp;quantity='.$ct-&gt;data[$key][2].'&amp;order='.$o.'""&gt;'.$ct-&gt;data[$key][2].'&lt;/a&gt;';
    $ct-&gt;data[$key][3]='&lt;a href=""quantity.php?partno='.$ct-&gt;data[$key][0].'&amp;description='.$ct-&gt;data[$key][1].'&amp;price='.$ct-&gt;data[$key][3].'&amp;order='.$o.'""&gt;'.$ct-&gt;data[$key][3].'&lt;/a&gt;';

    if($ct-&gt;data[$key][4] == """" || $ct-&gt;data[$key][4] == null)
        $ct-&gt;data[$key][4]='&lt;a href=""freight.php?partno='.$ct-&gt;data[$key][0].'&amp;description='.$ct-&gt;data[$key][1].'&amp;freight='.$ct-&gt;data[$key][4].'&amp;order='.$o.'""&gt;Edit Charge.&lt;/a&gt;';
    else
        $ct-&gt;data[$key][4]='&lt;a href=""freight.php?partno='.$ct-&gt;data[$key][0].'&amp;description='.$ct-&gt;data[$key][1].'&amp;freight='.$ct-&gt;data[$key][4].'&amp;order='.$o.'""&gt;'.$ct-&gt;data[$key][4].'&lt;/a&gt;';

    $Total =$Total+ $ct-&gt;data[$key][3];
    $freight =$freight+ $ct-&gt;data[$key][4];
}
</code></pre>

<p>And as you can see, in the foreach loop, i am trying to add up the contents of 2 columns.</p>

<p>The $Total column or, $ct->data[$key][3] lists the Prices for each row of products, and the $freight column does the same for each row of Freight charges.</p>

<p>And inside the foreach loop, I am trying to add together the total amount of prices. And Freight charges.</p>

<p>I'm not sure if i'm doing it the right way, because when I check the database, it just adds '0' (without the quotes). So it's not adding up!</p>

<p>For example, if there are a total of 3 rows in the table, and each product is 1 (dollar), it should add up to 3, right? And same goes for the $freight ones.</p>

<p>Can someone please tell me what I'm doing wrong here?</p>

<p>Thank you!</p>
"
62,"Using base X, how high can I count using Y characters?","<p>I know that the total number of permutations for a given base is the factorial... so the total number of permutations of ""abc"" is <code>3!</code> or <code>3x2x1</code> or <code>6</code>. </p>

<p>Obviously I'm not sure of the terminology to properly phrase my question, but I would like to find the highest numbered permutation before the ""length"" of it's representation increases to X characters. </p>

<p>For example, Using a Base 62 'alphabet', I can represent integers up to 238327 before the representation uses 4 characters instead of 3. I'd like to know the math behind finding this out, given arbitrary values for Base and Length of representation. </p>

<p>Essentially, ""using Base-X, how high can I count using Y characters?"". </p>
"
63,"Calculate resulting RGB from 2 colors, one is transparent","<p>I'm looking for a formula to convert them.</p>

<p>I know to convert a general transparency it is </p>

<p>alpha * new + ( 1 - alpha ) * old</p>

<p>I have:</p>

<pre><code>Color A : RGB( 85, 113, 135 )
Color B : RGB( 43, 169, 225 )
</code></pre>

<p>Color A has 90% opacity and is laid on top of Color B, resulting in</p>

<pre><code>Color C : RGB( 65, 119, 145 )
</code></pre>

<p>My question is, how does it get Color C? If I substitute Color B for another thing, how do I get Color C?</p>

<p>Here's another example, same base color:</p>

<pre><code>Color A : RGB( 85, 113, 135 )
Color B : RGB( 45, 67, 82 )
--------
Color C : RGB( 65, 109, 131 )
</code></pre>

<p>Those are working examples done with images -- I'm trying to now calculate the remaining Color C so I can assign a background color.</p>

<hr>

<p>UPDATE, please see the accepted answer. The <code>red</code> in the above examples is strange -- the accepted answer has the correct formula for all the colors, I tested it in Photoshop.</p>
"
64,Math in MYSQL/PHP,"<p>I'm a complete novice but I really need to get this done as soon as possible so any help would be greatly appreciated.</p>

<p>I have a table with the following fields:
<strong>userid</strong>, <strong>upvoteds</strong> , <strong>downvoteds</strong>, <strong>aposts</strong> and <strong>aselecteds</strong></p>

<p>Basically I need to run this formula for every row:</p>

<p>(<strong>upvoteds</strong> - <strong>downvoteds</strong>) / (<strong>aposts</strong> - <strong>aselecteds</strong>)
Apologies if that formula isn't structured properly.</p>

<p>I need the results presented in desc order along with <strong>userid</strong> so I know who it belongs to.</p>

<p>Thanks in advance!</p>
"
65,order rows in a matrix based on the index of most similar row in another matrix,"<p>I am trying to ""unshuffle"" the rows of a matrix containing the centroids of some clusters which are not in the same order as the order in which the samples were assigned to the clusters. Initially I was comparing the absolute value of the distance between the data points of the mean and the cluster centers and assign the index of the row which had the smallest distance. Of course, I am not allowed to have duplicate indexes. It worked pretty good but the symmetric values raise a problem (i.e., due to the absolute value for the distance, mirror clusters were not ordered properly). Also I tried to order them based on the variance, did not work as expected. I have been looking at the order() and sort() function and found an example which did not work.</p>

<pre><code>order(mean)        
order(mean)[centers]       
sort(order(mean)[centers]) 
mean[sort(order(mean)[centers])]
</code></pre>

<p>I also tried the </p>

<pre><code>apply(mean==centers,1,all)
</code></pre>

<p>but of course that just returns FALSE everywhere.</p>

<p>A sample of the matrices:</p>

<pre><code>means &lt;- c(0.055190097, 0.032412395,    0.015372307,    -0.008012372,
-0.018736792,   -0.078138715, -0.058707713,   -0.044020629,
-0.023750329,   -0.014402083, -0.069920581,   -0.064429216,
-0.059913345,   -0.052302253,   -0.047874074,  0.050557395,
0.047246979,    0.044577065,    0.040384336,    0.038140009,
0.114954601,    0.108110051,    0.102531680,    0.093341425,    0.088140310)
dim(means) &lt;- c(5,5)
means &lt;- t(means)


centers &lt;- c(-0.038754, -0.021588,-0.008851,    0.008579,   0.016579,
 0.018371,   0.006095,   -0.003026,  -0.015537, -0.021286,
-0.078143,  -0.069267,  -0.062197,   -0.051295,  -0.045521,
 0.033145,   0.033348,   0.033354,   0.032947,   0.032511,
 0.115464,   0.105248,   0.097172,   0.084732,   0.078162)
dim(centers) &lt;- c(5,5)
centers &lt;- t(centers)
</code></pre>

<p>For instance (with the above example), line 2 from the <strong>means</strong> matrix corresponds to line 3 from the <strong>centers</strong> matrix as it is the closest in distance (data point wise). So, I have to find which line from the <strong>means</strong> corresponds to which line in <strong>centers</strong> (no duplicates). 
My matrices are bigger, but this should be enough as example
Do you have any suggestions? 
Thank you</p>
"
66,Force mapply to return a list?,"<p>Suppose I have a function that creates data frames.  I'd like to run that function with different input values, and then rbind the results together into one big data frame, as below:</p>

<pre><code>CreateDataFrame &lt;- function(type=""A"", n=10, n.true=8) {
  data.frame(success=c(rep(TRUE, n.true), rep(FALSE, n - n.true)), type=type)
}
df &lt;- do.call(rbind, lapply(toupper(letters[1:5]), CreateDataFrame))
</code></pre>

<p>My CreateDataFrame function takes three arguments.  In the example above, the second and third arguments are held constant.  I'd like to do the same as above, but have the second and third arguments change on each call.  I think I have to use mapply, like this:</p>

<pre><code>mapply(""CreateDataFrame"", type=toupper(letters[1:5]), n=10, n.true=8:4)
</code></pre>

<p>I'm having trouble because mapply isn't returning a list, which prevents me from running <code>do.call(rbind, mapply(...))</code>.  How can I end up with a single data frame, as I did in the example at the top?</p>

<p>Looks like mapply is returning a matrix of lists.  I was expecting it to return a list of data frames.  What should I do differently?</p>
"
67,Mathematical Statistics,"<p>How do I find the answers to this question?</p>

<p>State Techs basketball team, the Fighting Loga-
rithms, have a 70% foul-shooting percentage.
(a) Write a formula for the exact probability that out of
their next one hundred free throws, they will make
between seventy-ve and eighty, inclusive.
(b) Approximate the probability asked for in part (a).</p>
"
68,Subsetting a vector with a logical condition,"<pre><code>&gt; x1=c(4,5,6,7,8)
&gt; x1
[1] 4 5 6 7 8
&gt; x2=x1[x1!=6]
&gt; x2
[1] 4 5 7 8
&gt; x3=x1[x1=6]
&gt; x3
[1] NA
</code></pre>

<p>Why x3 is not 6? i don't understand.</p>
"
69,"In R, how do you get the best fitting equation to a set of data?","<p>I'm not sure wether R can do this (I assume it can, but maybe that's just because I tend to assume that R can do anything :-)). What I need is to find the best fitting equation to describe a dataset.</p>

<p>For example, if you have these points:</p>

<p><code>df = data.frame(x = c(1, 5, 10, 25, 50, 100), y = c(100, 75, 50, 40, 30, 25))</code></p>

<p>How do you get the best fitting equation? I know that you can get the best fitting curve with:</p>

<pre><code>plot(loess(df$y ~ df$x))
</code></pre>

<p>But as I understood you can't extract the equation, see <a href=""http://stackoverflow.com/questions/1785118/loess-fit-and-resulting-equation"">Loess Fit and Resulting Equation</a>.</p>

<p>When I try to build it myself (note, I'm not a mathematician, so this is probably not the ideal approach :-)), I end up with smth like:</p>

<pre><code>y.predicted = 12.71 + ( 95 / (( (1 + df$x) ^ .5 ) / 1.3))
</code></pre>

<p>Which kind of seems to approximate it - but I can't help to think that smth more elegant probably exists :-)</p>

<p>I have the feeling that fitting a linear or polynomial model also wouldn't work, because the formula seems different from what those models generally use (i.e. this one seems to need divisions, powers, etc). For example, the approach in <a href=""http://stackoverflow.com/questions/3822535/fitting-polynomial-model-to-data-in-r"">Fitting polynomial model to data in R</a> gives pretty bad approximations.</p>

<p>I remember from a long time ago that there exist languages (Matlab may be one of them?) that do this kind of stuff. Can R do this as well, or am I just at the wrong place?</p>

<p>(Background info: basically, what we need to do is find an equation for determining numbers in the second column based on the numbers in the first column; but we decide the numbers ourselves. We have an idea of how we want the curve to look like, but we can adjust these numbers to an equation if we get a better fit. It's about the pricing for a product (a cheaper alternative to current expensive software for qualitative data analysis); the more 'project credits' you buy, the cheaper it should become. Rather than forcing people to buy a given number (i.e. 5 or 10 or 25), it would be nicer to have a formula so people can buy exactly what they need - but of course this requires a formula. We have an idea for some prices we think are ok, but now we need to translate this into an equation.</p>
"
70,"Mathematical formulas for easing functions (ElasticEase, CircleEase, BounceEase, BackEase, PowerEase)?","<p>I can figure out the formulas for all polynomial functions, 
like e.g. the formula for QuinticEase is:</p>

<blockquote>
  <p>(x - 1) ^ 5 + 1</p>
</blockquote>

<p>But what are the mathematical formulas for ElasticEase, CircleEase, BounceEase, BackEase, or PowerEase?</p>

<p>They should all be in the range 0..1</p>
"
71,color the cluster output in r,"<p>I have a set of cluster output. I want to show each cluster with unique color in parallel coordinate graph. I am using rggobi for parallel coordinate graph. I used this link
<a href=""http://www.ggobi.org/docs/parallel-coordinates/"" rel=""nofollow"">http://www.ggobi.org/docs/parallel-coordinates/</a></p>

<p>here is my code to load the data to ggobi</p>

<pre><code>library(rggobi)
mydata &lt;- read.table(""E:/Thesis/Experiments/R/input.cvs"",header = TRUE,sep = "","")
 g &lt;- ggobi(mydata)
</code></pre>

<p>here is my output
<img src=""http://i.stack.imgur.com/XnvIs.png"" alt=""parallel coordinate""></p>

<p>I want to use different color to represent different clusters.</p>
"
72,How to obtain all ensemble estimates in RandomForestRegressor (scikit-learn),"<p>I'm trying to fit a random forest regression and I'd like to obtain a distribution of my estimate by looking at the output of every regression tree in the ensemble, returned to me in some sort of list.  In R, providing the <code>predict.all</code> option in the <code>randomForest</code> predict method does this for me,  Is there any way to do this in the <code>RandomForestRegressor</code> class in sklearn?</p>

<p>In 0.13 and above, I see the method apply which returns leaf indices, however I'm not sure how to use these, and I don't see any more clues in the doc for <code>RandomForestRegressor</code>.</p>

<p>Thanks for you help.</p>
"
73,Where can I find simple beta cdf implementation,"<p>I need to use <a href=""http://en.wikipedia.org/wiki/Beta_distribution"" rel=""nofollow"">beta distribution</a> and inverse beta distribution in my project. </p>

<p>There is quite good but complicated implementation <a href=""http://www.gnu.org/software/gsl/manual/html_node/The-Beta-Distribution.html"" rel=""nofollow"">in GSL</a>, but I don't want to use such a big library only to get one function.</p>

<p>I would like to either, implement it on my own or link some simple library. Do you know any sources that could help me? I'm looking for any books/articles about numerical approximation of beta CDF <strong>and its inverse</strong>, libraries where it could be implemented. Any other suggestions would be also appreciated.<br>
Any programming language, but C++/C# preffered.</p>
"
74,R: how to subset list based on condition?,"<p>How can I subset a list based on a condition (TRUE, FALSE) in another list? Please, see my example below</p>

<pre><code>&gt; (l &lt;- list(a=c(1,2,3), b=c(4,5,6,5), c=c(3,4,5,6)))
$a
[1] 1 2 3

$b
[1] 4 5 6 5

$c
[1] 3 4 5 6

&gt; (cond &lt;- lapply(l, function(x) length(x) &gt; 3))
$a
[1] FALSE

$b
[1] TRUE

$c
[1] TRUE

&gt; l[cond]
Error in l[cond] : invalid subscript type 'list'
</code></pre>
"
75,Determine the centroid of multiple points,"<p>I'm writing a mapping application that I am writing in python and I need to get the lat/lon centroid of N points.
Say I have two locations</p>

<pre><code>a.lat = 101
a.lon = 230

b.lat = 146
b.lon = 200
</code></pre>

<p>Getting the center of two points is fairly easy using a euclidean formula. I would like 
to be able to do it for more then two points.</p>

<p>Fundamentally I'm looking to do something like <a href=""http://a.placebetween.us/"" rel=""nofollow"">http://a.placebetween.us/</a> where one can enter multiple addresses and find a the spot that is equidistant for everyone.</p>
"
76,Efficient way of resolving unknown words to known words?,"<p>I am designing a text processing program that will generate a list of keywords from a long itemized text document, and combine entries for words that are similar in meaning. There are metrics out there, however I have a new issue of dealing with words that are not in the dictionary that I am using. </p>

<p>I am currently using nltk and python, but my issues here are of a much more abstracted nature. Given a word that is not in a dictionary, what would be an efficient way of resolving it to a word that is within your dictionary? My only current solution involves running through the words in the dictionary and picking the word with the shortest Levenshtein distance (editing distance) from the inputted word.</p>

<p>Obviously this is a very slow and impractical method, and I don't actually need the absolute best match from within the dictionary, just so long as it is a contained word and it is pretty close. Efficiency is more important for me in the solution, but a basic level of accuracy would also be needed. </p>

<p>Any ideas on how to generally resolve some unknown word to a known one in a dictionary? </p>
"
77,condition on non-commutative Khinchine inequality,"<p>Let $\epsilon=(\epsilon_1,\ldots \epsilon_M)$ be Rademacher sequence. And let $B_j, j=1, \ldots, M$ be complex valued random matrices of the same dimension. Choose $n\in \mathbb{N}$. Then, the non-commutative Khinchine inequality states that
$$
E(\|\sum_{j=1}^M\epsilon_jB_j\|^{2n}_{S_{2n}})\leq \frac{(2n)!}{2^nn!}\max\{\|(\sum_{j=1}^MB_jB^*_j)^{1/2}\|^{2n}_{S_{2n}}, \|(\sum_{j=1}^MB^*_jB_j)^{1/2}\|^{2n}_{S_{2n}}\},
$$
where $\|\cdot\|_{S_{2n}}$ is a <a href=""http://en.wikipedia.org/wiki/Schatten_norm"" rel=""nofollow"">Schatten norm</a>, and $B_jB^*_j$ together with $B_j^*B_j$ are positive self-adjoint matrices.</p>

<p>I am curious, if this is a critical condition for Rademacher random variables $\epsilon_j, j=1, \ldots, M$ been independent. I mean, would it make science if one can consider some dependence between $\epsilon_j$, say sum of them is equal to 1.</p>

<p>Thank you.</p>
"
78,Convert list in tuple to numpy array?,"<p>I have tuple of lists. One of these lists is a list of scores. I want to convert the list of scores to a numpy array to take advantage of the pre-built stats that scipy provides.</p>

<p>In this case the tuple is called 'data'</p>

<pre><code>In [12]: type data[2]
-------&gt; type(data[2])
Out[12]: &lt;type 'list'&gt;

In [13]: type data[2][1]
-------&gt; type(data[2][1])
Out[13]: &lt;type 'list'&gt;

In [14]: type data[2][1][1]
-------&gt; type(data[2][1][1])
Out[14]: &lt;type 'float'&gt;

In [15]: print data[2][1]
-------&gt; print(data[2][1])
[16.66, 16.66, 16.66, 16.66, 5.5599999999999996, 16.699999999999999]

In [16]: print data[2][1][1]
-------&gt; print(data[2][1][1])
16.66
</code></pre>

<p>Can I do this easily once I have stored the tuple? </p>
"
79,how do i convert speed and bearing to azimuth and elevation in c#?,"<p>i have no clue to it. can someone help me out?</p>
"
80,Smoothing of data with unequal number of observations for plotting?,"<p>I have two data-frame with unequal number of rows.  But i need to smooth the data in both the data frames and plot them together.  I can smooth each dataframe with lowess/loess.  However, when i try to plot the lines for both the data-frames together, i usually get error ""unequal number of rows"".  I found a way around this by using <code>spline</code>.  I want to know if the following would be valid:</p>

<pre><code>tmp1 &lt;- spline( lowess( df1[,1], df[,2] ), n = 20 )
tmp2 &lt;- spline( lowess( df2[,1], df2[,2] ), n = 20 )

plot( tmp1[,1], tmp1[,2], type=""l"" )
lines( tmp2[,1], tmp2[,2], col=""red"" )
</code></pre>

<p>I want to know whether it is ""statistically"" valid to plot spline of a <code>lowess</code> object its its representation, because I want to limit number of data-points.  This is specifically for case where the <code>lowess</code> on to different series contain unequal number of points?</p>
"
81,How do I create a column containing aggregated means with R?,"<p>In R, I have a bunch of data in a dataframe like:</p>

<pre><code>state | zip   | value
______|_______|______
CA    | 94555 | 18
CA    | 94556 | 5
OH    | 12345 | 22
OH    | 12346 | 10
</code></pre>

<p>and so on.</p>

<p>I want an add a column to each row listing the mean 'value' for that state.</p>

<p>I can get a dataframe of the means via ""<code>(aggregate(data$value, list(State = data$state), mean))</code>"". That gives me a dataframe with 50 rows, one for each state. But I need to then go back into the original dataframe and put the state's average in rows belonging to that state.</p>

<p>How would I go about doing this?</p>
"
82,Need recommendation on SP statistical tool,"<p>looking for recommendation on SP Stats tool. I found following but I know for sure you guys seen this one and others and Looking for your feedback.</p>

<p>spstats ?
<a href=""http://www.avepoint.com/sharepoint-reporting-docave/?gclid=CP6m6tO9iawCFcTD7QodPH1Znw"" rel=""nofollow"">http://www.avepoint.com/sharepoint-reporting-docave/?gclid=CP6m6tO9iawCFcTD7QodPH1Znw</a> ?</p>

<p>who else?</p>
"
83,Concatenate lists with same .names,"<p>I have lists v1 and v2 with the same names:</p>

<pre><code>v1: structure(list(ID = c(""A1""), Name = c(""A2""),.Names = c(""ID"", ""Name"") 
    ...
v2: structure(list(ID = c(""B1""), Name = c(""B2""),.Names = c(""ID"", ""Name"") 
</code></pre>

<p>I want to concatenate the lists, while keeping the names, i.e. to get something like:</p>

<pre><code>v12:  structure(list(ID = c(""A1"",""B1""), Name = c(""A2"",""B2""), 
.Names = c(""ID"", ""Name"")
</code></pre>

<p>Manual concatenation works:</p>

<pre><code>v12&lt;-cbind(Map(c, v1, v2))
</code></pre>

<p>But, if v1 and v2 are results of applying lapply(), and are stored in a list themselves, the similar logic does not seem to work:</p>

<pre><code>v&lt;-lapply(...)
v12&lt;-cbind(Map(c,v))
</code></pre>

<p>What is the best way to automate the process? For example:</p>

<pre><code>v1 &lt;- structure(list(ID = c(""A1""), Name = c(""A2"")),.Names = c(""ID"", ""Name""))             
v2 &lt;-  structure(list(ID = c(""B1""), Name = c(""B2"")),.Names = c(""ID"", ""Name""))
v &lt;- list(v1, v2)
k&lt;-t(mapply(c, v))
</code></pre>

<p>results in:</p>

<pre><code>ID  Name
A1  A2
B1  B2
</code></pre>

<p>not in:</p>

<pre><code>  ID    Name
""A1"",""B1""   ""A2"",""B2""
</code></pre>
"
84,Redefine class of an object in R,"<p>My object has the class ""character"". g.g.</p>

<pre><code>x &lt;- rep(TRUE,4)
x &lt;- replace(x,3,FALSE)
</code></pre>

<p>I now would like to coerce/transform this into a being a <em>logical</em> class. How do I change the class of x? </p>
"
85,Neural nets as universal approximators,"<p>The formal statement of <a href=""http://en.wikipedia.org/wiki/Universal_approximation_theorem"" rel=""nofollow"">universal approximation theorem</a> states that neural nets with single hidden layer can approximate any function which is continuous on m-dimensional unit hypercube.
But how about functions which are not continuous, is anything known about whether they can be always approximated by neural nets?</p>

<p>For example take a function which calculates n'th digit of number pi. 
If I train some single hidden layer neural net on this data: (n, n'th digit of pi), will it be eventually able to return correct values for unseen n's?
How about multiple hidden layers neural nets?</p>
"
86,Quantify a change of direction given an array of 3d points,"<p>I am working on a piece of software written in Java that uses some processing.core library classes and simpleopenni to track a user's hand with the XBOX Kinect.</p>

<p>What I am trying to figure out is how to determine when a user's hand movement changes direction abruptly. </p>

<p>What I have at my disposal currently is an array of PVectors (essentially a vector of x,y,and z coordinates: <strong>A point in 3d space</strong>) that record the user's hand position for the past 30 frames or so.</p>

<p>I imagine that there must be a way to obtain a value that represents the amount of directional change in nearly real-time given the most recent few points recorded. Maybe fit a curve and take some derivatives?</p>

<p>Ideally the solution should not be very computationally expensive, as I am trying to implement a real-time worthy solution.</p>

<p>Any direction that you can offer will be greatly appreciated!</p>
"
87,Why is length of a group variable always 1 in data.table grouping?,"<p>I know this can be achieve with other packages, but I'm trying to do it in data.table (as it seems to be the fastest for grouping).</p>

<pre><code>library(data.table)
dt = data.table(a=c(1,2,2,3))
dt[,length(a),by=a]
</code></pre>

<p>results in</p>

<pre><code>   a V1
1: 1  1
2: 2  1
3: 3  1
</code></pre>

<p>whereas</p>

<pre><code>df = data.frame(a=c(1,2,2,3))
ddply(df,.(a),summarise,V1=length(a))
</code></pre>

<p>produces</p>

<pre><code>  a V1
1 1  1
2 2  2
3 3  1
</code></pre>

<p>which is a more sensible results. Just wondering why data.table is not giving the same results, and how this can be achieved.</p>

<p>thanks</p>
"
88,Conditional row removal based on number of NA's within the row,"<p>I am looking to remove rows from my dataset based on 2 conditions as follows:</p>

<ol>
<li>remove row if 3 consecutive cells are ""NA""
or</li>
<li>if 4 or more cells are ""NA"" </li>
</ol>

<p>My sample data: </p>

<pre><code>data &lt;- rbind(c(1,1,2,3,4,2,3,2),
              c(NA,1, NA, 4,1,1,NA,2), 
              c(1,4,6,7,3,1,2,2), 
              c(NA,3, NA, 1,NA,2,NA,NA), 
              c(1,4, NA, NA,NA,4,3,2))
</code></pre>

<p>I have researched within the existing questions and found that na.omit or complete.cases can remove rows with NA but as I have conditions, doing further research I have found the following code within the existing questions:</p>

<pre><code>data[! rowSums(is.na(data)) &gt;4  , ]   
data[! rowSums(is.na(data)) ==3  , ]
</code></pre>

<p>The first line full fill my second condition. the second line does remove rows with 3 NA's but not looking for consecutive and removing any rows with total 3 NA's. for example:</p>

<pre><code>&gt; data
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
[1,]    1    1    2    3    4    2    3    2
[2,]   NA    1   NA    4    1    1   NA    2
[3,]    1    4    6    7    3    1    2    2
[4,]   NA    3   NA    1   NA    2   NA   NA
[5,]    1    4   NA   NA   NA    4    3    2

&gt; data[! rowSums(is.na(data)) ==3  , ]
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
[1,]    1    1    2    3    4    2    3    2
[2,]    1    4    6    7    3    1    2    2
[3,]   NA    3   NA    1   NA    2   NA   NA
</code></pre>

<p>What I actually want is the 5th row to be removed only as this has 3 consecutive NA's and not the 2nd row.</p>

<p>Could anyone please advice me how can I overcome this?   </p>
"
89,Calculating a point on an ellipse,"<p>Say I have two points A and B positioned on the circumference of an ellipse, and they form an angle X from the center point.  Now say that point A is moved to a new point C.  How can I calculate the new point for B such that the angle X remains constant?  Pointers to code of some kind would be appreciated.</p>
"
90,How to drop decimal precision when writing a zoo object to csv file with R?,"<p>When writing out a zoo object to a CSV file using write.zoo(), I would actually like to drop decimal precision from the default of 14 to just 3. However, even with setting scipen &amp; digits in options(), I have not been able to drop the precision in output.</p>

<p>Here is a sample code to illustrate the issue.</p>

<pre><code>blah &lt;- zoo(cbind(c(1.590833333333335, NA), c(NA, 21.590833333333337)))
index(blah) &lt;- c(""Dec 1985"", ""Dec 1986"")
colnames(blah) &lt;- c(""FooHeader"", ""BarHeader"")
options(scipen = 3, digits = 3)
write.zoo(blah, file = ""blah.csv"", sep = "","")
</code></pre>

<p>If I open the <code>blah.csv</code> file, I see</p>

<pre><code>""Index"",""FooHeader"",""BarHeader""
""Dec 1985"",1.59083333333334,NA
""Dec 1986"",NA,21.5908333333333
</code></pre>

<p>But what I would really want to see is</p>

<pre><code>""Index"",""FooHeader"",""BarHeader""
""Dec 1985"",1.591,NA
""Dec 1986"",NA,21.591
</code></pre>

<p>How do I go about making this happen? Thanks a bunch in advance!</p>

<p>Note: I am aware that by dropping the precision, if I read the data into the R again, I would still lose the precision. That's fine. I can live with that.</p>
"
91,Pythonic way to populate numpy array,"<p>I find myself parsing lots of data files (usually in a .csv file or similar) using the csv reader and a for loop to iterate over every line. The data is usually a table of floats so for example.</p>

<pre><code>reader = csv.reader(open('somefile.csv'))
header = reader.next()

res_list = [list() for i in header]    

for line in reader:
  for i in range(len(line)):
    res_list[i].append(float(line[i]))

result_dict = dict(zip(header,res_list)) #so we can refer by column title
</code></pre>

<p>This is a ok way to populate so I get each column as a separate list however, I would prefer that the default data container for lists of items (and nested lists) be numpy arrays, since 99 times out 100 the numbers get pumped into various processing scripts/functions and having the power of numpy lists makes my life easier. </p>

<p>The numpy <code>append(arr, item)</code> doesn't append in-place and therefore would require re-creating arrays for every point in the table (which is slow and unneccesary). I could also iterate over the list of data-columns and wrap them into an array after I'm done (which is what I've been doing), but sometimes it isn't so clear cut about <em>when</em> I'm done parsing the file and may need to append stuff to the list later down the line anyway.</p>

<p>I was wondering if there is some less-boiler-heavy way (to use the overused phrase ""pythonic"") to process tables of data in a similar way, or to populate arrays (where the underlying container is a list) dynamically and without copying arrays all the time.</p>

<p>(On another note: its kind of annoying that in general people use columns to organize data but <code>csv</code> reads in rows if the reader incorporated a read_column argument (yes, I know it wouldn't be super efficient), I think many people would avoid having boiler plate code like the above to parse a csv data file. )</p>
"
92,How do i make a Histogram spanning multiple files?,"<p>i want to make a histogram spanning multiple files in a Folder. 
Example:</p>

<p>File 1:  </p>

<pre><code>Alpha
Beta 
Ceta  
Delta
</code></pre>

<p>File 2:</p>

<pre><code>Delta 
Ceta 
Alpha
</code></pre>

<p>File 3: </p>

<pre><code>Beta 
Delta
</code></pre>

<p>I know that i can create a histogram using Numpy with: 
<code>axHistx = plt.axes(range)</code></p>

<p>How can i use this to create a histogram over multiple files, that gives me the absolute number of Occurrences of the Strings?</p>
"
93,R skips lines from /dev/stdin,"<p>I have a file with a list of numbers (make it for yourself: <code>for x in $(seq 10000); do echo $x; done &gt; file</code>).</p>

<pre><code>$&gt; R -q -e ""x &lt;- read.csv('file', header=F); summary(x);""

&gt; x &lt;- read.csv('file', header=F); summary(x);
       V1       
 Min.   :    1  
 1st Qu.: 2501  
 Median : 5000  
 Mean   : 5000  
 3rd Qu.: 7500  
 Max.   :10000  
</code></pre>

<p>Now, one might expect <code>cat</code>ing the file and reading from <code>/dev/stdin</code> to have the same output, but it does not:</p>

<pre><code>$&gt; cat file | R -q -e ""x &lt;- read.csv('/dev/stdin', header=F); summary(x);""
&gt; x &lt;- read.csv('/dev/stdin', header=F); summary(x);
       V1       
 Min.   :    1  
 1st Qu.: 3281  
 Median : 5520  
 Mean   : 5520  
 3rd Qu.: 7760  
 Max.   :10000 
</code></pre>

<p>Using <code>table(x)</code> shows that a bunch of lines were skipped:</p>

<pre><code>    1  1042  1043  1044  1045  1046  1047  1048  1049  1050  1051  1052  1053 
    1     1     1     1     1     1     1     1     1     1     1     1     1 
 1054  1055  1056  1057  1058  1059  1060  1061  1062  1063  1064  1065  1066 
    1     1     1     1     1     1     1     1     1     1     1     1     1
 ...
</code></pre>

<p>It looks like R is doing something funny with <code>stdin</code>, as this Python will properly print all the lines in the file:</p>

<pre><code>cat file | python -c 'with open(""/dev/stdin"") as f: print f.read()'
</code></pre>

<hr>

<p><a href=""http://stackoverflow.com/questions/8568968/r-programming-read-csv-skips-lines-unexpectedly"">This question</a> seems related, but it is more about skipping lines in a malformed CSV file, whereas my input is just a list of numbers.</p>
"
94,Order Statistic Expectation / Probability,"<p>Let $Y_1&lt;Y_2$ be order statistics from a random sample of size $2$ from a normal distribution, $\mathcal{N}(\mu,\sigma^2)$, where $\sigma^2$ is known. Show that $P(Y_1&lt;\mu&lt;Y_2)=\frac12$ and find $E(Y_1-Y_2)$. </p>

<p>I am not exactly sure how to solve the question above. Any help would be appreciated. Thanks.</p>
"
95,R grep for a-Z except e-,"<p>I have a <code>data.frame</code> with let's say 5 columns and 30 rows. I iterate over each column and use the <code>grep</code> function to identify rows that are not numeric by looking for letters a-Z. I did quite some tests and it seemed to work fine. However, I just had a case where a number was 0.0000000009 which was translated by R to 9e-10. This row/number was identified by grep as being not numeric even though it is, apparently, a number. My question is now, how can I grep for letters a-Z but exclude the pattern ""e-""? Here's my R code:</p>

<pre><code>for(i in 1:ncol(m)) {

    if(length(grep(""[a-zA-Z]"", m[,1])) &gt; 0) { # do something...}

}
</code></pre>
"
96,"JQuery, load, parse, sum and display from external page","<p>I'm trying to load an external page which contains 1 table like this:</p>

<pre><code>&lt;table id=""uniquedatatable""&gt;
 &lt;tr class=""row""&gt; 
  &lt;td class=""num col""&gt;&lt;a&gt; 1 &lt;/a&gt;&lt;/td&gt;
  &lt;td class=""str col""&gt;&lt;a&gt;Text&lt;/a&gt;&lt;/td&gt;
  &lt;td class=""num col""&gt;&lt;a&gt;  7&lt;/a&gt;&lt;/td&gt;
  &lt;td class=""str col""&gt;&lt;a&gt;Text&lt;/a&gt;&lt;/td&gt;
 &lt;/tr&gt;
&lt;/table&gt;
</code></pre>

<p>*Notice the whitespace's in the class names of the td's and around the numbers (1,7).</p>

<p>I need to find all cell's (td's) with class ""num col"", get the numeric value between the a tag'a and sum all the values. Last I need to display this value on my own page.</p>

<p>So the result I would get from the above table should be: 1 + 7 = 8.</p>

<p>I've gotten this far:</p>

<pre><code> &lt;script src=""http://code.jquery.com/jquery-1.5.1.min.js"" type=""text/javascript""&gt;&lt;/script&gt;

 &lt;p id=""result""&gt;&lt;/p&gt;

 &lt;script type=""text/javascript""&gt;
   var url = ""http://someurl"";
   var sum = 0;

   $('#result').load('url #uniquedatatable tr td.num col a', function() {
     sum += Number($(this).text());
   });

   _gel('result').innerHTML = sum;
 &lt;/script&gt;
</code></pre>

<p>Realizing something is wrong (close to being correct I hope :)), I need help connecting the ""dots"" here so that I can:</p>

<ol>
<li><p>Calculate the sum (this is the ""big"" problem :S)</p></li>
<li><p>Write the result back. </p></li>
</ol>

<p>Any help is appreciated :).</p>

<p>Kind regards</p>
"
97,"Given enough DNA samples, would it be possible to reconstruct the entire genealogy tree of humanity?","<p>DNA samples of live individuals. </p>

<p>It would be more of a mesh/graph than a tree but you get what I mean. I guess we could only have access to potential graphs with varying degrees of probability. In this case, what would be the figures like? What would be the probability of getting the nth level right? Also, we don't care about dead branches (eg. your childless great uncle)</p>

<p>To be honest I don't know if I'm posting this in the right StackExchange.</p>
"
98,Need code for extracting useable data from IMDB text files with R,"<p>Does anyone have any code handy that extracts data from the downloadable imdb text files and transposes them into a more useable format? The text files are not in an immediately useable format and are somewhat problematic to convert. For example, the business.list file looks like this for each movie, a sort of multiple tags (different tags and number of each tag) for each moive (some with less tags, some with more tags). </p>

<p>I am looking for a way to change these files into a data frame.</p>

<hr>

<p>MV: The Clearing (2004)</p>

<p>GR: USD 5,763,875 (USA) (10 October 2004) 
GR: USD 5,761,124 (USA) (3 October 2004) 
</p>

<p>OW: USD 618,674 (USA) (4 July 2004) (56 screens) 
</p>

<p>SD: 23 September 2002 - November 2002 </p>

<p>WG: USD 1,350 (USA) (10 October 2004) (2 screens) 
</p>

<hr>

<p>More information is here: <a href=""http://www.imdb.com/interfaces"" rel=""nofollow"">http://www.imdb.com/interfaces</a></p>

<p>In the mean time, I am going to poke around with some of the other interfaces such as the text unix one.</p>
"
99,Is there a way to use SVM to build a model that contains a pre-defined percentage of the training data?,"<p>Say, I wish to use LIBSVM to build a model that contains 70% of the training data. Is that possible?</p>
"
100,Finding $E(|X_1|\ |\max |X_i| )$,"<p>Suppose $X_1,\ldots,X_n$ are a random sample of $U(-\theta,\theta)$ with $\theta&gt;0$. How can find
$$
E\left(|X_1|\ \big|\max |X_i| \right)
$$ that $1\leq i\leq n$?</p>
"
101,R: Rolling rank of multivariate time series?,"<p>I want to rank a set of variables every day (starting with a <code>zoo</code> series).  </p>

<p>Here's an example:</p>

<pre><code>set.seed(1)
x &lt;- zoo(matrix(rnorm(9), nrow=3), as.Date(""2010-01-01"") + 0:2)
colnames(x) &lt;- letters[1:3]
</code></pre>

<p>The only way I know to do this is with <code>rollapply</code>, but this is quite slow.  </p>

<pre><code>&gt;  rollapply(x, 1, rank, by.column=FALSE)
           a b c
2010-01-01 1 3 2
2010-01-02 1 2 3
2010-01-03 1 2 3
</code></pre>

<p>Any other suggestions?</p>
"
102,Standard error of the mean for root mean square of data,"<p>If I have a distribution of data, X, representing N samples taken during one measurement, then the mean square of X is $\bar{X^2} = \langle X^2 \rangle$, the variance of $X^2$, $\mathrm{var}(X^2)$ is $\langle\langle X^4 \rangle - \langle X^2 \rangle \rangle$, and the standard error of the mean square is $\sqrt{\frac{\mathrm{var}(X^2)}{N}}$.</p>

<p>Thus, the standard error of the mean square represents one standard deviation of the distribution that would be produced by repeating the measurement (taking N samples each time), assuming that $X^2$ is normally distributed with the variance of the original measurement.</p>

<p>What is the standard error of the quantity $\sqrt{\langle X^2 \rangle}$ (the standard error of the root mean square?)?</p>

<p>A rephrasing:  assume Y is normally distributed with mean $\mu$ and variance $\sigma^2$.  Define Z = $\sqrt{\frac{1}{N}\sum_1^N Y_i}$ What is the variance of Z?</p>
"
103,Saving a list of plots by their names(),"<p>Let's say I have a list of plots that I've created. </p>

<pre><code>library(ggplot2)
plots &lt;- list()
plots$a &lt;- ggplot(cars, aes(speed, dist)) + geom_point()
plots$b &lt;- ggplot(cars, aes(speed)) + geom_histogram()
plots$c &lt;- ggplot(cars, aes(dist)) + geom_histogram()
</code></pre>

<p>Now, I would like to save all of these, labelling each with their respective names(plots) element. </p>

<pre><code>lapply(plots, 
       function(x) { 
         ggsave(filename=paste(...,"".jpeg"",sep=""""), plot=x)
         dev.off()
         }
       )
</code></pre>

<p>What would I replace ""..."" with such that in my working directory the plots were saved as: </p>

<pre><code>a.jpeg
b.jpeg
c.jpeg
</code></pre>
"
104,Grouping variable using if statement in R,"<p>I would like to do a simple if statement to group codes into groups.  The variable has number codes and I would like to create a new variable that groups several number codes together.  I have written the following if statement but because they are many code numbers (30 codes), I need help writing a more elegant code to group the variable rather than writing 30+ if statements.</p>

<pre><code>Data2$RevisedSIC.Group &lt;-c()
for (i in 1:length(Data2$SIC.Group )) {
if (Data2$SIC.Group[i] ==""10110"") Data2$RevisedSIC.Group [i]=""Metal"" else 
if (Data2$SIC.Group[i] ==""10410"") Data2$RevisedSIC.Group [i]=""Metal"" else
if (Data2$SIC.Group[i] ==""10439"") Data2$RevisedSIC.Group [i]=""Metal"" else
if (Data2$SIC.Group[i] ==""14111"") Data2$RevisedSIC.Group [i]=""Stone"" else
if (Data2$SIC.Group[i] ==""10421"") Data2$RevisedSIC.Group [i]=""Stone"" }
</code></pre>
"
105,ValueError: setting an array element with a sequence. scipy minimize,"<pre><code>"""""" ___ """"""
from scipy.optimize import minimize
import numpy as np


LENGTH = 100

def process(x):
    return x * 2 + 5

def draw(process, length):
    """""" """"""
    y = np.random.normal(0, 10, length)
    data = [process(y_) for y_ in y]
    rnd = np.random.normal(3, 1, len(data))
    return y, rnd + data


def maximum_likelyhood(y, X):
    objective = lambda b: np.transpose(X) * (y - X * b)
    x0 = np.zeros(100)
    res =  minimize(objective, x0=x0)
    return res.x

y, X = draw(process, LENGTH)
print maximum_likelyhood(y, X)
</code></pre>

<p>produces a</p>

<pre><code>ValueError: setting an array element with a sequence.
</code></pre>

<p>There is several similar problems, they all point out that x0 is not a 1D array, but here it is a 1D array. (or not? in case please explain why and how to make it 1D)</p>
"
106,How do I replace excel that is too slow / runs out of resources? (complete noob),"<p>What I need to do is very simple. It's basic math calculations, finding the largest number in rows 1-100, 2-101, 3-102 (to a million), Finding the first number greater than X in row 1 all columns, row 2 all columns, ect.</p>

<p>The problem is the amount of data I have. It is a million rows. I have no experience with programming, except running websites before using php which is a little different. Unless I were to use php with mysql (if that is good) but I still don't know how to do that. </p>

<p>I'm seeking recommendations on what to do. Would using a CSV file and some programming language with it be good?</p>

<p>I know how to search online for how to do simple math. And it seems simple in programming languages. What I can't find out is how to join that knowledge with editing files. </p>

<p>I have searched online for ages and can't find a thing. I can find how to read files, and how to write to files. I can not find anything about how to read files, apply some math to it and write it elsewhere to the file or to some other file. </p>

<p>Any help / recommendations would be very helpful!</p>
"
107,What data stucture should I use for BigInt class,"<p>I would like to implement a BigInt class which will be able to handle really big numbers. I want only to add and multiply numbers, however the class should also handle negative numbers. </p>

<p>I wanted to represent the number as a string, but there is a big overhead with converting string to int and back for adding. I want to implement addition as on the high school, add corresponding order and if the result is bigger than 10, add the carry to next order. </p>

<p>Then I thought that it would be better to handle it as a array of unsigned long long int and keep the sign separated by bool. With this I'm afraid of size of the int, as C++ standard as far as I know guarantees only that int &lt; float &lt; double. Correct me if I'm wrong. So when I reach some number I should move in array forward and start adding number to the next array position. </p>

<p>Is there any data structure that is appropriate or better for this? Thanks in advance.</p>
"
108,Hypothesis testing approach,"<p>I have the following situation. I'm studying how malicious host on the internet choose their victims. I must discover if a number of host are acting an in independent way or not. I'm using an hypothesis testing approach:</p>

<ul>
<li>The null hypothesis is that the host scan the internet in a independent fashion</li>
<li>The alternative hypothesis is that they scan coordinately</li>
</ul>

<p>Stating that the first is true (independent scan) we can calculate the distribution of the number of destination ip addresses that receive no scanning.
My question is, given that distribution how can I prove that the host scan in an independent fashion? In particular, I am observing a certain number of ip addresses.
From this observation I can say what is the number of addresses that have not received any scanning. Using only that number and the given distribution can I use some type of test to prove the null hypothesis?</p>

<p>Another idea was to divide the addresses I'm observing in a series of groups and use the chi square goodness fit test. Do you think that this approach will be formally correct?</p>
"
109,numpy with multidimensional array,"<p>I have checked example which is wrapping c by using numpy.
there is two converting function( function for converting numpy to c,
function for converting c to numpy)
and they both use malloc function.
but if I do not use memory copy, how can I make the code? </p>

<pre><code>def matmul1(np.ndarray[DTYPE_t, ndim=2] a, np.ndarray[DTYPE_t, ndim=2]b):
    ''' Matrix multiplication. Takes two square Float32 numpy arrays.'''

    cdef int N = a.shape[0]
    cdef int i
    cdef float **a_c
    cdef float **b_c
    cdef float **res

    # check if square arrays:
    if a.shape[1] != N or b.shape[0] != N or b.shape[1] != N:
        raise ValueError, 'matmul1: need square arrays for multiplication!'

    # check if contiguous, if not force C contiguous arrays
    if not (&lt;object&gt;a).flags[""C_CONTIGUOUS""]:
        a = a.copy('C')
    if not (&lt;object&gt;b).flags[""C_CONTIGUOUS""]:
        b = b.copy('C')

    # convert using the function
    a_c = npy2c_float(a)
    b_c = npy2c_float(b)

    # allocate res
    res = &lt;float **&gt; malloc(N*sizeof(float*))
    for i in range(N):
        res[i] = &lt;float *&gt; malloc(N * sizeof(float))

    matmul(a_c,b_c,res,N)

    free(a_c)
    free(b_c)

    # convert to numpy array and free res
    result = c2npy_float(res,N,N)

    return result
</code></pre>
"
110,Maths help with exponentials,"<p>Evaluate</p>

<p>(z x^-1 y)^5 y^5</p>

<p>~~~~~~~~~~~~~~~~~~~~~~~ OVER</p>

<p>x^-4 z^-4</p>

<p>How would I evaluate this if X = 10, y = -3 and z = 3? I would like a step-by-step solution to help me fully understand it.</p>
"
111,Lagging Forward in plm,"<p>This is a very simple question, but I haven't been able to find a definitive answer, so I thought I would ask it.  I use the <code>plm</code> package for dealing with panel data.  I am attempting to use the <code>lag</code> function to lag a variable FORWARD in time (the default is to retrieve the value from the previous period, and I want the value from the NEXT).  I found a number of old articles/questions (circa 2009) suggesting that this is possible by using <code>k=-1</code> as an argument.  However, when I attempt this, I get an error.</p>

<p>Sample code:</p>

<pre><code>library(plm)
df&lt;-as.data.frame(matrix(c(1,1,1,2,2,3,20101231,20111231,20121231,20111231,20121231,20121231,50,60,70,120,130,210),nrow=6,ncol=3))
names(df)&lt;-c(""individual"",""date"",""data"")
df$date&lt;-as.Date(as.character(df$date),format=""%Y%m%d"")
df.plm&lt;-pdata.frame(df,index=c(""individual"",""date""))
</code></pre>

<p>Lagging:</p>

<pre><code>lag(df.plm$data,0)
##returns
1-2010-12-31 1-2011-12-31 1-2012-12-31 2-2011-12-31 2-2012-12-31 3-2012-12-31 
         50           60           70          120          130          210

lag(df.plm$data,1)
##returns
1-2010-12-31 1-2011-12-31 1-2012-12-31 2-2011-12-31 2-2012-12-31 3-2012-12-31 
         NA           50           60           NA          120           NA

lag(df.plm$data,-1)
##returns
Error in rep(1, ak) : invalid 'times' argument
</code></pre>

<p>I've also read that <code>plm.data</code> has replaced <code>pdata.frame</code> for some applications in <code>plm</code>.  However, <code>plm.data</code> doesn't seem to work with the <code>lag</code> function at all:</p>

<pre><code>df.plm&lt;-plm.data(df,indexes=c(""individual"",""date""))
lag(df.plm$data,1)
##returns
[1]  50  60  70 120 130 210
attr(,""tsp"")
[1] 0 5 1
</code></pre>

<p>I would appreciate any help.  If anyone has another suggestion for a package to use for lagging, I'm all ears.  However, I do love <code>plm</code> because it automagically deals with lagging across multiple individuals and skips gaps in the time series.</p>
"
112,"android - showing a fraction of numbers , one above the other , just like on libreoffice","<p>libreoffice (and other apps , like latex) allows you to use some kind of script language to show really cool mathematical formulas . </p>

<p>i need to use a view (probably a textView , but not mandatory) that will be able to shows such formulas , and fractions in particular . </p>

<p>for example : ""x over y "" would look like :</p>

<p>x<br/>
--<br/>
y</p>

<p>and so on.</p>

<p>is there any such solution for this on android ? maybe a library (with license similar to apache or better ) that allows you to write such things ? </p>
"
113,Opencv integration with wxpython,"<p>I just wanted to integrate the opencv video stream from my web cam into a more complex gui than highgui can offer, nothing fancy just a couple of buttons and something else, however it's proven to be not that trivial. I can't find any base example from which I can start designing the gui.
I tried converting this <a href=""https://wxpython-users.googlegroups.com/attach/90dcdd863e015578/cv_webcam.py?view=1&amp;part=2"" rel=""nofollow"">code</a> to the new opencv interface with quite a poor result. I'm a new to opencv, numpy and gui design. Some time does stream the video but most of the time it just hangs there. I guess my one mistake might be in wx.BitmapFromBuffer(col, row, img) since in the older version they used pil image format and now it's using numpy arrays so in the original code the used the pil function ""imageData"", instead of passing directly the numpy array as I'm doing.
Any help it's really appreciated.
<img src=""http://i.stack.imgur.com/p4DXq.png"" alt=""color_channels_pic""></p>

<p>This is my code conversion.</p>

<pre><code>import wx
import cv2

class MyFrame(wx.Frame):
   def __init__(self, parent):
       wx.Frame.__init__(self, parent)
       self.displayPanel = wx.Panel(self)
       self.displayPanel.SetSize(wx.Size(800,640))

       self.cam = cv2.VideoCapture(1)
       self.cam.set(3, 640)
       self.cam.set(4, 480)
       ret, img = self.cam.read()

       cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
       row, col, x = img.shape
       self.SetSize((col,row))
       self.bmp = wx.BitmapFromBuffer(col, row, img)
       self.displayPanel.Bind(wx.EVT_PAINT, self.onPaint)

       self.playTimer = wx.Timer(self)
       self.Bind(wx.EVT_TIMER, self.onNextFrame)

       self.playTimer.Start(1000/15)

    def onPaint(self, evt):
        if self.bmp:
            dc = wx.BufferedPaintDC(self.displayPanel)
            self.PrepareDC(dc)
            dc.DrawBitmap(self.bmp, 0, 0, True)
        evt.Skip()

    def onNextFrame(self, evt):
        ret, img = self.cam.read()
        if ret == True:
            cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            self.bmp.CopyFromBuffer(img)
            self.displayPanel.Refresh()
        evt.Skip()

if __name__==""__main__"":
    app = wx.App()
    MyFrame(None).Show()
    app.MainLoop()
</code></pre>
"
114,MySQL to generate an Age Pyramid,"<p>How to write a query suitable for generating an age pyramid like this:
<img src=""http://i.stack.imgur.com/FgtGU.gif"" alt=""alt text""></p>

<p>I have a table with a DATE field containing their birthday and a BOOL field containing the gender (male = 0, female = 1). Either field can be NULL.</p>

<p>I can't seem to work out how to handle the birthdays and put them into groups of 10 years.</p>

<p>EDIT:</p>

<p>Ideally the X axis would be percent rather than thousands :)</p>
"
115,Adding a row to a dataframe,"<p>I am reading a file line by line and then adding specific lines to a dataframe. Here is an example of a line I would add to a dataframe:</p>

<p>ATOM    230  CA  GLU A  31      66.218 118.140   2.411  1.00 31.82           C</p>

<p>I have verified that my checks are ok, I think it has specifically to do with my rbind command. Thanks for your help!</p>

<p>Edit: The error is as follows, the output of the dataframe is:</p>

<pre><code>Residue AtomCount SideChain XCoord  YCoord ZCoord
2       MET         1         A 62.935  97.579 30.223
21     &lt;NA&gt;         2         A 63.155  95.525 27.079
3      &lt;NA&gt;         3         A 65.289  96.895 24.308
</code></pre>

<p>It seems like it stops picking up the name of the residue..</p>

<p>The code I am using is:</p>

<pre><code>get.positions &lt;- function(sourcefile, chain_required = ""A""){
positions = data.frame()
visited = list()
filedata &lt;- readLines(sourcefile, n= -1)
for(i in 1: length(filedata)){
  input = filedata[i]
  id = substr(input,1,4)
  if(id == ""ATOM""){
    type = substr(input,14,15)
      if(type == ""CA""){
        #if there are duplicates it takes the first one
        residue = substr(input,18,20)
        type_of_chain = substr(input,22,22)
        atom_count = strtoi(substr(input, 23,26))
        if(atom_count &gt;=1){
          if(type_of_chain == chain_required &amp;&amp; !(atom_count %in% visited) ){
            position_string = trim(substr(input,30,54))
            position_string = lapply(unlist(strsplit(position_string,"" +"")),as.numeric)
            positions&lt;- rbind(positions, list(residue, atom_count, type_of_chain, position_string[[1]], position_string[[2]], position_string[[3]]))
            }
        }
      }
     }

    } 
        return (positions)
 }
</code></pre>
"
116,Force mapply to return a list?,"<p>Suppose I have a function that creates data frames.  I'd like to run that function with different input values, and then rbind the results together into one big data frame, as below:</p>

<pre><code>CreateDataFrame &lt;- function(type=""A"", n=10, n.true=8) {
  data.frame(success=c(rep(TRUE, n.true), rep(FALSE, n - n.true)), type=type)
}
df &lt;- do.call(rbind, lapply(toupper(letters[1:5]), CreateDataFrame))
</code></pre>

<p>My CreateDataFrame function takes three arguments.  In the example above, the second and third arguments are held constant.  I'd like to do the same as above, but have the second and third arguments change on each call.  I think I have to use mapply, like this:</p>

<pre><code>mapply(""CreateDataFrame"", type=toupper(letters[1:5]), n=10, n.true=8:4)
</code></pre>

<p>I'm having trouble because mapply isn't returning a list, which prevents me from running <code>do.call(rbind, mapply(...))</code>.  How can I end up with a single data frame, as I did in the example at the top?</p>

<p>Looks like mapply is returning a matrix of lists.  I was expecting it to return a list of data frames.  What should I do differently?</p>
"
117,Make this process more processor intensive and less memory intensive,"<p>This question is a follow-up to <a href=""http://stackoverflow.com/questions/9465817/count-days-per-year"">Count days per year</a>.</p>

<p>I did what Dirk suggested with a huge data.frame. My commands look like this:</p>

<pre><code>dateSeq &lt;- function(df) {
  res &lt;- seq(as.Date(df[""begin""]), as.Date(df[""end""]), by = ""1 day"")
  format(res, ""%Y"")
}

dataFrame$seq &lt;- apply(dataFrame, 1, dateSeq)
dataFrame_years &lt;- do.call(""c"", dataFrame[[""seq""]])

rm(dataFrame)
gc()
gc()

dataFrame_tab &lt;- table(dataFrame_years)
</code></pre>

<p>Now, these commands fill up my 8 GB Ram and 2 GB swap space. In the mean time my processor is bored having a processor load of maybe 15 %.</p>

<p>Besides, it takes ages for my computer to fulfill my ""desires"".
Can I shift some of the work to the CPU and unburden my Ram a bit?</p>
"
118,How to find the surrounding area 25 miles using latitude & longitude from the current user location,"<p>I found Haversine Formula in C# is there any other method better than this.</p>

<pre><code>public double HaversineDistance(LatLng pos1, LatLng pos2, DistanceUnit unit)
    {
        double R = (unit == DistanceUnit.Miles) ? 3960 : 6371;
        var lat = (pos2.Latitude - pos1.Latitude).ToRadians();
        var lng = (pos2.Longitude - pos1.Longitude).ToRadians();
        var h1 = Math.Sin(lat / 2) * Math.Sin(lat / 2) +
                      Math.Cos(pos1.Latitude.ToRadians()) * Math.Cos(pos2.Latitude.ToRadians()) *
                      Math.Sin(lng / 2) * Math.Sin(lng / 2);
        var h2 = 2 * Math.Asin(Math.Min(1, Math.Sqrt(h1)));
        return R * h2;
    }
</code></pre>
"
119,Most representative instance of a cluster,"<p>After performing a cluster analysis to my dataset (a dataframe named <em>data.matrix</em>), I added a new column, named <em>cluster</em>, at the end (col 27) containing the cluster name that each instance belongs to.</p>

<p>What I want now, is a representative instance from each cluster. I tried to find the instance having the smallest euclidean distance from the cluster's centroid (and repeat the procedure for each one of my clusters)</p>

<p>This is what I did. Can you think of other -perhaps more elegant- ways? (assume numeric columns with no nulls). </p>

<pre><code>clusters &lt;- levels(data.matrix$cluster)
cluster_col = c(27)

for (j in 1:length(clusters)) {
    # get the subset for cluster j
    data = data.matrix[data.matrix$cluster == clusters[j],]

    # remove the cluster column
    data &lt;- data[,-cluster_col]

    # calculate the centroid
    cent &lt;- mean(data)

    # copy data to data.matrix_cl, attaching a distance column at the end
    data.matrix_cl &lt;- cbind(data, dist = apply(data, 1, function(x) {sqrt(sum((x - cent)^2))}))

    # get instances with min distance
    candidates &lt;- data.matrix_cl[data.matrix_cl$dist == min(data.matrix_cl$dist),]

    # print their rownames
    print(paste(""Candidates for cluster "",j))
    print(rownames(candidates))
}
</code></pre>
"
120,How do I efficiently find the maximum value in an array containing values of a smooth function?,"<p>I have a function that takes a floating point number and returns a floating point number.  It can be assumed that if you were to graph the output of this function it would be 'n' shaped, ie. there would be a single maximum point, and no other points on the function with a zero slope.  We also know that input value that yields this maximum output will lie between two known points, perhaps 0.0 and 1.0.</p>

<p>I need to efficiently find the input value that yields the maximum output value to some degree of approximation, without doing an exhaustive search.</p>

<p>I'm looking for something similar to <a href=""http://en.wikipedia.org/wiki/Newton%27s_method"" rel=""nofollow"">Newton's Method</a> which finds the roots of a function, but since my function is opaque I can't get its derivative.</p>
"
121,twitter followers connectedness R Python,"<p>I'm trying to get an adjacency matrix of the followers of an institution in Twitter with the connections between them. I want to see how do the followers relate to each other. I know what to do from the adjacency matrix on, <code>igraph</code> and <code>statnet</code> packages allow that. But I'm having troubles getting the matrix.</p>

<p>I've tried working with <code>twitteR</code> package to get the users one by one but I get timeout or the machine hangs. I haven't seen this done, but I'm sure there's a shorter way. Any insights or references on this matter would be great. </p>

<p>Thanks.</p>
"
122,Changing facet label to math formula in ggplot2,"<p>I wonder how to change the <code>facet</code> label to math formula in <code>ggplot2</code>. </p>

<pre><code>d &lt;- ggplot(diamonds, aes(carat, price, fill = ..density..)) +
  xlim(0, 2) + stat_binhex(na.rm = TRUE) + opts(aspect.ratio = 1)
d + facet_wrap(~ color, ncol = 4)
</code></pre>

<p><img src=""http://i.stack.imgur.com/XxIpB.png"" alt=""enter image description here""></p>

<p>For example, I want to change facet label from <code>D</code> to <code>Y[1]</code>, where 1 is subscript. Thanks in advance for your help.</p>

<p>I found this <a href=""http://stackoverflow.com/a/6539953/707145"">answer</a> but it does not work for me. I'm using <code>R 2.15.1</code> and <code>ggplot2 0.9.1</code>.</p>
"
123,How to improve a spatial raster map using ggplot when compared to spplot?,"<p>How can I improve the legend of spatial raster map plot using ggplot when compared to a spplot() legend?</p>

<p>I would like plot spatial maps using ggplot() instead of ssplot() however there are a few things that I would like to improve when compared to the spplot:</p>

<ol>
<li>create a ggplot legend that goes from small (bottom) to large values (top)  </li>
<li>Have the breaks in the ggplot legend similar to the ssplot() legend so that I know what the boundaries are of each color.  </li>
</ol>

<hr>

<pre><code>## load packages
require(raster)
require(ggplot2)
require(rgdal)
require(RColorBrewer)
set.seed(1)

r &lt;- raster(xmn=-110, xmx=-90, ymn=40, ymx=60, ncols=40, nrows=40,
          crs=""+proj=lcc +lat_1=48 +lat_2=33 +lon_0=-100
+ellps=WGS84"")
r &lt;- setValues(r,matrix(rnorm(1600, mean=0.4,sd=0.2))) 

## 1. spatial map with spplot
cuts &lt;-seq(minValue(r),maxValue(r),length.out=8)
cuts = round(cuts,digits=2)
col.regions = brewer.pal(length(cuts)+3-1, ""RdYlGn"")
print( 
spplot(as(r, 'SpatialGridDataFrame'),at=cuts,
col.regions=col.regions,
colorkey=list(labels=list(at=cuts),at=cuts), pretty=TRUE,
scales=list(draw=T)
) 
)

## 2. spatial map with ggplot
p = rasterToPoints(r); df = data.frame(p)
colnames(df) = c(""x"", ""y"", ""NDVI"")

p  &lt;- ggplot(data=df) + geom_tile(aes(x, y, fill=NDVI)) +
coord_equal() + labs(x=NULL, y=NULL) + 
scale_fill_gradient2(low=""red"", mid=""yellow"",high=""green"",
limits=c(minValue(r),maxValue(r)), midpoint = 0.4) + theme_bw() +
scale_x_continuous(expand=c(0,0)) + scale_y_continuous(expand=c(0,0))
print(p)
</code></pre>

<p>ssplot() result
<img src=""http://i.stack.imgur.com/tKSq7.png"" alt=""ssplot""></p>

<p>ggplot() result
<img src=""http://i.stack.imgur.com/Oc69g.png"" alt=""ggplot""></p>
"
124,A function that returns a dataset,"<p>I want to create a function that takes a dataset name and a package name and returns the dataset as data.frame. Here is my try</p>

<pre><code>loadDataSet &lt;- function(name, pkg) {
      varname &lt;- data(name, package=pkg)
      return(get(varname[[1]]))
    }
loadDataSet(""acme"", ""boot"")
</code></pre>

<p>However, this function fails. The problem seems to be, that the call to data() does not look up the value of the name variable, but rather ""name"".</p>

<p>I already know how to go from a variable to its name, via deparse(substitute(var)). But how do I go the other way, from ""var"" to var?</p>

<p>Any hint appreciated!</p>
"
125,Why do double and float exist?,"<h3>Duplicate</h3>

<blockquote>
  <p><a href=""http://stackoverflow.com/questions/803225/when-should-i-use-double-instead-of-decimal"">http://stackoverflow.com/questions/803225/when-should-i-use-double-instead-of-decimal</a></p>
  
  <blockquote>
    <p>.. and many more...</p>
  </blockquote>
</blockquote>

<p>We use the C# and SQL Server decimal datatypes   throughout our apps because of their accuracy. Never had any of those irritating problems where the total doesn't add up to the detail, etc.</p>

<p>I was wondering why double and float exist at all given their inaccuracy</p>
"
126,Perfect Powers check,"<p>A perfect power is a number N where A^B = N (A >= 1 , B >= 2)</p>

<p>This is my code. I'm trying to find how many of these numbers exist between 1 and the top limit I select.</p>

<pre><code>    static void Main(string[] args)
    {
        int PPwr_Count = 1; //number 1 is included by default.
        int Top_Limit = 1000000; //Can be any number up to 10^9
        for (int Number = 2; Number &lt;= Top_Limit; Number++)
        {
            int myLog = (int)Math.Floor(Math.Log(Number, 2) + 1);
            for (int i = 2; i &lt;= myLog; i++) 
            {
                //As a Math rule I only need to check below Base 2 Log of number
                int x = Convert.ToInt32(Math.Pow(Number, 1.0 / i));
                 if (Number == Math.Pow(x, i))
                 {
                     PPwr_Count++;
                     break;
                 }
                 else continue;
            }
        }
     } 
</code></pre>

<p>It's currently working. Sadly it becomes quite slow after around 1,000,000 checks. Anyhow to improve this algorithm's speed?</p>
"
127,Theoretical proof of convergence of sequential weight update procedure (Neural Networks and Machine Learning),"<p>My question is at the bottom.
(Most of the descriptive words come from Chris. Bishop's <em>Neural Networks for Pattern Recognition</em>)</p>

<p>let $w$ be the weight vector of the neural network and $E$ the error function. </p>

<p>According to the Robbins-Monro algorithm, this $$w_{kj}^{(r+1)}=w_{kj}^{(r)}-\eta\left.\frac{\partial E}{\partial w_{kj}}\right|_{w^{(r)}}$$ will converge to where $$\frac{\partial E}{\partial w_{kj}}=0$$</p>

<p>In general the error function is given by a sum of terms each of which is calculated using one of the patterns from the training set, so that $$E=\sum_nE^n(w)$$
And in applications we just update the weight vector using one pattern at a time $$w_{kj}^{(r+1)}=w_{kj}^{(r)}-\eta\frac{\partial E^n}{\partial w_{kj}}$$</p>

<p>My question is: Why the algorithm will converge using the last formula? Once we use it to update the $w$, the value of $w$ is changed, I can't prove the convergence using $$\frac{\partial E}{\partial w_{kj}}=\sum_n \frac{\partial E^n}{\partial w_{kj}}$$</p>
"
128,How to analyze risk vs. reward for spending on research and development work?,"<p>Imagine I have a company that makes widgets, where each widget costs me A dollars to make. Each month I can allocate money toward research and development with the aim of finding a new process that will allow me to build widgets for a cost of A/B dollars. Presume that I know that for each C dollars I spend on research and development there's a D% chance of finding a breakthrough. Of course, spending money on research and development means that I have less to spend on building widgets.</p>

<p>I have a monthly budget of E dollars. This budget is not directly tied to my profit margin, but it is safe to say that it my profit margins influence future budgets (i.e., if I make no widgets for three straight months b/c I do all research and development, it's likely that my budget will be reduced, whereas if I discover a breakthrough the first month my profits will skyrocket and I'll likely see that budget grow over time).</p>

<p>In case that is too abstract, here's the real world scenario I'm interested in solving (although I'd like a more general approach, as well):</p>

<ul>
<li>A = 15 dollars</li>
<li>B = 3</li>
<li>C = 5 dollars</li>
<li>D = 2.75%</li>
<li>E = 30 dollars</li>
</ul>

<p>That is, today widgets cost me 15 dollars to build but if I can find a breakthrough I know I can make them at 1/3 the cost (5 dollars). For each 5 dollars I spend on research and development there is a 2.75% chance I'll find the breakthrough. However, I have only 30 dollars to spend each month. If I spend it all on research and development and have no success then I have made no widgets for sale. If I spend it all on widget construction I have no chance of finding a breakthrough.</p>

<p>Is there some statistical distribution or formula that can let me plug in these variables and see some sort of breakdown that gives me an idea of whether it's a good idea to spend any money on research and development each month and, if so, how much?</p>
"
129,What machine learning classifiers are the most parallelizeable?,"<p>What machine learning classifiers are the most parallelizeable? If you had a difficult classification problem, limited time, but a decent LAN of computers to work with, what classifiers would you try?</p>

<p>Off hand it looks to me like some standard classifiers I know of stack up as follows but I could be totally wrong:</p>

<p>Random Forests - Very parallelizeable as long as each machine can hold all the data (i.e. can't divide up the training data per se, but otherwise parallelizeable).</p>

<p>Boosting - ?</p>

<p>Support Vector Machine - Not very parallelizable.</p>

<p>Decision trees - Can be divided up in part, but not very efficiently. </p>
"
130,Installing RStudio from source on Ubuntu 10.04 requiring newer versions of R,"<p>When trying to install RStudio on Ubuntu 10.04 from source R needs to be pre-installed, having gone through a bunch of steps to install the latest version of R (i.e. version 2.14.1), when trying to use the cmake function to install RStudio it still throws up an error saying that my version is too old and i need at least R version 2.11.</p>

<p>From the error message I have looked at the CMakeLists.txt, but am not really too sure what to do in order to make it point to the correct newer version of R.</p>

<p>Below is an illustration of the issue it was created on an EC2 instance.</p>

<p>Any help would be appreciated...</p>

<pre><code>hideyoshi@ip-10-77-70-100:~$ R

R version 2.14.1 (2011-12-22)
Copyright (C) 2011 The R Foundation for Statistical Computing
ISBN 3-900051-07-0
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

&gt; q()
Save workspace image? [y/n/c]: n
hideyoshi@ip-10-77-70-100:~$ cd rstudio
hideyoshi@ip-10-77-70-100:~/rstudio$ cmake -DRSTUDIO_TARGET=Desktop -    DCMAKE_BUILD_TYPE=Release ..
-- Found R: /usr/lib64/R
CMake Error at src/cpp/CMakeLists.txt:178 (message):
  Minimum R version (2.11.1) not found.


-- Configuring incomplete, errors occurred!
</code></pre>

<p>and here is the CMakeLists.txt file</p>

<pre><code>hideyoshi@ip-10-77-70-100:~/rstudio/src/cpp$ cat CMakeLists.txt     
#
# CMakeLists.txt
#
# Copyright (C) 2009-11 by RStudio, Inc.
#
# This program is licensed to you under the terms of version 3 of the
# GNU Affero General Public License. This program is distributed WITHOUT
# ANY EXPRESS OR IMPLIED WARRANTY, INCLUDING THOSE OF NON-INFRINGEMENT,
# MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Please refer to the
# AGPL (http://www.gnu.org/licenses/agpl-3.0.txt) for more details.
#
#

cmake_minimum_required(VERSION 2.6)
project (RSTUDIO_CPP)

# include globals (normally these are included at the root but we also
# include them here to support cpp only configurations for development)
include(""${CMAKE_CURRENT_SOURCE_DIR}/../../CMakeGlobals.txt"")

# global directives
add_definitions(-DBOOST_ENABLE_ASSERT_HANDLER)

# UNIX specific global directivies
if(UNIX)
   # cmake modules
   include(CheckFunctionExists REQUIRED)
   include(CheckSymbolExists REQUIRED)

   # need pkgconfig for pango cairo on linux
   if(NOT APPLE)
      find_package(PkgConfig REQUIRED)
   endif()

   # compiler flags
   add_definitions(-Wall -pthread)

   # workaround boost bug (https://svn.boost.org/trac/boost/ticket/4568)
   # by disabling kqueue support. note that this bug was fixed in boost 1.45
   add_definitions(-DBOOST_ASIO_DISABLE_KQUEUE)

   # if present, set osx deployment target variables from environment vars
   if(APPLE)
      if(NOT $ENV{CMAKE_OSX_SYSROOT} STREQUAL """")
         set(CMAKE_OSX_SYSROOT $ENV{CMAKE_OSX_SYSROOT})
         message(STATUS ""Set CMAKE_OSX_SYSROOT to ${CMAKE_OSX_SYSROOT}"")    
      endif()
      if(NOT $ENV{CMAKE_OSX_DEPLOYMENT_TARGET} STREQUAL """")
         set(CMAKE_OSX_DEPLOYMENT_TARGET $ENV{CMAKE_OSX_DEPLOYMENT_TARGET})
         message(STATUS ""Set CMAKE_OSX_DEPLOYMENT_TARGET to         ${CMAKE_OSX_DEPLOYMENT_TARGET}"")
      endif()
   endif()

   # gcc hardending options (see: http://wiki.debian.org/Hardening)
   if(NOT APPLE)
      add_definitions(-Wformat -Wformat-security)
      add_definitions(-D_FORTIFY_SOURCE=2)
      add_definitions(-fstack-protector --param ssp-buffer-size=4)
      add_definitions(-pie -fPIE)
      set(CMAKE_EXE_LINKER_FLAGS ""${CMAKE_EXE_LINKER_FLAGS} -Wl,-z,relro,-z,now"")
   endif()

# Win32 specific global directives
else()
   add_definitions(-DWINVER=0x501)

   if(RSTUDIO_SESSION_WIN64)

      # increase stack size to 10MB, avoid mingw auto-importing warning,
      set(CMAKE_EXE_LINKER_FLAGS ""${CMAKE_EXE_LINKER_FLAGS} -Wl,--stack=0x00a00000,--enable-auto-import"")

      add_definitions(-D_WIN64
                      -D_WIN64_WINNT=0x0501
                      -D_WIN64_IE=0x600
                      -DWIN64_LEAN_AND_MEAN
                      -DBOOST_USE_WINDOWS_H)
   else()

      # increase stack size to 10MB, avoid mingw auto-importing warning,
      # and ensure that we are large address aware
      set(CMAKE_EXE_LINKER_FLAGS ""${CMAKE_EXE_LINKER_FLAGS} -Wl,--stack=0x00a00000,--enable-auto-import,--large-address-aware"")

      add_definitions(-D_WIN32_WINNT=0x0501
                      -D_WIN32_IE=0x600
                      -DWIN32_LEAN_AND_MEAN)
   endif()

    endif()

# determine whether we should statically link boost. we always do this
# unless we are building a non-packaged build on linux (in which case
# boost dynamic libraries are presumed to be installed on the system ldpath)
if(APPLE OR WIN32 OR RSTUDIO_PACKAGE_BUILD)
   set(Boost_USE_STATIC_LIBS ON)
endif()

if(APPLE OR WIN32)
   # Require Boost 1.44 on Windows and Mac to avoid bugs in Boost Filesystem v2
   # that cause the process to abort instead of returning errors
   set(BOOST_VERSION 1.44.0)
else()
   set(BOOST_VERSION 1.42.0)
endif()
list(APPEND BOOST_LIBS
   date_time
   filesystem
   iostreams
   program_options
   regex
   signals
   system
   thread
)

# UNIX BOOST
if(UNIX)
   # prefer static link to /usr/local/include/boost (our custom built version)
   if(EXISTS /usr/local/include/boost AND
      EXISTS /usr/local/lib/libboost_system.a)

      # find headers
      set(Boost_USE_STATIC_LIBS ON)
      set(BOOST_INCLUDEDIR /usr/local/include)
      find_package(Boost ${BOOST_VERSION} REQUIRED)

      # define library list manually (find_package doesn't always pick them up)
      set(BOOST_LIB_DIR /usr/local/lib)
      foreach(BOOST_LIB ${BOOST_LIBS})
         list(APPEND Boost_LIBRARIES ${BOOST_LIB_DIR}/libboost_${BOOST_LIB}.a)
      endforeach()
   else()
      find_package(Boost ${BOOST_VERSION} REQUIRED COMPONENTS ${BOOST_LIBS})
   endif()

# WIN32 BOOST
else()
     # hard-code to our own prebuilt boost libs
     if(RSTUDIO_SESSION_WIN64)
        set(BOOST_ROOT ""${RSTUDIO_WINDOWS_DEPENDENCIES_DIR}/boost-win/boost64"")
     else()
        set(BOOST_ROOT ""${RSTUDIO_WINDOWS_DEPENDENCIES_DIR}/boost-win/boost32"")
     endif()
     set(BOOST_INCLUDEDIR ""${BOOST_ROOT}/include/boost-1_44"")

   find_package(Boost ${BOOST_VERSION} REQUIRED COMPONENTS ${BOOST_LIBS})
endif()

# core library
add_subdirectory(core)

# are we in CORE_DEV mode? if so then just add the core/dev project
# otherwise, add the rest of our projects
if(RSTUDIO_CONFIG_CORE_DEV)

   add_subdirectory(core/dev)

else()

   # find LibR
   if(RSTUDIO_SESSION_WIN64)
      set(LIBR_FIND_WINDOWS_64BIT TRUE)
   endif()
   find_package(LibR REQUIRED)

   # verify we got the required R version
   if(LIBR_FOUND AND RSTUDIO_VERIFY_R_VERSION)
      include(CheckCSourceRuns)
      set(CMAKE_REQUIRED_INCLUDES ${LIBR_INCLUDE_DIRS})
      check_c_source_runs(""
        #include &lt;Rversion.h&gt;
        int main()
        {
           int meetsRequirement = R_VERSION &gt;= R_Version(${RSTUDIO_R_MAJOR_VERSION_REQUIRED},${RSTUDIO_R_MINOR_VERSION_REQUIRED},${RSTUDIO_R_PATCH_VERSION_REQUIRED});
           return !meetsRequirement;
        }""
        LIBR_MINIMUM_VERSION)
      if(NOT LIBR_MINIMUM_VERSION)
         message(FATAL_ERROR ""Minimum R version (${RSTUDIO_R_MAJOR_VERSION_REQUIRED}.${RSTUDIO_R_MINOR_VERSION_REQUIRED}.${RSTUDIO_R_PATCH_VERSION_REQUIRED}) not found."")
      endif()
   endif()

   # r library
   add_subdirectory(r)

   # initialize subdirectories
   file(MAKE_DIRECTORY conf)

   # add desktop subprojects if we aren't building in server only mode
   if(RSTUDIO_DESKTOP)

      add_subdirectory(desktop)
      configure_file(rdesktop-dev.in ${CMAKE_CURRENT_BINARY_DIR}/rdesktop-dev)
      configure_file(conf/rdesktop-dev.conf ${CMAKE_CURRENT_BINARY_DIR}/conf/rdesktop-dev.conf)

   endif()

   # add this after desktop so it is not included in fixup_bundle
   # processing which we do in desktop
   add_subdirectory(session)


   # add server subprojects if we aren't building in desktop only mode
   if(RSTUDIO_SERVER)

      add_subdirectory(server)

      configure_file(rserver-dev ${CMAKE_CURRENT_BINARY_DIR}/rserver-dev)
      configure_file(conf/rserver-dev.conf ${CMAKE_CURRENT_BINARY_DIR}/conf/rserver-dev.conf)
      configure_file(conf/rsession-dev.conf ${CMAKE_CURRENT_BINARY_DIR}/conf/rsession-dev.conf)

   endif()

endif()
</code></pre>
"
131,Collision algorithm for 2 moving object,"<p>this is a Math problem for a 2d game.</p>

<pre><code>Given 2 object ( 2 car, 2 tank, 2...), for each object i know: 
1. X, Y 
2. Actual Speed
3. Degree of movement (or radiant)
</code></pre>

<p>How to calculate the ""effect"" of a collision for the 2 object</p>

<p>Ps:</p>

<p>I ""move"" the object with this simple formula each tick of my ""game loop"": </p>

<pre><code>ychange = Math.Sin(radiant) * ((carspeed / xVar));
xchange = Math.Cos(radiant) * ((carspeed / xVar));
</code></pre>

<p>(where xVar is a ""multiplicator"" to make more slow my car on the screen)</p>

<p>And for the ""first"" object which collide, if i add a ""minus"" (-) in front of that formula, i can simulate a good ""bounce"" of the first object.. but not for the second.</p>

<p>So my question regard a way to calculate a good (not perfect ! ) and more realistic ""impact effect"" for the second object.</p>

<p>Thanks for your help </p>
"
132,How to display text with Quotes in R?,"<p>I have started learning R and am trying to create vector as below:</p>

<pre><code>c(""""check"""")
</code></pre>

<p>I need the output as : ""check"". But am getting syntax error. How to escape the quotes while creating a vector?</p>
"
133,mlbench example,"<p>I heard R is the ""de facto"" language amongst statistical software developers, and I'm giving it a try. I already know the basics, but it still looks ""weird"" to me (a C developer). I think it would be very useful to see a working example to see how a real R program is built.
I thought that an R solution for any of the mlbench problems would be optimal, because I'm already familiar with it and it would allow me to compare it to other languages, but any other ""toy problem"" example is welcome.</p>

<p>The <code>mlbench</code> package is pointed out in the answers below, but it seems that it provides only sample data and functions to generate sample data, with the exception of a generic bayes classifier. I'm searching for solutions of any of the mlbench data problems (DNA, Glass, Ionosphere, etc.). Maybe I'm missing something?</p>
"
134,How do I read multiple binary files in R?,"<p>Suppose we have files in one folder file1.bin, file2.bin, ... , and file1460.bin in directory C:\R\Data and we want to read them and  make a loop to go from 1 to 4 and take the average then from 4 to 8 average  and so on till  1460.in the end will get 360 files
I tried to have them in a list,but  did not know how to make the loop.</p>

<p>How do I read multiple files  and manupulat them? in R language<br>
I have been wasting countless hourse to figuer it out.any help</p>
"
135,Extract the gradient from the deriv command,"<p>A colleague asked me the following question the other day. In the following piece of code, how do you extract the gradient:</p>

<pre><code>&gt; x=5
&gt; a = eval(deriv(~ x^3, ""x""))
&gt; a
[1] 125
attr(,""gradient"")
      x
[1,] 75
</code></pre>

<p>My answer was </p>

<pre><code>&gt;  attr(a, ""gradient"")[1]
[1] 75
</code></pre>

<p>This syntax seems clunky to me. Is there a better way of extracting the gradient?</p>
"
136,How to expand an ellipsis (...) argument without evaluating it in R,"<p>I need a function that accepts an arbitrary number of arguments and stores them in a variable as an expression without evaluating them. I managed to do it with <code>match.call</code> but it seems a little ""kludgy"".</p>

<pre><code>foo &lt;- function(...) {
  expr &lt;- match.call()
  expr[[1]] &lt;- expression
  expr &lt;- eval(expr)
  # do some stuff with expr
  return(expr)
}

&gt; bla
Error: object 'bla' not found
&gt; foo(x=bla, y=2)
expression(x = bla, y = 2)
</code></pre>

<h1>Clarification</h1>

<p>To clarify, I'm asking how to write a function that behaves like <code>expression()</code>. I can't use <code>expression()</code> directly for reasons that are too long to explain.</p>
"
137,R insert a row by comparing two dataframe columns,"<p>I have one data frame with two columns (language, articles) as column headers, and then I have another dataframe with column headers being language and count. </p>

<p>I want to iterate over first dataframe and if the value in the language column of first dataframe matches the value of the language column in second dataframe. I want to insert the value of articles column to the second dataframe.</p>

<pre><code>  language articles
1       en  4200596
2       de  1571581
3       fr  1369891
4       nl  1405514
5       it  1020971
6       es   981124

  language count numArticles
1       gv    86          NA
2      sco     3          NA
3      zea    19          NA
4      szl     0          NA
5      pnb     2          NA
6      cdo    28          NA
</code></pre>

<p>I accomplished it by writing a loop but I am sure this is not the best way to do it or may be there is R way of doing this in much cleaner and a faster way:</p>

<pre><code>for(i in 1:numberOfElements) {
  for(k in 1:numberOfElements) {
    if (as.character(wiki.template.count$language[i]) == as.character(wiki.lang.codes.size$language[k])) {
      wiki.template.count$numArticles[i] &lt;- wiki.lang.codes.size$articles[k]
    }    
  }
}
</code></pre>
"
138,Javascript + math function to exclude a certain number x times,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stackoverflow.com/questions/4228356/integer-division-in-javascript"">Integer division in JavaScript</a>  </p>
</blockquote>



<p>Lets say I have a specific number.</p>

<pre><code>B = 350
</code></pre>

<p>And I have elements depending on that number, but they all have different amounts of that number in them, like:</p>

<pre><code>c = 351,
d = 710,
e = 1100
</code></pre>

<p>What I want is to be presented by the leftover from dividing the c, d, e with B x times. i.e.</p>

<pre><code>c = 1,
d = 10,
e = 50
</code></pre>
"
139,Changing parameters while using plotCI in R. (Shift points left or right),"<p>I know that you can shift boxplots left or right on a graph by adding ""at=1:6-0.2"" or ""at=1:6+0.2"" to the code, but the same is not the case when I am using plotCI. Does anyone know how to perform this simple parameter adjustment? I know it has to be easy but there are very few questions about plotCI on here. It is in the package {gplots}. This is driving me crazy! Thanks for any help.
  -Alex</p>
"
140,how to communicate with R through VBnet( or C#),"<p>Recently, I Developed an Experiment Application with VB.net(in Windows platform), When the application collected the data,I want to use R to Analysis the data, But I don't know how to <strong><em>Communicate  with R</em></strong> (In other word, <strong><em>I want to send R script  to R in  my own application</em></strong>). I will appreciate if anyone could give me some suggetions or some reference documents.  Thank you very much! </p>
"
141,Can't figure out error in lapply,"<p>I am a beginner having picked up R a few weeks ago and am trying to learn the <code>apply</code> family. Can&#39;t figure out how to use <code>lapply</code> and it is maddenning. Yes, I looked up <code>?lapply</code> and several books including <a href=""http://rads.stackoverflow.com/amzn/click/059680170X"" rel=""nofollow"">R in a nutshell</a> and <a href=""http://rads.stackoverflow.com/amzn/click/0596809158"" rel=""nofollow"">R cookbook</a> and still can&#39;t figure out what am I doing wrong.</p></p>

<pre><code>lapply(X = c(""ggplot2"", ""gtable"", ""grid""), library)
## Error: 'package' must be of length 1
lapply(X = c(""ggplot2"", ""gtable"", ""grid""), FUN = function(x) library(x))
## Error: there is no package called 'x'
lapply(X = c(""ggplot2"", ""gtable"", ""grid""), FUN = library)
## Error: 'package' must be of length 1
x = c(""ggplot2"", ""gtable"", ""grid"")
lapply(x, library)
## Error: 'package' must be of length 1
lapply(x, FUN = function(x) library(x))
## Error: there is no package called 'x'
</code></pre>
"
142,Select observations from a subset to create a new subset based on a large dataframe in R,"<p>I have a dataset (Purchase.df) that contains many columns and rows. The important variable names for this question are ""Customer"", ""OrderDate"", ""DateRank"" (which ranks the dates so I can find the smallest date) and ""BrandName."" Below is a very small sample of what I'm working with: (I'm new to this website, so I hope what I paste below works)</p>

<pre><code>Purchase.df&lt;-structure(list(Customer = c(10071535L, 10071535L, 10071535L, 
10071535L, 10071535L, 10071535L, 10071711L, 10071711L, 10071711L, 
10071711L, 10071711L, 10071711L, 10071711L, 10071711L, 10071711L, 
10071711L, 10071711L, 10071711L, 10072059L, 10072059L, 10072059L, 
10072113L, 10072113L, 10072113L, 10072113L, 10072113L, 10072113L, 
10072113L), BrandName = structure(c(1L, 2L, 2L, 2L, 3L, 3L, 2L, 
2L, 2L, 2L, 3L, 3L, 1L, 3L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 
2L, 3L, 3L, 3L, 3L), .Label = c(""X"", ""Y"", ""Z""), class = ""factor""), 
OrderDate = structure(c(14L, 14L, 15L, 16L, 19L, 20L, 11L, 
18L, 5L, 6L, 1L, 17L, 21L, 22L, 23L, 8L, 10L, 13L, 7L, 9L, 
12L, 4L, 4L, 2L, 2L, 2L, 3L, 3L), .Label = c(""1/17/2011 0:00"", 
""1/19/2010 0:00"", ""1/25/2010 0:00"", ""1/4/2010 0:00"", ""10/22/2010 0:00"", 
""11/15/2010 0:00"", ""11/23/2011 0:00"", ""12/14/2011 0:00"", 
""12/16/2011 0:00"", ""2/7/2012 0:00"", ""3/16/2010 0:00"", ""3/21/2012 0:00"", 
""4/16/2012 0:00"", ""4/27/2012 0:00"", ""5/16/2012 0:00"", ""5/30/2012 0:00"", 
""5/5/2011 0:00"", ""6/1/2010 0:00"", ""6/12/2012 0:00"", ""7/3/2012 0:00"", 
""8/1/2011 0:00"", ""8/16/2011 0:00"", ""9/19/2011 0:00""), class = ""factor""), 
DateRank = c(18.5, 18.5, 20, 21, 24, 25, 15, 23, 9, 10, 1, 
22, 26, 27, 28, 12, 14, 17, 11, 13, 16, 7.5, 7.5, 3, 3, 3, 
5.5, 5.5)), .Names = c(""Customer"", ""BrandName"", ""OrderDate"", 
""DateRank""), row.names = c(NA, -28L), class = ""data.frame"")
</code></pre>

<p>I've created a subset of this large dataset (subset.df) which finds the first OrderDate for each customer, and tells me which brand they purchased. I used the following code to do this:</p>

<pre><code>subset1&lt;-split(Purchase.df,Purchase.df$Customer)
subset2&lt;-lapply(split(Purchase.df,Purchase.df$Customer), function(chunk) chunk[which(chunk$DateRank==min(chunk$DateRank)),])
subset.df&lt;-do.call(rbind, as.list(subset2))
</code></pre>

<p>Now, I want to figure out which customers ordered Brand X on their first OrderDate, and create a new dataset (BigSubset.df) that contains all of the OrderDates for the customers that purchased Brand X on their first order date.</p>

<p>Should look something like this:</p>

<pre><code>Customer    BrandName   OrderDate   DateRank
10071535    X   4/27/2012 0:00  18.5
10071535    Y   4/27/2012 0:00  18.5
10071535    Y   5/16/2012 0:00  20
10071535    Y   5/30/2012 0:00  21
10071535    Z   6/12/2012 0:00  24
10071535    Z   7/3/2012 0:00   25
10072059    X   11/23/2011 0:00 11
10072059    X   12/16/2011 0:00 13
10072059    X   3/21/2012 0:00  16
10072113    X   1/4/2010 0:00   7.5
10072113    Y   1/4/2010 0:00   7.5
10072113    Y   1/19/2010 0:00  3
10072113    Z   1/19/2010 0:00  3
10072113    Z   1/19/2010 0:00  3
10072113    Z   1/25/2010 0:00  5.5
10072113    Z   1/25/2010 0:00  5.5
</code></pre>

<p>I can't seem to get R to reference the smaller dataset when I attempt to create BigSubset.df from Purchase.df because the number of rows are not equal. I've searched on Google and haven't seen any answers, so I'm not even sure if this is possible in R. Let me know what you think.</p>
"
143,For loop skipping large part of data set,"<p>I have another problem with my data set. Basically, there is a list of genes with associated features including position numbs (columns 3 and 4) and strand orientation (+ or -). I am trying to do a calculation with the positions to make them relative to the start codon TYPE (second column) for each gene, rather than the entire genome (as it is now). The problem is that, the calculation is only performed on the + STRAND sequences, the - STRAND sequences are not showing up in the output. Below is a sample of the data set, my code, the output, and what I've tried.</p>

<p>Here's the data set:</p>

<pre><code>    GENE_ID TYPE    POS1    POS2    STRAND
PITG_00002  start_codon 10520   10522   -
PITG_00002  stop_codon  10097   10099   -
PITG_00002  exon    10474   10522   -
PITG_00002  CDS 10474   10522   -
PITG_00002  exon    10171   10433   -
PITG_00002  CDS 10171   10433   -
PITG_00002  exon    10097   10114   -
PITG_00002  CDS 10100   10114   -
PITG_00003  start_codon 38775   38777   +
PITG_00003  stop_codon  39069   39071   +
PITG_00003  exon    38775   39071   +
PITG_00003  CDS 38775   39068   +
</code></pre>

<p>Here is the code:</p>

<pre><code>import numpy
import pandas
import pandas as pd
import sys

sys.stdout = open(""outtry2.txt"", ""w"")
data = pd.read_csv('pinfestans-edited2.csv', sep='\t')
groups = data.groupby(['STRAND', 'GENE_ID'])

corrected = []

for (direction, gene_name), group in groups:
    ##print direction,gene_name
    if group.index[group.TYPE=='start_codon']:
        start_exon = group.index[group.TYPE=='exon'][0]
    if direction == '+':
        group['POSA'] = 1 + abs(group.POS1 - group.POS1[start_exon])
        group['POSB'] = 1 + abs(group.POS2 - group.POS1[start_exon])
    else:
        group['POSA'] = 1 - abs(group.POS2 - group.POS2[start_exon])
        group['POSB'] = 1 - abs(group.POS1 - group.POS2[start_exon])
    ##print group
    corrected.append(group)
</code></pre>

<p>Here is a sample of the output:</p>

<pre><code>     + PITG_00003
    GENE_ID     TYPE         POS1   POS2   STRAND  POSA  POSB
8   PITG_00003  start_codon  38775  38777  +       1     3   
9   PITG_00003  stop_codon   39069  39071  +       295   297 
10  PITG_00003  exon         38775  39071  +       1     297 
11  PITG_00003  CDS          38775  39068  +       1     294 
</code></pre>

<p>Previously I was getting an array value error (<a href=""http://stackoverflow.com/questions/14543663/tab-delimited-dataset-valueerror-truth-of-array-with-more-than-one-element-is-am#comment20286851_14543663"">Tab delimited dataset ValueError truth of array with more than one element is ambiguous error</a>) but that has been taken care of. So next I tried only doing this part:</p>

<pre><code>import numpy
import pandas
import pandas as pd
import sys

##sys.stdout = open(""outtry2.txt"", ""w"")
data = pd.read_csv('pinfestans-edited2.csv', sep='\t')#,
              #converters={'STRAND': lambda s: s[0]})
groups = data.groupby(['STRAND', 'GENE_ID'])

corrected = []

for (direction, gene_name), group in groups:
    print direction,gene_name
</code></pre>

<p>And the output printed out all the GENE_IDs and their STRAND symbol (+ or -), and it did it for both the + and - sequences. So somewhere below that it isn't selecting any of the sequences with - in the STRAND column.</p>

<p>So I tried adding this to the original code:</p>

<pre><code>if direction == '+':
    group['POSA'] = 1 + abs(group.POS1 - group.POS1[start_exon])
    group['POSB'] = 1 + abs(group.POS2 - group.POS1[start_exon])
elif direction == '-':
    group['POSA'] = 1 - abs(group.POS2 - group.POS2[start_exon])
    group['POSB'] = 1 - abs(group.POS1 - group.POS2[start_exon])
else:
    break
print group
# put into the result array
corrected.append(group)
</code></pre>

<p>and this is the very end of the output, it printed the first - and then froze for awhile before ending:</p>

<pre><code>+
        GENE_ID     TYPE         POS1    POS2    STRAND  POSA  POSB
134991  PITG_23350  start_codon  161694  161696  +       516   518 
134992  PITG_23350  stop_codon   162135  162137  +       957   959 
134993  PITG_23350  exon         161179  162484  +       1     1306
134994  PITG_23350  CDS          161694  162134  +       516   956 
-
</code></pre>
"
144,finding colors in Image and redraw on second image(using numpy),"<p>I am trying to find specific colors(or more likely ranges of colors) in a picture and redraw the coordinates on a second picture for further investigation. Since I read a lot about how great numpy is, I started using that(I am pretty new with python and especially numpy).
So, I wrote something, that works good with one color, but has it's (performance-)problems with a lot of color and I am pretty sure, I can solve that, by using more functions that numpy provides. Basically, I want to kill the for-loops. That's my code(I have to add more ranges of colors later):</p>

<pre><code>import PIL, numpy
from PIL import Image

def add_color_range(p_a_c, r,r1,g,g1,b,b1):
    for ir in range(r, r1+1):
        for ig in range(g, g1+1):
            for ib in range(b, b1+1):
                p_a_c.extend([[ir,ig,ib]])
    return p_a_c

for i in range(1):
    im = Image.open('%*s.bmp'% (1, i))
    n_test = numpy.asarray(im)
    ni_test = numpy.zeros([1050,1680,3],dtype=numpy.uint8)
    ni_test.fill(255)
    c=[]
    c=add_color_range(c,5,10,5,10,5,10)

    for ic in range(len(c)):
        ind=numpy.where(numpy.all(n_test==c[ic],  axis=-1))
        for ii in range(len(ind[0])):
            ni_test[ind[0][ii],ind[1][ii]]=[0,0,0]
    im_test = Image.fromarray(ni_test, 'RGB')
    im_test.save('test_%*s.bmp'% (1, i))
</code></pre>
"
145,How to convert 4d array to 3d array subsetting on specific elements of one of the dimensions,"<p>Here is probably an easy question.. but I am really struggling so help is very much appreciated.</p>

<p>I have 4d data that I wish to transform into 3d data. The data has the following attributes:  </p>

<pre><code>lon &lt;- 1:96  
lat &lt;- 1:73  
lev &lt;- 1:60  
tme &lt;- 1:12

data &lt;- array(runif(96*73*60*12), 
              dim=c(96,73,60,12) ) # fill with random test values  
</code></pre>

<p>What I would like to do is calculate the mean of the first few levels (say 1:6). The new data would be of the form:  </p>

<pre><code>new.data &lt;- array(96*73*12), dim=c(96,73,12) ) # again just test data  
</code></pre>

<p>But would contain the mean of the first 5 levels of data. At the moment the only way I have been able to make it work is to write a rather inefficient loop which extracts each of the first 5 levels and divides the sum of those by 5 to get the mean.  </p>

<p>I have tried:  </p>

<pre><code>new.data &lt;- apply(data, c(1,2,4), mean)  
</code></pre>

<p>Which nicely gives me the mean of ALL the vertical levels but can't understand how to subset the 3rd dimension to get an average of only a few! e.g.  </p>

<pre><code>new.data &lt;- apply(data, c(1,2,3[1:5],4), mean) # which returns   
  Error in ds[-MARGIN] : only 0's may be mixed with negative subscripts
</code></pre>

<p>I am desperate for some help!</p>
"
146,Python/Numpy MemoryError,"<p>Basically, I am getting a memory error in python when trying to perform an algebraic operation on a numpy matrix. The variable u, is a large matrix of double (in the failing case its a 288x288x156 matrix of doubles. I only get this error in this huge case, but I am able to do this on other large matrices, just not this big). Here is the python error:</p>

<pre><code> Traceback (most recent call last):

 File ""S:\Lab_KSwanson\3D_Simulation_Data\BB8559\Patient SPM Segmentation\20 pc
t perim erosion flattop\2010-11-24 12.41.35\SwSim.py"", line 121, in __init__
   self.mainSimLoop()

 File ""S:\Lab_KSwanson\3D_Simulation_Data\BB8559\Patient SPM Segmentation\20 pc
t perim erosion flattop\2010-11-24 12.41.35\SwSim.py"", line 309, in mainSimLoop
   u = solver.solve_cg(u,b,tensors,param,fdHold,resid) # Solve the left hand si
de of the equation Au=b with conjugate gradient method to approximate u

 File ""S:\Lab_KSwanson\3D_Simulation_Data\BB8559\Patient SPM Segmentation\20 pc
t perim erosion flattop\2010-11-24 12.41.35\conjugate_getb.py"", line 47, in solv
e_cg

u = u + alpha*p

MemoryError
</code></pre>

<p>u = u + alpha*p is the line of code that fails.</p>

<p>Alpha is just a double, while u and r are the large matrices described above (both of the same size).</p>

<p>I don't know that much about memory errors especially in python. Any insight/tips into solving this would be very appreciated! </p>

<p>Thanks</p>
"
147,Why does JavaScript Math.log(1.001) return the wrong value?,"<p>JavaScript returns .0009995003330834232.</p>

<p>Every other way of calculating returns 0.000434077479319.</p>
"
148,Installing RPostgresql on a Mac with OSX 10.7.2,"<p>I'm trying to install rpostgresql on a mac. I want to be able to connect to a server using an IP address and read the data into a dataframe. I downloaded the package from CRAN, and ran the following line:</p>

<pre><code>install.packages('/Users/celenius/Downloads/RPostgreSQL_0.2-1.tar.gz', type='source')
</code></pre>

<p>This started to compile the package but resulted with the following error message:</p>

<pre><code>&gt; install.packages('/Users/celenius/Downloads/RPostgreSQL_0.2-1.tar.gz', type='source')
Installing package(s) into /Library/Frameworks/R.framework/Versions/2.14/Resources/library
(as lib is unspecified)
inferring 'repos = NULL' from the file name
* installing *source* package RPostgreSQL ...
** package RPostgreSQL successfully unpacked and MD5 sums checked
checking for gcc... gcc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables... 
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ISO C89... none needed
checking for pg_config... /usr/bin/pg_config
checking for ""/usr/include/libpq-fe.h""... yes
configure: creating ./config.status
config.status: creating src/Makevars
** libs
*** arch - x86_64
gcc-4.2 -arch x86_64 -std=gnu99 -I/Library/Frameworks/R.framework/Resources/include -I/Library/Frameworks/R.framework/Resources/include/x86_64 -I/usr/include -I/usr/local/include    -fPIC  -g -O2 -c RS-DBI.c -o RS-DBI.o
make: gcc-4.2: No such file or directory
make: *** [RS-DBI.o] Error 1
ERROR: compilation failed for package RPostgreSQL
* removing /Library/Frameworks/R.framework/Versions/2.14/Resources/library/RPostgreSQL
Warning in install.packages :
  installation of package /Users/celenius/Downloads/RPostgreSQL_0.2-1.tar.gz had non-zero exit status
</code></pre>

<p>I've looked through advice on the rpostgresql webpage (<a href=""http://code.google.com/p/rpostgresql/wiki/MacOSXInstallFinkPostgresql"" rel=""nofollow"">1</a>, <a href=""http://code.google.com/p/rpostgresql/w/list"" rel=""nofollow"">2</a>) but a lot of the suggestions are more than a year old so I thought there might be more recent advice on how to install this package.</p>
"
149,Using R in Processing through rJava/JRI?,"<p>Is it possible to run R in Processing through rJava/JRI? If I deployed a Processing app on the web, would the client need R on their system?</p>

<p>I'm looking to create an interactive information dashboard that I can deploy on the web. It seems that Processing is probably my best bet for the interactive/web part of things. Unfortunately, it doesn't look like there are many math/stats functions built-in. And there aren't any libraries for plotting data either. </p>

<p>I've been using R and gpplot2 for a few months and am thrilled (amazed) at how easily it manipulates and plots data. </p>

<p>So I'm wondering now if can get the best of both worlds and run R through a Processing applet.</p>

<p>From the <a href=""http://rosuda.org/JRI/"">JRI</a> website:</p>

<blockquote>
  <p>JRI is a Java/R Interface, which allows to run R inside Java
  applications as a single thread.
  Basically it loads R dynamic library
  into Java and provides a Java API to R
  functionality. It supports both simple
  calls to R functions and a full
  running REPL.</p>
  
  <p>In a sense JRI is the inverse of rJava
  and both can be combined (i.e. you can
  run R code inside JRI that calls back
  to the JVM via rJava). The JGR project
  makes the full use of both JRI and
  rJava to provide a full Java GUI for
  R.</p>
  
  <p>JRI uses native code, but it supports
  all platforms where Sun's Java (or
  compatible) is available, including
  Windows, Mac OS X, Sun and Linux (both
  32-bit and 64-bit).</p>
</blockquote>

<p>Thanks for the advice :)</p>
"
150,r reshape data long to wide with unknown number of columns,"<p>I'm sure this is trivial but I can't find how to do it.</p>

<p>I have a data frame in which there are individuals, each of which can have several properties, and each property is classified in a number of ways. Currenly it's in long shape, with a record looking like (in schematic form, actually it's a little more complicated):</p>

<pre><code>IndividualID Property PropClass 
1            X         A 
1            Y         B 
2            X         A 
3            Y         B
3            W         C
3            Z         A
</code></pre>

<p>What I want is one row for each individual ID, with the individual ID and then pairs of columns for each property and PropClass that that individual has on the original file, so in this case:</p>

<pre><code> IndividualID  Prop1   PropClass1 Prop2  PropClass2  Prop3  PropClass3
 1             X       A          Y      B           NA     NA
 2             X       A          NA     NA          NA     NA
 3             Y       B          W      C           Z      A
</code></pre>

<p>So there have to be as many Prop and PropClass variables as the maximum number of rows for any individualID in the original data set (which is not large, about 5), and where an individual has fewer rows in the original dataset than that maximum number, the extra columns that don't mean anything for that individual have NAs in them. The order of the Prop and PropClass variables for an individual doesn't matter (though it may as well be the original order on the long format file). </p>

<p>Obviously it's easy to do this (e.g. using reshape) if you have one pair of Prop and propClass columns for every possible value of Prop, but there are several hundred possible values of Prop so the file gets huge and unhelpful. I can't believe there is not a simple way to do what I want, but I haven't found it despite what seems to me to be assiduous searching. Please tell me I'm being an idiot, and if so, how I might cure my idiocy.</p>
"
151,numpy unique without sort,"<p>How can I use numpy unique without sorting the result but just in the order they appear in the sequence? Something like this?</p>

<p><code>a = [4,2,1,3,1,2,3,4]</code></p>

<p><code>np.unique(a) = [4,2,1,3]</code></p>

<p>rather than</p>

<p><code>np.unique(a) = [1,2,3,4]</code></p>

<p>Use naive solution should be fine to write a simple function. But as I need to do this multiple times, are there any fast and neat way to do this?</p>
"
152,Quickly apply xts vector operations across wide zoo objects in R,"<p>This is really an extension of my <a href=""http://stackoverflow.com/questions/4309248/aggregate-by-week-in-r"">question</a> yesterday where I learned about <code>apply.weekly</code>. This works great, but I want to do this over wide <code>zoo</code> objects. If I use <code>apply.weekly</code> on a wide <code>zoo</code> it sums the columns, then performs the weekly aggregation:</p>

<pre><code>&gt; library(xts)
&gt; set.seed(2001)
&gt; zoo.daily &lt;- zoo(data.frame(a=rnorm(20), b=rnorm(20), c=rnorm(20)), order.by=as.Date(""2001-05-25"") + 0:19)
&gt; apply.weekly(zoo.daily, sum)
2001-05-27 2001-06-03 2001-06-10 2001-06-13 
  1.091999  -3.017688   3.842305   2.045370 
&gt; apply.weekly(zoo.daily[, 1] + zoo.daily[, 2] + zoo.daily[, 3], sum) 
2001-05-27 2001-06-03 2001-06-10 2001-06-13 
  1.091999  -3.017688   3.842305   2.045370 
</code></pre>

<p>I tried the <code>apply</code> family of operators, but those seem to strip out the <code>zoo</code> date index. I can do it in a <code>for</code> loop, but it's really time-consuming (much, much more than a factor of four slower than the <code>aggregate</code> function on <code>as.yearmon</code> periodicity). Here's the <code>for</code> loop:</p>

<pre><code>week.ends &lt;- index(zoo.daily[endpoints(zoo.daily, ""weeks"")[-1], ])
num.weeks &lt;- nweeks(zoo.daily)
num.stocks &lt;- ncol(zoo.daily)
zoo.weeks &lt;- zoo(matrix(NA, num.weeks, num.stocks), order.by=week.ends)
for (i in seq(num.stocks)) {
    zoo.weeks[, i] &lt;- apply.weekly(zoo.daily[, i], mean)
}
</code></pre>

<p>Which works (i.e., keeps each vector separate):</p>

<pre><code>2001-05-27 -0.36663040 -0.108648725  0.8392788
2001-06-03  0.33032998  0.003025018 -0.7644534
2001-06-10  0.07816992  0.620198931 -0.1494681
2001-06-13  0.02114608  0.956226189 -0.2955824
</code></pre>

<p>Is there a way to quickly operate on all columns with <code>apply.weekly</code>? Thanks!</p>

<p>UPDATE: Joshua Ulrich points out that I need a column aware function (like <code>colMeans</code> or <code>colSums</code>). When I do this, I get the correct answers, but as a transposed matrix. Should I just reclass and move on? Or do I have an option/setting wrong?</p>

<pre><code>&gt; apply.weekly(zoo.daily, colSums)
        [,1]        [,2]       [,3]        [,4]
a -1.0998912  2.31230989  0.5471894  0.06343824
b -0.3259462  0.02117512  4.3413925  2.86867857
c  2.5178365 -5.35117351 -1.0462765 -0.88674717
</code></pre>
"
153,Missing Coordinates. Basic Trigonometry Help,"<p>please refer to my quick diagram attached below.</p>

<p>what i'm trying to do is get the coordinates of the yellow dots by using the angle from the red dots' known coordinates.  assuming each yellow dot is about 20 pixels away from the x:50/y:250 red dot at a right angle (i think that's what it's called) how do i get their coordinates?</p>

<p>i believe this is very basic trigonometry and i should use Math.tan(), but they didn't teach us much math in art school.</p>

<p><img src=""http://www.freeimagehosting.net/uploads/e8c848a357.jpg"" alt=""alt text""></p>
"
154,Perpendicular on a line segment from a given point,"<p>I want to calculate a point on a given line that is perpendicular from a given point. </p>

<p>I have a line segment AB and have a point C outside line segment. I want to calculate a point D on AB such that CD is perpendicular to AB.</p>

<p><img src=""http://i.stack.imgur.com/vgsm5.png"" alt=""Find point D""></p>

<p>I have to find point D. </p>

<p>It quite similar to <a href=""http://stackoverflow.com/questions/1811549/perpendicular-on-a-line-from-a-given-point"">this</a>, but I want to consider to Z coordinate also as it does not show up correctly in 3D space.</p>
"
155,Solving simultaneous multivariate polynomial equations with python,"<p><em>edit: the reference I got my equations from contained a couple of errors. I've fixed it here. Solutions might actually make sense now!</em></p>

<p>When a two layer fluid flows over topography, there exist a number
of different solutions depending on the relative size of the flow
speed and the wave speed in the fluid.</p>

<p><img src=""http://i.stack.imgur.com/DLeEN.png"" alt=""critical-flow""></p>

<p>These are termed 'supercritical', 'subcritical' and 'critical' (the
first two I refer to here as 'extra-critical').</p>

<p>The following equations define the bounding lines between critical
and extra-critical behaviour in (h, U0) parameter space:</p>

<p><img src=""http://latex.codecogs.com/gif.latex?U_0%5E2%20%5Cleft%28%5Cfrac%7Bd_0%5E2%7D%7Bd_%7B1c%7D%5E3%7D%20&plus;%20%5Cfrac%7B%281%20-%20d_0%29%5E2%7D%7B%281%20-%20d_%7B1c%7D%20-%20h%29%5E3%7D%20%5Cright%29%20-%201%20%3D%200"" alt=""eq1""></p>

<p><img src=""http://latex.codecogs.com/gif.latex?%5Cfrac%7B1%7D%7B2%7D%20U_0%5E2%20%5Cleft%28%20%5Cfrac%7Bd_0%5E2%7D%7Bd_%7B1c%7D%5E2%7D%20-%20%5Cfrac%7B%281%20-%20d_0%29%5E2%7D%7B%281%20-%20d_%7B1c%7D%20-%20h%29%5E2%7D%20%5Cright%29%20&plus;%20d_%7B1c%7D%20&plus;%20%28h%20-%20d_0%29%20%3D%200"" alt=""eq2""></p>

<p>I want to eliminate <code>d_1c</code> (i.e. I don't care what it is) and find
solutions to these equations in <code>(h, U_0)</code>.</p>

<p>Simplifying factors:</p>

<ul>
<li>I only need answers for <em>given</em> <code>d_0</code></li>
<li><em>I do not need exact solutions</em>, just an outline of the solution
curves, so this can be solved either analytically or numerically.</li>
<li>I only want to plot over the region (h, U0) = (0,0) to (0.5, 1).</li>
</ul>

<p>I'd like to solve this using modules available in the Enthought
distribuion (numpy, scipy, sympy), but really don't know where to
start. It's the elimination of the variable d1c that really confuses
me.  </p>

<p>Here are the equations in python:</p>

<pre><code>def eq1(h, U0, d1c, d0=0.1):
    f = (U0) ** 2 * ((d0 ** 2 / d1c ** 3) + (1 - d0) ** 2 / (1 - d1c - d0) ** 3) - 1
    return f

def eq2(h, U0, d1c, d0=0.1):
    f = 0.5 * (U0) ** 2 * ((d0 ** 2 / d1c ** 2) - (1 - d0) ** 2 / (1 - d1c - d0) ** 2) + d1c + (h - d_0)
    return f
</code></pre>

<p>I'm expecting a solution that has a number of solution branches (not
always physical, but don't worry about that) and looks roughly
like this:</p>

<p><img src=""http://i.stack.imgur.com/egrNP.png"" alt=""critical-regime-diagram""></p>

<p>How do I go about implementing this?</p>
"
156,Finding the closest point(s) by ranking them,"<p>I Have a <code>N x D</code> dimensional features, which I need to rank according to their distance to a <code>1 x D</code> dimensional vector. Any fast way to implement that in python without recursively apply <code>argmin</code>?</p>

<p>Thanks!</p>
"
157,how to match two numpy array of unequal length?,"<p>i have two 1D numpy arrays. The lengths are unequal. I want to make pairs (array1_elemnt,array2_element) of the elements which are close to each other. Lets consider following example</p>

<pre><code>    a = [1,2,3,8,20,23]
    b = [1,2,3,5,7,21,35]
</code></pre>

<p>The expected result is</p>

<pre><code>    [(1,1), 
    (2,2), 
    (3,3), 
    (8,7),
    (20,21),
    (23,25)]
</code></pre>

<p>It is important to note that 5 is left alone. It could easily be done by loops but I have very large arrays. I considered using nearest neighbor. But felt like killing a sparrow with a canon. </p>

<p>Can anybody please suggest any elegant solution.</p>

<p>Thanks a lot.</p>
"
158,Order categorical data in a stacked bar plot with ggplot2,"<p>I have a matrix with the following entries:</p>

<pre><code>dput(MilDis[1:200,])
structure(list(hhDomMil = c(""HED"", ""ETB"", ""HED"", ""ETB"", ""PER"", 
""BUM"", ""EXP"", ""TRA"", ""TRA"", ""PMA"", ""MAT"", ""MAT"", ""KON"", ""ETB"", 
""PMA"", ""PMA"", ""HED"", ""BUM"", ""BUM"", ""HED"", ""PMA"", ""PMA"", ""HED"", 
""TRA"", ""BUM"", ""EXP"", ""BUM"", ""PMA"", ""ETB"", ""MAT"", ""ETB"", ""ETB"", 
""KON"", ""MAT"", ""TRA"", ""BUM"", ""BUM"", ""TRA"", ""TRA"", ""PMA"", ""PMA"", 
""PMA"", ""MAT"", ""ETB"", ""TRA"", ""BUM"", ""TRA"", ""MAT"", ""BUM"", ""ETB"", 
""TRA"", ""TRA"", ""BUM"", ""KON"", ""ETB"", ""ETB"", ""ETB"", ""BUM"", ""KON"", 
""ETB"", ""ETB"", ""PMA"", ""TRA"", ""PER"", ""PER"", ""MAT"", ""HED"", ""KON"", 
""TRA"", ""TRA"", ""TRA"", ""EXP"", ""TRA"", ""BUM"", ""MAT"", ""MAT"", ""TRA"", 
""PMA"", ""HED"", ""PER"", ""TRA"", ""PER"", ""EXP"", ""PER"", ""BUM"", ""KON"", 
""BUM"", ""ETB"", ""ETB"", ""TRA"", ""PER"", ""ETB"", ""KON"", ""KON"", ""BUM"", 
""ETB"", ""BUM"", ""MAT"", ""BUM"", ""KON"", ""KON"", ""ETB"", ""MAT"", ""KON"", 
""PER"", ""ETB"", ""ETB"", ""KON"", ""PMA"", ""PER"", ""HED"", ""HED"", ""PMA"", 
""MAT"", ""PMA"", ""PER"", ""PMA"", ""TRA"", ""TRA"", ""MAT"", ""BUM"", ""BUM"", 
""KON"", ""ETB"", ""ETB"", ""ETB"", ""PMA"", ""TRA"", ""TRA"", ""PMA"", ""PER"", 
""KON"", ""PER"", ""BUM"", ""KON"", ""ETB"", ""ETB"", ""BUM"", ""TRA"", ""ETB"", 
""PMA"", ""HED"", ""MAT"", ""TRA"", ""BUM"", ""PMA"", ""BUM"", ""ETB"", ""TRA"", 
""TRA"", ""TRA"", ""PER"", ""EXP"", ""HED"", ""BUM"", ""EXP"", ""HED"", ""BUM"", 
""MAT"", ""DDR"", ""BUM"", ""MAT"", ""KON"", ""HED"", ""HED"", ""TRA"", ""BUM"", 
""PMA"", ""PMA"", ""PMA"", ""KON"", ""KON"", ""MAT"", ""ETB"", ""MAT"", ""TRA"", 
""MAT"", ""ETB"", ""ETB"", ""TRA"", ""MAT"", ""ETB"", ""TRA"", ""HED"", ""BUM"", 
""MAT"", ""TRA"", ""PMA"", ""BUM"", ""BUM"", ""EXP"", ""ETB"", ""EXP"", ""EXP"", 
""MAT"", ""TRA"", ""KON"", ""BUM"", ""BUM"", ""HED""), kclust = c(1L, 2L, 
15L, 4L, 5L, 6L, 5L, 7L, 8L, 5L, 6L, 5L, 11L, 6L, 5L, 1L, 9L, 
10L, 2L, 1L, 9L, 8L, 4L, 11L, 14L, 5L, 8L, 11L, 12L, 5L, 5L, 
14L, 15L, 2L, 10L, 6L, 8L, 4L, 6L, 8L, 14L, 14L, 16L, 10L, 5L, 
1L, 12L, 17L, 12L, 16L, 16L, 5L, 10L, 14L, 8L, 19L, 5L, 4L, 4L, 
14L, 2L, 14L, 9L, 7L, 1L, 14L, 4L, 15L, 18L, 16L, 9L, 14L, 6L, 
14L, 12L, 11L, 4L, 7L, 8L, 12L, 9L, 16L, 2L, 6L, 15L, 1L, 1L, 
3L, 14L, 5L, 5L, 9L, 14L, 6L, 5L, 14L, 15L, 2L, 14L, 2L, 1L, 
8L, 5L, 10L, 1L, 1L, 16L, 5L, 2L, 9L, 9L, 1L, 12L, 10L, 1L, 4L, 
1L, 9L, 8L, 8L, 5L, 10L, 1L, 10L, 2L, 6L, 15L, 2L, 2L, 10L, 5L, 
6L, 10L, 19L, 19L, 6L, 5L, 6L, 7L, 7L, 8L, 5L, 16L, 5L, 6L, 6L, 
1L, 10L, 12L, 4L, 7L, 19L, 7L, 8L, 16L, 10L, 5L, 16L, 12L, 7L, 
7L, 19L, 4L, 6L, 1L, 15L, 7L, 8L, 16L, 4L, 10L, 15L, 11L, 10L, 
1L, 10L, 17L, 1L, 2L, 1L, 14L, 8L, 8L, 14L, 10L, 8L, 6L, 6L, 
8L, 5L, 7L, 5L, 1L, 5L, 7L, 9L, 2L, 1L, 9L, 14L), order = c(9, 
1, 9, 1, 3, 7, 10, 5, 5, 2, 8, 8, 4, 1, 2, 2, 9, 7, 7, 9, 2, 
2, 9, 5, 7, 10, 7, 2, 1, 8, 1, 1, 4, 8, 5, 7, 7, 5, 5, 2, 2, 
2, 8, 1, 5, 7, 5, 8, 7, 1, 5, 5, 7, 4, 1, 1, 1, 7, 4, 1, 1, 2, 
5, 3, 3, 8, 9, 4, 5, 5, 5, 10, 5, 7, 8, 8, 5, 2, 9, 3, 5, 3, 
10, 3, 7, 4, 7, 1, 1, 5, 3, 1, 4, 4, 7, 1, 7, 8, 7, 4, 4, 1, 
8, 4, 3, 1, 1, 4, 2, 3, 9, 9, 2, 8, 2, 3, 2, 5, 5, 8, 7, 7, 4, 
1, 1, 1, 2, 5, 5, 2, 3, 4, 3, 7, 4, 1, 1, 7, 5, 1, 2, 9, 8, 5, 
7, 2, 7, 1, 5, 5, 5, 3, 10, 9, 7, 10, 9, 7, 8, 6, 7, 8, 4, 9, 
9, 5, 7, 2, 2, 2, 4, 4, 8, 1, 8, 5, 8, 1, 1, 5, 8, 1, 5, 9, 7, 
8, 5, 2, 7, 7, 10, 1, 10, 10, 8, 5, 4, 7, 7, 9)), .Names = c(""hhDomMil"", 
""kclust"", ""order""), row.names = c(NA, 200L), class = ""data.frame"")
</code></pre>

<p>I want to create a stacked bar plot like this one <img src=""http://i.stack.imgur.com/Eu7bL.png"" alt=""Barplot"">.</p>

<p>The only problems are, that  I would like to have the order of the stacks to fit this (ETB,PMA,PER,KON,TRA,DDR,BUM,MAT,HED,EXP) - the order numbers in the matrix and some aesthetic problems. I searched for a solution here but nun of the ordering suggestions worked for me... :-\ </p>

<ol>
<li>How do I plot such a ordered plot?</li>
<li>How do I set up x so that each bar is ""on"" one number?</li>
<li>How do I seperate the bars - here I tried that with a white border...?</li>
<li>How do I print all kclust numbers in x?</li>
</ol>

<p>Thanks a lot for your help!
Dominik</p>

<hr>

<p><strong>UPDATE</strong></p>

<p>Here is the code I used to draw my plot:</p>

<pre><code>mycols &lt;- c('#FFFD00', '#97CB00', '#3168FF', '#FF0200', '#FB02FE', \
'#CCFCCC', '#FE9900', '#98CBF8', '#00CCFF', '#00FD03') # Set milieu colors


ggplot(MilDis) +
 geom_bar(aes(kclust, fill=factor(hhDomMil), \
 colour=mycols), position='fill', binwidth=1, colour='white') +
 scale_fill_manual(values = mycols)
</code></pre>

<hr>

<p><strong>UPDATE 2:</strong></p>

<p>That's how I did it now:</p>

<pre><code>    mycols &lt;- c('#3168FF', '#00CCFF', '#98CBF8', '#CCFCCC', '#00FD03',\
   '#97CB00', '#FFFD00', '#FE9900', '#FB02FE', '#FF0200') # Set milieu colors

    ggplot(MilDis) +
      geom_bar(aes(factor(kclust), fill=reorder(hhDomMil,order)),\
      position='fill') +
      scale_fill_manual(values = mycols)
</code></pre>

<p>With that result:</p>

<p><img src=""http://i.stack.imgur.com/sQltg.png"" alt=""Image""></p>

<p>Thank you all for your help!</p>
"
159,Is this language decidable?,"<p>I'm struggling with whether or not this is decidable: </p>

<p>A = {x is an element of the set of Natural Numbers | for every y greater than x, 2y is the sum of two primes}</p>

<p>I'm inclined to think that this is decidable given the fact that when fed into a Turing Machine, it will never reach an accept state and loop for infinity unless it rejects.  However, I also do know that for a language to be decidable, there must only exist an algorithm to decide it; we don't necessarily have to know how it's done.  With this, part of me think that it is decidable? Does anyone know how to prove either? </p>
"
160,Removing all comments from an .r file?,"<p>Is there an r code I can use that will remove all of the comments from an .r file?</p>

<p>(SO asks me to add more text - but I think the question is simple and self explanatory)</p>
"
161,Remove an arbitrary rotation axis from a rotation matrix,"<p>I'm currently working on a tilt compensated linear accelerometer on an android phone. What i basically want to reach is the acceleration, which is in the earth frame, rather than sensor frame. I also want make my vector gravity free. X-axis of the accelerometer will be alligned seperately with the x-axis of a land vehicle. Therefore yaw compensation (angle between the x axis of the phone and magnetic north) is not wanted.</p>

<p>For this we have 2 sensor readings:</p>

<p>1) 3x3 rotation matrix <strong><em>R</em></strong> (rotation from earth frame to sensor frame)</p>

<p>2) 3x1 acceleration vector <strong><em>a = (x,y,z)^T</em></strong> from sensor frame</p>

<p>To manually remove gravity from <strong>a_transposed</strong> we can simply define a gravity vector in earth frame:</p>

<p>g = (0,0,9.81)^T </p>

<p>I can now multiply gravity with rotation matrix R and substract it from <strong><em>a</em></strong> vector. As a result <strong><em>a'</em></strong> is the gravity free acceleration vector.</p>

<p>a' = a - (R * g)
(at this step a' has the same values as the software sensor LINEAR_ACCELERATION in API)</p>

<p>Until here everything works fine. </p>

<p>Now i want to rotate my linear acceleration vector <strong><em>a'</em></strong> in the earth frame <strong>without taking Z-axis rotation in to account</strong>. In order to do so first i calculate the inverse of the rotation matrix R, which is equal to the tranpose of it:</p>

<p>R^(-1) = R^T</p>

<p>Then i multiply <strong><em>a'</em></strong> with <strong><em>R^(-1)</em></strong> and rotate it back to earth frame.</p>

<p>a' = R^(-1) * a'</p>

<p>At this step I have a tilt compensated linear acceleration data, if my phone is facing magnetic north, because i have also rotated around z axis, which is not needed. I have to rerotate it around z-axis to get the final right result. For this purpose i have calculated the euler angle (rotation around z axis) from the rotation matrix, which is yaw. And rotated my vector once in the opposite direction by using the yaw angle. This should be the final correction step but there is a cost using euler angles.</p>

<p>This method does not work if i hold the phone directly upwards, beacause if the roll angle is equal to 90 degrees and it causes a gimbal lock. My calculated yaw angle is then <em>unknown</em> and my back-rotation fails.</p>

<p>My question is how to modify/change the rotation matrix so that i can only rotate around x and y axis by ignoring z axis, without using euler angles? </p>

<p>i have alread checked but it didnt helped me:
<a href=""http://stackoverflow.com/questions/5741711/how-do-i-remove-axis-from-a-rotation-matrix"">How do I remove axis from a rotation matrix?</a></p>
"
162,How can each element of a numpy array be operated upon according to its relative value?,"<p>Let say that we have an array</p>

<pre><code>a = np.array([10,30,50, 20, 10, 90, 0, 25])
</code></pre>

<p>The pseudo code for what I want -</p>

<pre><code>if a[x] &gt; 80 then perform funcA on a[x]
if 40 &lt; a[x] &lt;= 80 then perform funcB on a[x]
if a[x] &lt;= 40 then perform funcC on a[x]
</code></pre>

<p>What is the cleanest way to perform this using numpy functions?</p>
"
163,Emulate ggplot2 default color palette,"<p>What function can I use to emulate ggplot2's default color palette for a desired number of colors.  For example, an input of 3 would produce a character vector of HEX colors with these colors:
<img src=""http://i.stack.imgur.com/AVc1t.png"" alt=""enter image description here""></p>
"
164,Assigning multiple array indices at once in Python/Numpy,"<p>I'm looking to quickly (hopefully without a for loop) generate a Numpy array of the form:</p>

<pre><code>array([a,a,a,a,0,0,0,0,0,b,b,b,0,0,0, c,c,0,0....])
</code></pre>

<p>Where a, b, c and other values are repeated at different points for different ranges. I'm really thinking of something like this:</p>

<pre><code>import numpy as np
a = np.zeros(100)
a[0:3,9:11,15:16] = np.array([a,b,c])
</code></pre>

<p>Which obviously doesn't work. Any suggestions?</p>

<p>Edit (jterrace answered the original question):
The data is coming in the form of an N*M Numpy array. Each row is mostly zeros, occasionally interspersed by sequences of non-zero numbers. <strong>I want to replace all elements of each such sequence with the last value of the sequence. I'll take any fast method to do this!</strong> Using where and diff a few times, we can get the start and stop indices of each run.</p>

<pre><code>raw_data = array([.....][....])
starts = array([0,0,0,1,1,1,1...][3, 9, 32, 7, 22, 45, 57,....])
stops = array([0,0,0,1,1,1,1...][5, 12, 50, 10, 30, 51, 65,....])
last_values = raw_data[stops]
length_to_repeat = stops[1]-starts[1]
</code></pre>

<p>Note that starts[0] and stops[0] are the same information (which row the run is occurring on). At this point, since the only route I know of is what jterrace suggest, we'll need to go through some contortions to get similar start/stop positions for the zeros, then interleave the zero start/stop with the values start/stops, and interleave the number 0 with the last_values array. Then we loop over each row, doing something like:</p>

<pre><code>for i in range(N)
    values_in_this_row = where(starts[0]==i)[0]
    output[i] = numpy.repeat(last_values[values_in_this_row], length_to_repeat[values_in_this_row])
</code></pre>

<p>Does that make sense, or should I explain some more?</p>
"
165,ff package write error,"<p>I'm trying to work with a 1909x139352 dataset using R. Since my computer only has 2GB of RAM, the dataset turns out to be too big (500MB) for the conventional methods. So I decided to use the <code>ff</code> package. However, I've been having some troubles. The function <code>read.table.ffdf</code> is unable to read the first chunk of data. It crashes with the next error:</p>

<pre><code>txtdata &lt;- read.table.ffdf(file=""/directory/myfile.csv"", 
                           FUN=""read.table"", 
                           header=FALSE, 
                           sep="","", 
                          colClasses=c(""factor"",rep(""integer"",139351)), 
                          first.rows=100, next.rows=100, 
                          VERBOSE=TRUE)

  read.table.ffdf 1..100 (100)  csv-read=77.253sec
  Error en  ff(initdata = initdata, length = length, levels = levels, ordered = ordered,  : 
   write error
</code></pre>

<p>Does anyone have any idea of what is going on?</p>
"
166,Detecting rare incidents from multivariate time series intervals,"<p>Given a time series of sensor state intervals, how do I implement a classifier which learns from supervised training data to detect an incident based on a sequence of state intervals? To simplify the problem, sensor states are reduced to either <code>true</code> or <code>false</code>.</p>

<p><strong>Update:</strong> I've found <a href=""http://fuzzy.cs.uni-magdeburg.de/wiki/uploads/Mitarbeiter.Kempe/Mining_Sequences_of_Temporal_Intervals.pdf"" rel=""nofollow"">this paper (PDF)</a> on <em>Mining Sequences of Temporal Intervals</em> which addresses a similar problem. <a href=""http://docs.google.com/viewer?a=v&amp;q=cache%3azBDmsljfWmUJ%3aciteseerx.ist.psu.edu/viewdoc/download%3Fdoi%3D10.1.1.88.603%26rep%3Drep1%26type%3Dpdf+Mining+Hierarchical+Temporal+Patterns+in+Multivariate+Time+Series+site%3a.edu&amp;hl=en&amp;pid=bl&amp;srcid=ADGEESivIUmQ69g2JVOM0G07NVLuehGh8Nn95DkffNcctH6_u8xti38b7WQadxgfEX8Nh1gvOcRHha_QRpDqGuf1uM8wZeZCcVFzC-MjNXxmC-GhEWCxLadKiBApOx-6BRjxD-8UU5i0&amp;sig=AHIEtbRcfxh9KxfqyOMxfePG7aXtRPpQVA"" rel=""nofollow"">Another paper (Google Docs)</a> on <em>Mining Hierarchical Temporal Patterns in Multivariate Time Series</em> takes a novel approach, but deals with hierarchical data.</p>

<h2>Example Training Data</h2>

<p>The following data is a training example for an incident, represented as a graph over time, where <code>/\</code> represents a <code>true</code> state interval and <code>\___/</code> a <code>false</code> state interval for a sensor.</p>

<pre><code> Sensor   |  Sensor State over time
          |  0....5....10...15...20...25...  // timestamp
 ---------|--------------------------------
 A        |  \________/
 B        |  \___________________/
 C        |  ______________________________  // no state change
 D        |  /\_/\_/\_/\_/\_/\_/\_/
 E        |  _________________/\___
</code></pre>

<h2>Incident Detection vs Sequence Labeling vs Classification</h2>

<p>I initially generalised my problem as a two-category sequence labeling problem, but my categories really represented ""normal operation"" and a rare ""alarm event"" so I have rephrased my question as incident detection. Training data is available for ""normal operation"" and ""alarm incident"".</p>

<p>To reduce problem complexity, I have discretized sensor events to boolean values, but this need not be the case.</p>

<h2>Possible Algorithms</h2>

<p>A hidden Markov model seems to be a possible solution, but would it be able to use the state intervals? If a sequence labeler is not the best approach for this problem, alternative suggestions would be appreciated.</p>

<h2>Bayesian Probabilistic Approach</h2>

<p>Sensor activity will vary significantly by time of day (busy in mornings, quiet at night). My initial approach would have been to measure normal sensor state over a few days and calculate state probability by time of day (hour). The combined probability of sensor states at an unlikely hour surpassing an ""unlikelihood threshold"" would indicate an incident. But this seemed like it would raise a false alarm if the sensors were noisy. I have not yet implemented this, but I believe that approach has merit.</p>

<h2>Feature Extraction</h2>

<p>Vector states could be represented as state interval changes occurring at a specific time and lasting a specific duration.</p>

<pre><code>struct StateInterval
{
    int sensorID;
    bool state;
    DateTime timeStamp;
    TimeSpan duration; 
}
</code></pre>

<p>eg. Some State Intervals from the process table:</p>

<pre><code>[ {D, true, 0, 3} ]; [ {D, false, 4, 1} ]; ...
[ {A, true, 0, 12} ]; [ {B, true, 0, 6} ]; [ {D, true, 0, 3} ]; etc.
</code></pre>

<p>A good classifier would take into account state-value intervals and recent state changes to determine if a combination of state changes closely matches training data for a category.</p>

<p><strong>Edit:</strong> Some ideas after sleeping on how to extract features from multiple sensors' alarm data and how to compare it to previous data...</p>

<p>Start by calculating the following data for each sensor for each hour of the day:</p>

<ul>
<li>Average state interval length (for <code>true</code> and <code>false</code> states)</li>
<li>Average time between state changes</li>
<li>Number of state changes over time</li>
</ul>

<p>Each sensor could then be compared to every other sensor in a matrix with data like the following:</p>

<ul>
<li>Average time taken for sensor B to change to a true state after sensor A did. If an average value is 60 seconds, then a 1-second wait would be more interesting than a 120-second wait.</li>
<li>Average number of state changes sensor B underwent while sensor A was in one state</li>
</ul>

<p>Given two sets of training data, the classifier should be able to determine from these feature sets which is the most likely category for classification.</p>

<p>Is this a sensible approach and what would be a good algorithm to compare these features?</p>

<hr>

<p><strong>Edit:</strong> the direction of a state change (<code>false-&gt;true</code> vs <code>true-false</code>) is significant, so any features should take that into account.</p>
"
167,knitr multiple plot alignment,"<p>Here is a minimal example:</p>

<pre><code>\documentclass{article}

\begin{document}
&lt;&lt;fig.height=4,out.width=\textwidth,echo=FALSE&gt;&gt;=
plot(1,1)
plot(2,2)
@ 

\newpage
&lt;&lt;fig.height=4,out.width=\textwidth,echo=TRUE&gt;&gt;=
plot(1,1)
plot(2,2)
@ 

\end{document}
</code></pre>

<p>Note the screenshots below.
If <code>echo=FALSE</code>, horizontal alignment is off. Top plot is shifted to the right relative to the bottom plot.
If <code>echo=TRUE</code>, horizontal alignment is correct</p>

<p><img src=""http://i.stack.imgur.com/QMJ1x.png"" alt=""horizontal alignment is wrong""></p>

<p><img src=""http://i.stack.imgur.com/BDzjX.png"" alt=""horizontal alignment is correct""></p>

<p>How would I fix the first example to align correctly with <code>echo=FALSE</code>?</p>

<p>Thank you!</p>
"
168,"Best Free C# Math Parser using variables, user defined functions, custom operators","<p>I'm looking for a .NET Math Parser which uses variables, custom operators and user defined functions...
Since today i've used muParser (there's a wrapped version for .NET), but i noticed it is too slow!</p>

<p>Does anybody knows another Math Parser (FREE!) that works pretty good?</p>

<p>I tried NCalc, but it doesn't have variables, so it fails..</p>

<p>Thank you very much!</p>
"
169,selecting non-consecutive columns in R tables,"<p>Let's say I have a some table T. </p>

<p>Assume T has columns 1,2,3,4,5. How do I select, for example, any arbitrary subset of columns? Let's say I want to select column 1 and [start:stop] where start and stop are indices that I define - how do I go about doing this? Another type of selection I may want to do, and not sure how to, is selecting random columns from T. </p>

<p>Now, I understand how to select any consecutive subset of columns and store them as a new table. </p>

<p>For that, I would use
    newT &lt;- T[,start:stop]</p>

<p>or something similar. Now how to select arbitrary, non consecutive columns as stated above?</p>
"
170,Formatting numbers consistently in Python,"<p>I have a series of numbers:</p>

<pre><code>from numpy import r_
r_[10**(-9), 10**(-3), 3*10**(-3), 6*10**(-3), 9*10**(-3), 1.5*10**(-2)]
</code></pre>

<p>and I would like to have them displayed in a plot's legend in the form:</p>

<pre><code>a 10^(b)
</code></pre>

<p>(with <code>^</code> meaning superscript)</p>

<p>so that e.g. the third number becomes <code>3 10^(-3)</code>.</p>

<p>I know I have to use Python's string formatting operator <code>%</code> for this, but I don't see a way to do this. Can someone please show me how (or show me an alternative way)?</p>
"
171,Order statistics and their CDF,"<p>Here's a problem that seems rather peculiar to me. This time I have no initial idea about how to solve it.</p>

<p>Let $X_1$, ..., $X_n$ be independent, real valued random variables with density $f$ and CDF $F$. Let $F_i$ denote the CDF of $X_{(i)}$.</p>

<p>a) What is the distribution of $F(X_{(i)})$ and $F_i(X_{(i)})$?</p>

<p>b) What is the variance of $F(X_{(i)})$?</p>

<p>[edit: I have realised by now that my approach was flawed and furthermore that my question needs clarification. So here is my second try.]</p>

<p>As I understand the question, we have real valued, i.i.d. random variables $X_1$, ..., $X_n$ with density $f$ and CDF $F$.</p>

<p>$X_{(1)}&lt; ... &lt; X_{(n)}$ are the corresponding order statistics with CDF $F_i:=F_{X_{(i)}}$.</p>

<p>I know the general formula for the CDF of order statistics. It is given by $$F_i(t)=\sum_{k=i}^n \binom{n}{k}F(t)^k(1-F(t))^{n-k}.$$</p>

<p>Now, the CDF of a random variable is a measurable function. Thus $F(X_{(i)})$ and $F_i(X_{(i)})$ are real valued random variables again. And a) asks for their distribution.</p>

<p>Per definition, we have $$F(t)=\mathbb{P}(X_i\leq t).$$ Hence $$F(X_{(i)})=\mathbb{P}(X_i\leq X_{(i)}).$$ This is the probability of the event that any $X_i$ is less or equal $X_{(i)}$. By definition of order statistics, we have $$F(X_{(n)})=\mathbb{P}(X_i\leq X_{(n)})=1,$$ as $X_{(n)}$ is the maximum of the $X_i$. But how do I derive the distribution of $F(X_{(i)})$ for $i \in \{1, ..., n-1\}$?</p>

<p>I think, if they were uniformly distributed, the answer would simply be $$F(X_{(i)})=i/n.$$ But their distribution ist unknown, so I'm stuck and have no idea how to proceed from here.</p>
"
172,Limits on NLME regression?,"<p>I am currently using <code>nlme</code> to perform mixed-effects regression.</p>

<p>I would like to perform constrained  optimization by providing upper and lower bounds to the parameters within the call to <code>nlme</code>.</p>

<p>Is this possible?</p>
"
173,what is the practical use of a hyper graph?,"<p>what is the practical use of a <a href=""http://en.wikipedia.org/wiki/Hypergraph"" rel=""nofollow"">hyper graph</a>? Is there any example application that uses hypergraph?</p>
"
174,Formula notation in boxplot (graphics) vs bwplot (lattice),"<p>This has been nagging me since I posted an answer to <a href=""http://stackoverflow.com/q/11495406/1270695"">this question</a> yesterday.</p>

<p>Consider the following data:</p>

<pre><code>carpaint &lt;- data.frame(paint = c(rep(c(""blue"", ""black"", ""red""), 
                                     times=3)),
                       car1 = c(100, 138, 123, 143, 112, 
                                144, 343, 112, 334), 
                       car2 = c(111, 238, 323, 541, 328, 
                                363, 411, 238, 313), 
                       car3 = c(432, 123, 322, 342, 323, 
                                522, 334, 311, 452))
</code></pre>

<p>If I wanted to generate boxplots by color (ignoring secondary grouping by ""car""), I can easily use the formula notation in <code>bwplot</code> from <code>lattice</code>.</p>

<pre><code>library(lattice)
bwplot(car1 + car2 + car3 ~ paint, data=carpaint)
</code></pre>

<p><img src=""http://i.stack.imgur.com/ICUVA.png"" alt=""Lattice Output""></p>

<p>However, to get a similar plot using <code>boxplot</code> from <code>graphics</code>, the formula notation is different, so I have to first convert the data to a long format and then plot, like this:</p>

<pre><code>carpaint.l = reshape(carpaint, direction=""long"", varying=2:4, sep="""")
boxplot(car ~ paint, data=carpaint.l)
</code></pre>

<p><img src=""http://i.stack.imgur.com/zhO11.png"" alt=""Graphics Output""></p>

<p><strong>The Question</strong>: Is there a way to get the same plot with <code>boxplot</code> from <code>graphics</code> without first reshaping the data, preferably using formula notation? (This is ignoring the fact that this might not even be the type of output the OP of the linked question had wanted--they still haven't commented on my answer or the question by @RomanLutrik. I'm just trying to satisfy my curiosity and learn to understand formula notation better.)</p>

<blockquote>
  <blockquote>
    <p><strong>Note</strong>: providing alternatives is definitely welcome, but I'm hoping that any answers also help me understand why some options are working and others are not. For instance, why does <code>boxplot(as.matrix(carpaint[, 2:4]) ~ carpaint$paint)</code> work but <code>boxplot(carpaint[, 2:4] ~ carpaint$paint)</code> does not. I find that strange because the documentation for <code>boxplot</code> has examples where they convert matrices to data frames before plotting even though it does not seem to be necessary.</p>
  </blockquote>
</blockquote>
"
175,Academic Studies Into Benefits of Team Work in Software Development,"<p>I need to find some good statistics from reputable sources on the benefits of software developers working in a team.  </p>

<p>We use Scrum - so anything linked to this in particular would be really beneficial but not essential.</p>

<p>I have 5 developers of differing skill levels and I feel that they need to work as a team.  However, if my boss had his way they would all have a large single project each (or worse, multiple projects on the go at the same time) each.  It's a constant battle.</p>

<p>I am looking to put facts in front of him that are difficult or impossible to argue with.  Things like 'working as a team increases productivity by xx% would be perfect.  Increases morale by xx%, reduces bugs by xx% - you get the idea.</p>

<p>There are some posts already here that may help and I will go through them but I don't think they're exactly what I'm looking for.</p>

<p>Some may feel they want to side with my boss. In the interests of good discussion this is OK.  However, I know the dynamics within my team and Organization and am convinced that team working is the best route for us.  I'm unlikely to change my mind :)</p>

<p>Any help would really be appreciated - before my guys walk out on me :)</p>
"
176,plotting using a for loop,"<p>I want to plot a number of graph using this for loop. however, I can only get one output (foo0001).</p>

<pre><code>for (i in 1:5) {
 bitmap(""foo%03d.jpg"")
 plot(runif(20), ylim = c(0, 1))
 dev.off()
}
</code></pre>

<p>please help!</p>
"
177,find a point which perpendicular to given line,"<p>I want to find a point z(x3,y3) which perpendicular to given line. In my example I am given 2 coordinates A(x1 , y1) and B(x2 , y2). I want to find the point z which is perpendicular(AZ) to AB line and distance (h) from point B. ABZ angle is 90.
here is my c++ code.</p>

<p><code>double AB_slope = m; // know it</code></p>

<p>//find z point which perpendicular to AB line</p>

<p><code>double AZ_slope = - 1/m;</code></p>

<p><code>double x3 = x2 + prescribed_distance * dx;</code></p>

<p><code>double y3 = y2 + prescribed_distance * dy;</code></p>

<p>But I don't know to find dx , dy and prescribed_distance. please help me.</p>
"
178,Basic render 3D perspective projection onto 2D screen with camera (without opengl),"<p>Let's say I have a data structure like the following:</p>

<pre><code>Camera {
   double x, y, z

   /** ideally the camera angle is positioned to aim at the 0,0,0 point */
   double angleX, angleY, angleZ;
}

SomePointIn3DSpace {
   double x, y, z
}

ScreenData {
   /** Convert from some point 3d space to 2d space, end up with x, y */
   int x_screenPositionOfPt, y_screenPositionOfPt

   double zFar = 100;

   int width=640, height=480
}
</code></pre>

<p>...</p>

<p>Without screen clipping or much of anything else, how would I calculate the screen x,y position of some point given some 3d point in space.  I want to project that 3d point onto the 2d screen.</p>

<pre><code>Camera.x = 0
Camera.y = 10;
Camera.z = -10;


/** ideally, I want the camera to point at the ground at 3d space 0,0,0 */
Camera.angleX = ???;
Camera.angleY = ????
Camera.angleZ = ????;

SomePointIn3DSpace.x = 5;
SomePointIn3DSpace.y = 5;
SomePointIn3DSpace.z = 5;
</code></pre>

<p>ScreenData.x and y is the screen x position of the 3d point in space.  How do I calculate those values?</p>

<p>I could possibly use the equations found here, but I don't understand how the screen width/height comes into play.  Also, I don't understand in the wiki entry what is the viewer's position vers the camera position.</p>

<p><a href=""http://en.wikipedia.org/wiki/3D_projection"">http://en.wikipedia.org/wiki/3D_projection</a></p>
"
179,Combine values of data.frame,"<p>I'm R-beginner, I assume this is not so difficult. I want to combine values of a <code>data.frame</code>, like this:</p>

<p>Input data.frame:</p>

<pre><code>  col1 col2
1 a   50
2 a   80
3 b   40
4 c   20 
</code></pre>

<p>Output data.frame:</p>

<pre><code> col1 col1
1 a [50,80]
2 b [40]
3 c [20]
</code></pre>

<p>In the input, <code>col1</code> is not unique. For each value in <code>col1</code>, I want to combine all values in <code>col2</code> in a vector. In the output, <code>col1</code> is unique.</p>

<p>Can you help me with this?</p>
"
180,"""Aggregating"" non-numeric variables in Reshape","<p>I have a dataset in long format and would like to convert it into a wide format using Reshape or any pre-processing prior to Reshape. The difficulty is that ""value"" variable is non-numeric. Note there is a legit duplicate record in the original data as well. The following code shows data layout for each. </p>

<pre><code>id = c(1, 1, 1, 1, 1, 1, 1)
month &lt;- c(""jan"", ""feb"", ""feb"", ""march"", ""april"", ""april"", ""april"")
stress &lt;- c(""mild"", ""mild"", ""high"", ""moderate"", ""mild"", ""high"", ""mild"")
Longdata &lt;- data.frame(id, month, stress, stringsAsFactors = FALSE)
</code></pre>

<p>This is the orginal format:</p>

<pre><code>&gt; Longdata
  id month   stress
1  1   jan     mild
2  1   feb     mild
3  1   feb     high
4  1 march moderate
5  1 april     mild
6  1 april     high
7  1 april     mild
</code></pre>

<p>This is how I would like to get data organized:</p>

<pre><code>id &lt;- c(1)
jan &lt;- c(""mild"")
feb &lt;- c(""mild-high"")
march &lt;- c(""moderate"")
april &lt;- c(""mild-high-mild"")
widedata &lt;- data.frame(id, jan, feb, march, april, stringsAsFactors = FALSE)
&gt; widedata
  id  jan       feb    march          april
1  1 mild mild-high moderate mild-high-mild
</code></pre>
"
181,ggplot2: Legend overlapping the plot area - is it possible to manually adjust legend position?,"<p>Lets say I have a dataset about carrot yield from different fields and different breeds:</p>

<pre><code>carrots&lt;-list(Yield=c(345,226,74,559,288,194),
          Field=c(""A"",""B"",""C"",""D"",""E"",""F""),
          Breed=rep(c(""Long"",""Short""),each=3))
carrots&lt;-data.frame(carrots)
</code></pre>

<p>I want to plot a bar plot showing the yield for each field, coloured by breed:</p>

<pre><code>ggplot(carrots,aes(y=Yield,x=Field,fill=Breed)) +
   geom_bar() +
   opts(legend.direction = ""horizontal"",
        legend.position = ""top"") +
   labs(fill="""")
</code></pre>

<p>But the legend is always slightly overlapping the plot area:</p>

<p><img src=""http://users.utu.fi/susjoh/Rplot.png"" alt=""plot with slight legend overlap""></p>

<p>I've tried manually adjusting the legend position to be outside the plot area, such as with</p>

<pre><code>opts(legend.position=c(0.5,1.1)
</code></pre>

<p>but then the plot margins cut off the legend and I'm not sure how I can adjust them. Is there a more subtle solution to this problem? </p>
"
182,What is the math behind this ray-like animation?,"<p>I have unobfuscated and simplified <a href=""http://www.p01.org/releases/128b_raytraced_checkboard/128b.htm?utm_source=javascriptweekly&amp;utm_medium=email"">this animation</a> into a jsfiddle available <a href=""http://jsfiddle.net/fEMfm/"">here</a>. Nevertheless, I still don't quite understand the math behind it.</p>

<p>Does someone have any insight explaining the animation?</p>
"
183,Replace values based on value in another column,"<p>Ok I have a data frame like this:</p>

<pre><code>   A          B
A -0.3855906  0.78
B  0.7186365  0.02
C -0.5591167  0.17304717
D  1.6098104  0.01
E -1.7616215  0.09
F  0.0000000  0.00
</code></pre>

<p>I want to make the values in column A 0 if the value in B is greater than 0.05. I've tried various different things, this is what I thought was closest.</p>

<pre><code>y$A[y$B &gt; 0.05] = 0
</code></pre>

<p>I also tried if statements to no avail.</p>

<p>Thanks in advance!</p>
"
184,how do i convert speed and bearing to azimuth and elevation in c#?,"<p>i have no clue to it. can someone help me out?</p>
"
185,Matlab:K-means clustering,"<p>I have a matrice of A(369x10) which I want to cluster in 19 clusters.
I use this method</p>

<pre><code>[idx ctrs]=kmeans(A,19)
</code></pre>

<p>which yields
idx(369x1) and ctrs(19x10)</p>

<p>I get the point up to here.All my rows in A is clustered in 19 clusters.</p>

<p>Now I have an array B(49x10).I want to know where the rows of this B corresponds in the among given 19 clusters.</p>

<p>How is it possible in MATLAB?</p>

<p>Thank you in advance</p>
"
186,Numpy index out of range error,"<p>I'm using this code :</p>

<pre><code>r = mlab.csv2rec(datafile, delimiter=';')

fig = plt.figure()
fig.subplots_adjust(bottom=0.2)
ax = fig.add_subplot(111)
ax.plot(r.date, r.close)
</code></pre>

<p>but it's returning this :</p>

<blockquote>
  <p>ax.plot(r.date, r.close)</p>
  
  <p>IndexError: index out of range for array</p>
</blockquote>

<p>How do I make sure that I`m staying inside the array range ?</p>

<p>if I print out len(r.date) and len(r.close) they are both returning : 500</p>

<hr>

<p>EDIT, this is a sample code from matplotlib, using a npy file, I'd like to do the same for e CSV file :</p>

<pre><code>datafile = cbook.get_sample_data('goog.npy')
r = np.load(datafile).view(np.recarray)

fig = plt.figure()
ax = fig.add_subplot(111)
ax.plot(r.date, r.adj_close)
</code></pre>

<hr>

<p>EDIT, full error log:</p>

<pre><code>Traceback (most recent call last):
  File ""main02.py"", line 66, in &lt;module&gt;
    ax.plot(r['date'], r['close'])
  File ""/usr/lib/python2.6/site-packages/matplotlib/axes.py"", line 3788, in plot
    self.autoscale_view(scalex=scalex, scaley=scaley)
  File ""/usr/lib/python2.6/site-packages/matplotlib/axes.py"", line 1824, in autoscale_view
    y0, y1 = ylocator.view_limits(y0, y1)
  File ""/usr/lib/python2.6/site-packages/matplotlib/ticker.py"", line 1170, in view_limits
    return np.take(self.bin_boundaries(dmin, dmax), [0,-1])
  File ""/film/tools/PythonExtensions/v41/py26_linux-x64/numpy/core/fromnumeric.py"", line 103, in take
    return take(indices, axis, out, mode)
IndexError: index out of range for array
</code></pre>
"
187,R create strings in a loop,"<p>How can I create variables and strings using the value passed by the loop?</p>

<hr>

<p><strong>Example</strong></p>

<p>I have a set of countries = c('USA','Canada','Mexico'). </p>

<p>I have a dataframe 'population'. </p>

<p>I want to query my database to get their population, and assign it to a column in a data frame. Don't worry about accessing the database, I am only concerned with dynamically creating the query string and the dataframe column name.</p>

<pre><code>for (country in countries) {
   query = ""SELECT population FROM population_database WHERE location='country';""
   population$country = mysql_query(query) 
}
</code></pre>
"
188,Tool to do computer math,"<p>For example, just now I compared file permissions in C.</p>

<p>Turns out, the permissions were 33216!</p>

<p>After some total confusion, with this</p>

<pre><code>echo $(printf %o 33216) % 100000 | bc
</code></pre>

<p>I got it back to the familiar 700.</p>

<p>My first thought was to use units. I think units is excellent, so I guess I'm looking for units but with a tilt towards computing.</p>

<p>Another example of usage could be, you enter % and HTML, and you get <code>&amp;#37;</code></p>
"
189,"optimization function in R that can accept objective, gradient, AND hessian?","<p>I have a complex objective function I am looking to optimize. The optimization problem takes a considerable time to optimize. Fortunately, I do have the gradient and the hessian of the function available.</p>

<p>Is there an optimization package in R that can take all three of these inputs? The class 'optim' does not accept the Hessian. I have scanned the <a href=""http://cran.r-project.org/web/views/Optimization.html"">CRAN task page for optimization</a> and nothing pops.</p>

<p>For what it's worth, I am able to perform the optimization in MATLAB using '<a href=""http://www.mathworks.com/help/toolbox/optim/ug/fminunc.html"">fminunc</a>' with the  the 'GradObj' and 'Hessian' arguments.</p>
"
190,Interval search on a data frame,"<p>I've a data frame of time series and values. The time series are seconds from the epoch. Here is how the top few elements look like in that data frame</p>

<pre><code>val = seq(1,19)
ts = seq(1342980888,1342982000,by=60)
x = data.frame(ts = ts,val = val)
head(x)

      ts val
1 1342980888   1
2 1342980948   2
3 1342981008   3
4 1342981068   4
5 1342981128   5
6 1342981188   6
</code></pre>

<p>I would like to some kind of an interval search function which takes in as input a time stamp say 1342980889 (+1 the the ts in the first row) and it should return 1,2 (the row number) as output. Basically, I want to find the two rows which have time stamps that bracket the input time stamp 1342980889. While this is relatively easy to do using ""which"", I suspect ""which"" does a vector scan and as the real data frame is quite large I want to do it using a binary search. Thanks much in advance</p>
"
191,Test if S4 object is an instance,"<p>How do I test if an object is a <strong>instance</strong> of an S4 class (not sure if this is the right terminology)? I know about the function <code>isS4</code> however, this also returns true for class defintions, etc. E.g.:</p>

<pre><code>traj &lt;- setClass(
  Class=""Trajectories"",
  representation=representation(
    times = ""numeric"",
    traj = ""matrix""
  )
)

trajclass &lt;- getClass(""Trajectories"")
trajobject &lt;- new(Class=""Trajectories"",times=c(1,3),traj=matrix(1:4,ncol=2))

isS4(traj)
isS4(trajclass)
isS4(trajobject)
</code></pre>

<p>I am only interested in objects containing data, <code>trajobject</code> in this case; not so much in methods or class definitions. Is there a native fuction that I can use to test if an S4 object is an actual object? I.e. when using <code>print(object)</code> the output starts with:</p>

<p><em>An object of class ""foo"".....</em></p>
"
192,"R, removing the bagged samples to generate out of bag sample","<p>I'm trying to set up voting in a bagged model based on performance on the out of bag sample.</p>

<pre><code>construct.annet = function(trainset,n,p=1){
  annet.struct = vector(mode=""list"",length=n)
  cat(""Constructing Agregate Neural Network with "",p,""\n"")
  for(i in 1:n){
    cat(""iteration "",i,""\n"")
    bsamp = trainset[sample(p*dim(trainset)[1],replace=T),]
    annet.struct[[i]] = nnet(class~.,data=bsamp,size=sample(4:12,1),maxit=1000)
  }
  return(annet.struct)
}
</code></pre>

<p>Printing iterations is just to tell me how long things are taking.  As far as why I'm randomly varying the size of the hidden layer, it seemed like the thing to do at the time.</p>

<p>What I want to do is add in another line after building the model on each iteration, where I test the model on the out of bag sample, and then record its predictive accuracy.  Then I'll use that data to weight the class percentage votes on the final model. (lower performing models get less weight, etc)</p>

<p>The trouble is, I can't figure out how to remove the bootstrapped samples from the incoming data.  And my google-fu clearly isn't helping.</p>

<p>Thanks.</p>
"
193,Multi-Exponentiation Implementation,"<p>Is anyone aware of an implemented multi-exponentiation algorithm?  I'm looking for something that given vectors A, B would compute the product of A[i]^B[i] using some of the fast algorithms out there.</p>

<p>Thanks!</p>
"
194,Normal Bayes implementation in OpenCV,"<p>I'm a newbie to Machine Learning. I have a question about how Normal Bayes is implemented in OpenCV. </p>

<p>I have a mis-understanding regarding the terms <strong>Normal Bayes</strong> and <strong>Naive Bayes</strong>.</p>

<p>This <a href=""http://www.emgu.com/wiki/index.php/Normal_Bayes_Classifier_in_CSharp"" rel=""nofollow"">site</a> tells that <strong>Normal Bayes</strong> and <strong>Naive Bayes</strong> mean the same.</p>

<p>The <a href=""http://opencv.itseez.com/modules/ml/doc/normal_bayes_classifier.html"" rel=""nofollow"">NormalBayes</a> documentation on <strong>OpenCV website</strong> specifies that the features are Normally distributed and not necessarily independent.</p>

<p>The <a href=""http://en.wikipedia.org/wiki/Naive_Bayes_classifier"" rel=""nofollow"">wikipedia</a> article on Naive Bayes classifier tells us that it is assumed that features are independent. Therefore, <strong>Covariance Matrix</strong> need not be determined. </p>

<p>However, when I look at the source of the implementation of Normal Bayes classifier, it <strong>does calculate</strong> Covariance Matrix.</p>

<p>I also found a similar question over <a href=""http://old.nabble.com/Normal-Bayes-Implementation--td33253038.html"" rel=""nofollow"">here</a> which wasn't answered.</p>

<p><strong>Am I missing something here? or is it that Normal Bayes classifier in OpenCV is not a standard Naive Bayes classifier?</strong></p>
"
195,Methods for quickly calculating standard deviation of large number set in Numpy,"<p>What's the best(fastest) way to do this?</p>

<p><img src=""http://i.stack.imgur.com/2ZvDZ.png"" alt=""question""></p>

<p>This generates what I believe is the correct answer, but obviously at N = 10e6 it is painfully slow. I think I need to keep the Xi values so I can correctly calculate the standard deviation, but are there any techniques to make this run faster?</p>

<pre><code>def randomInterval(a,b):
    r = ((b-a)*float(random.random(1)) + a)
    return r 

N = 10e6
Sum = 0
x = []
for sample in range(0,int(N)):
    n = randomInterval(-5.,5.)
    while n == 5.0:
        n = randomInterval(-5.,5.) # since X is [-5,5)
    Sum += n
    x = np.append(x, n)

A = Sum/N

for sample in range(0,int(N)):
    summation = (x[sample] - A)**2.0

standard_deviation = np.sqrt((1./N)*summation)
</code></pre>
"
196,R: Why this strange ccf result with xts data,"<p>I saw <a href=""https://stat.ethz.ch/pipermail/r-sig-finance/2011q4/008924.html"" rel=""nofollow"">here</a> that you should use <code>drop</code> when passing a (single-column) XTS object to the <code>ccf</code> (cross-correlation) function. (The sample data is quite big, so I put it <a href=""https://gist.github.com/3291932"" rel=""nofollow"">in a gist</a>)</p>

<pre><code>library(xts)
gist=""https://gist.github.com/raw/3291932""
tmp1=dget(file.path(gist,""e620647218626929b4ee370a05aa7748b2f9a32b/tmp1.txt""))
tmp2=dget(file.path(gist,""49b732db3eafa52f96006e3b1bb0be28380f5df0/tmp2.txt""))
ccf(drop(tmp1),drop(tmp2)) #Weird?
</code></pre>

<p>I expected a small peak around lag=0, with mostly noise either side. Instead I got a straight line:</p>

<p><img src=""http://i.stack.imgur.com/JOIR7.png"" alt=""ccf on all 400 bars""></p>

<p>That was 400 bars. I got the same kind of line on my full data of thousands of bars. But if I use just the tail-end 100 bars of that data I get something closer to what I expected: (50 bars looks even more plausible)</p>

<p><img src=""http://i.stack.imgur.com/j3Co7.png"" alt=""ccf for just the last 100 bars""></p>

<p>I'm a bit stumped if this is a <code>ccf</code> bug, a problem with the way I use xts objects, my misunderstanding of what <code>ccf</code> is doing, or I've magically discovered the formula to beat the stock market...</p>
"
197,Computational query complexity of SQ-learning,"<p>It is known that for PAC learning, there are natural concept classes (e.g. subsets of decision lists) for which there are polynomial gaps between the sample complexity needed for information theoretic learning by a computationally unbounded learner, and the sample complexity needed by a polynomial-time learner. (see, e.g. <a href=""http://portal.acm.org/citation.cfm?id=267489&amp;dl=GUIDE"">http://portal.acm.org/citation.cfm?id=267489&amp;dl=GUIDE</a> or <a href=""http://portal.acm.org/citation.cfm?id=301437"">http://portal.acm.org/citation.cfm?id=301437</a>)</p>

<p>These results seem to depend on encoding a secret in particular examples, however, and so don't naturally translate into the SQ-model of learning, where the learner just gets to query statistical properties of the distribution. </p>

<p>Is it known whether there exist concept classes for which information-theoretic learning in the SQ model is possible with O(f(n)) queries, but computationally efficient learning is only possible with Omega(g(n)) queries for g(n) >> f(n)?</p>
"
198,Numpy: checking if an element in a multidimensional array is in a tuple,"<p>It seems I still struggle with <a href=""http://stackoverflow.com/questions/7989676/numpy-need-a-hand-in-understanding-what-happens-with-the-in-operator/7989956#7989956"">the ""in"" operator in numpy</a>. Here's the situation:</p>

<pre><code>&gt;&gt;&gt; a = np.random.randint(1, 10, (2, 2, 3))
&gt;&gt;&gt; a
array([[[9, 8, 8],
        [4, 9, 1]],

       [[6, 6, 3],
        [9, 3, 5]]])
</code></pre>

<p>I would like to get the indexes of those triplets whose second element is in <code>(6, 8)</code>. The way I intuitively tried is:</p>

<pre><code>&gt;&gt;&gt; a[:, :, 1] in (6, 8)
ValueError: The truth value of an array with more than one element...
</code></pre>

<p><strong>My ultimate goal would be to insert at those positions the the number preceding those multiplied by two.</strong> Using the example above, <code>a</code> should become:</p>

<pre><code>array([[[9, 18, 8],   #8 @ pos #2 --&gt; replaced by 9 @ pos #1 by 2
        [4, 9, 1]],

       [[6, 12, 3],   #6 @ pos #2 --&gt; replaced by 6 @ pos #1 by 2
        [9, 3, 5]]])
</code></pre>

<p>Thank you in advance for your advice and time!</p>
"
199,Use plyr to compute margins,"<p>I have a data frame with something like the following structure:</p>

<pre><code>Trial Index    Condition1    Condition2    Measures
1              A             Y             ...
2              A             Y             ...        
3              B             Y             ...
4              B             Y             ...
5              A             Z             ...
6              A             Z             ...        
7              B             Z             ...
8              B             Z             ...
</code></pre>

<p>I would like to compute a number of summary measures on each combination of Condition1 and Condition2, <em>and</em> for the margins. I can use multiple calls to ddply to do this, but I was wondering if there is some simple way to get a single data structure out of it, something like:</p>

<pre><code>Condition1    Condition2    Mean    Median    ....
A             Y             ...     ...       ....
A             Z             ...     ...       ....
A             -             ...     ...       ....             
B             Y             ...     ...       ....
B             Z             ...     ...       ....
B             -             ...     ...       ....
-             Y             ...     ...       ....
-             Z             ...     ...       ....
</code></pre>
"
200,"In R, how to do nonlinear least square optimization which involves solving differential equations?","<h1>Update with reproducible examples to illustrate my problem</h1>

<p>My original question was ""Implementation of trust-region-reflective optimization algorithm in R"". However, on the way of producing a reproducible example(thanks @Ben for his advice), I realize that my problem is that in Matlab, one function <code>lsqnonlin</code> is good(meaning no need to choose a good starting value, fast) enough for most cases I have, while in R, there is not such a one-for-all function. Different optmization algorithm works well in different cases. Different algorithms reach different solutions. The reason behind this may not be that the optimization algorithms in R is inferior to the trust-region-reflective algorithm in Matlab, it could also be related to how R handles Automatic Differentiation. This problem comes actually from interrupted work two years ago. At that time, Prof. John C Nash, one of the authors of the package <strong>optimx</strong> has suggested that there has been quite a lot of work for Matlab for Automatic Differentiation, which might be the reason that the Matlab lsqnonlin performs better than the optimization functions/algorithms in R. I am not able to figure it out with my knowledge. </p>

<p>The example below shows some problems I have encountered(More reproducible examples are coming). To run the examples, first run <code>install_github(""KineticEval"",""zhenglei-gao"")</code>. You need to install package <strong>mkin</strong> and its dependencies and may also need to install a bunch of other packages for different optimization algorithms.</p>

<p>Basically I am trying to solve nonlinear least-squares curve fitting problems as described in the Matlab function <code>lsqnonlin</code> 's documentation (<a href=""http://www.mathworks.de/de/help/optim/ug/lsqnonlin.html"" rel=""nofollow"">http://www.mathworks.de/de/help/optim/ug/lsqnonlin.html</a>). The curves in my case are modeled by a set of differential equations. I will explain a bit more with the examples. Optimization algorithms I have tried including:</p>

<ul>
<li>Marq from <code>nls.lm</code>, the Levenburg-Marquardt</li>
<li>Port from <code>nlm.inb</code></li>
<li>L-BGFS-B from <code>optim</code></li>
<li>spg from <code>optimx</code></li>
<li><code>solnp</code> of package <strong>Rsolnp</strong></li>
</ul>

<p>I have also tried a few others but did not show here. </p>

<h2>A summary of my questions</h2>

<ul>
<li>Is there in R a reliable function/algorithm to use, like <code>lsqnonlin</code> in Matlab that can solve my type of nonlinear least square problems? (I could not find one.)</li>
<li>What is the reason that for a simple case, different optimization reach different solutions?</li>
<li>What makes <code>lsqnonlin</code> superior to the functions in R? The trust-region-reflective algorithm or other reasons?</li>
<li>Are there better ways to solve my type of problems, formulate differently? Maybe there is a much simple solution but I just don't know it. </li>
</ul>

<h2>Example 1: A simple case</h2>

<p>I will give the R codes first and explain later.
<img src=""http://i.stack.imgur.com/ah5jr.png"" alt=""Fitted Plot for Example 1""></p>

<pre><code>ex1 &lt;- mkinmod.full(
  Parent = list(type = ""SFO"", to = ""Metab"", sink = TRUE,
                k = list(ini = 0.1,fixed = 0,lower = 0,upper = Inf),
                M0 = list(ini = 195, fixed = 0,lower = 0,upper = Inf),
                FF = list(ini = c(.1),fixed = c(0),lower = c(0),upper = c(1)),
                time=c(0.0,2.8,   6.2,  12.0,  29.2,  66.8,  99.8, 127.5, 154.4, 229.9, 272.3, 288.1, 322.9),
                residue = c( 157.3, 206.3, 181.4, 223.0, 163.2, 144.7,85.0, 76.5, 76.4, 51.5, 45.5, 47.3, 42.7),
                weight = c( 1,  1,   1, 1, 1,   1,  1,     1,     1,     1,     1,     1,     1)),
  Metab = list(type = ""SFO"",
               k = list(ini = 0.1,fixed = 0,lower = 0,upper = Inf),
              M0 = list(ini = 0, fixed = 1,lower = 0,upper = Inf),
                    residue =c( 0.0,  0.0,  0.0,  1.6,  4.0, 12.3, 13.5, 12.7, 11.4, 11.6, 10.9,  9.5,  7.6),
               weight = c( 1,  1,   1, 1, 1,   1,  1,     1,     1,     1,     1,     1,     1))
  )
ex1$diffs
Fit &lt;- NULL
alglist &lt;- c(""L-BFGS-B"",""Marq"", ""Port"",""spg"",""solnp"")
for(i in 1:5) {
  Fit[[i]] &lt;- mkinfit.full(ex1,plot = TRUE, quiet= TRUE,ctr = kingui.control(method = alglist[i],submethod = 'Port',maxIter = 100,tolerance = 1E-06, odesolver = 'lsoda'))
  }
names(Fit) &lt;- alglist
kinplot(Fit[[2]])
(lapply(Fit, function(x) x$par))
unlist(lapply(Fit, function(x) x$ssr))
</code></pre>

<p>The output from the last line is:</p>

<pre><code>L-BFGS-B     Marq     Port      spg    solnp 
5735.744 4714.500 5780.446 5728.361 4714.499 
</code></pre>

<p>Except for ""Marq"" and ""solnp"", the other algorithms did not reach the optimum.Besides, 'spg' method (also other methods like 'bobyqa') need too many function evaluations for such a simple case . Moreover, if I change the starting value and make <code>k_Parent=0.0058</code> (the optimum value for that parameter) instead of the random choosen <code>0.1</code>,  ""Marq"" cannot find the optimum any more! (Code provided below). I have also had datasets where ""solnp"" does not find the optimum. However, if I use <code>lsqnonlin</code> in Matlab, I haven't encountered any difficulties for such simple cases. </p>

<pre><code>ex1_a &lt;- mkinmod.full(
  Parent = list(type = ""SFO"", to = ""Metab"", sink = TRUE,
                k = list(ini = 0.0058,fixed = 0,lower = 0,upper = Inf),
                M0 = list(ini = 195, fixed = 0,lower = 0,upper = Inf),
                FF = list(ini = c(.1),fixed = c(0),lower = c(0),upper = c(1)),
                time=c(0.0,2.8,   6.2,  12.0,  29.2,  66.8,  99.8, 127.5, 154.4, 229.9, 272.3, 288.1, 322.9),
                residue = c( 157.3, 206.3, 181.4, 223.0, 163.2, 144.7,85.0, 76.5, 76.4, 51.5, 45.5, 47.3, 42.7),
                weight = c( 1,  1,   1, 1, 1,   1,  1,     1,     1,     1,     1,     1,     1)),
  Metab = list(type = ""SFO"",
               k = list(ini = 0.1,fixed = 0,lower = 0,upper = Inf),
              M0 = list(ini = 0, fixed = 1,lower = 0,upper = Inf),
                    residue =c( 0.0,  0.0,  0.0,  1.6,  4.0, 12.3, 13.5, 12.7, 11.4, 11.6, 10.9,  9.5,  7.6),
               weight = c( 1,  1,   1, 1, 1,   1,  1,     1,     1,     1,     1,     1,     1))
  )

Fit_a &lt;- NULL
alglist &lt;- c(""L-BFGS-B"",""Marq"", ""Port"",""spg"",""solnp"")
for(i in 1:5) {
  Fit_a[[i]] &lt;- mkinfit.full(ex1_a,plot = TRUE, quiet= TRUE,ctr = kingui.control(method = alglist[i],submethod = 'Port',maxIter = 100,tolerance = 1E-06, odesolver = 'lsoda'))
  }
names(Fit_a) &lt;- alglist
lapply(Fit_a, function(x) x$par)
unlist(lapply(Fit_a, function(x) x$ssr))
</code></pre>

<p>Now the output from last line is:</p>

<pre><code>L-BFGS-B     Marq     Port      spg    solnp 
5653.132 4866.961 5653.070 5635.372 4714.499 
</code></pre>

<p>I will explain what I am optimising here. If you have run the above script and see the curves, we use a two-compartment model with first order reactions to describe the curves. The differential equations to express the model are in <code>ex1$diffs</code>:</p>

<pre><code>                                                             Parent 
                                    ""d_Parent = - k_Parent * Parent"" 
                                                               Metab 
""d_Metab = - k_Metab * Metab + k_Parent * f_Parent_to_Metab * Parent"" 
</code></pre>

<p>For this simple case, from the differential equations we can derive the equations to describe the two curves. The to be optimized parameters are <code>$M_0,k_p, k_m, c=\mbox{FF_parent_to_Met} $</code> with the constraints <code>$M_0&gt;0,k_p&gt;0, k_m&gt;0, 1&gt; c &gt;0$</code>.</p>

<pre><code>$$
\begin{split}
            y_{1j}&amp;= M_0e^{-k_pt_i}+\epsilon_{1j}\\
            y_{2j} &amp;= cM_0k_p\frac{e^{-k_mt_i}-e^{-k_pt_i}}{k_p-k_m}+\epsilon_{2j}
            \end{split}
$$
</code></pre>

<p>Therefore we can fit the curve without solving differential equations. </p>

<pre><code>BCS1.l &lt;- mkin_wide_to_long(BCS1)
BCS1.l &lt;- na.omit(BCS1.l)
indi &lt;- c(rep(1,sum(BCS1.l$name=='Parent')),rep(0,sum(BCS1.l$name=='Metab')))
sysequ.indi &lt;- function(t,indi,M0,kp,km,C)
  {
    y &lt;- indi*M0*exp(-kp*t)+(1-indi)*C*M0*kp/(kp-km)*(exp(-km*t)-exp(-kp*t));
    y
  }
M00 &lt;- 100
kp0 &lt;- 0.1
km0 &lt;- 0.01
C0 &lt;- 0.1
library(nlme)
result1 &lt;- gnls(value ~ sysequ.indi(time,indi,M0,kp,km,C),data=BCS1.l,start=list(M0=M00,kp=kp0,km=km0,C=C0),control=gnlsControl())
#result3 &lt;- gnls(value ~ sysequ.indi(time,indi,M0,kp,km,C),data=BCS1.l,start=list(M0=M00,kp=kp0,km=km0,C=C0),weights = varIdent(form=~1|name))
## Coefficients:
##          M0           kp           km            C 
## 1.946170e+02 5.800074e-03 8.404269e-03 2.208788e-01 
</code></pre>

<p>Doing this way, the elapsed time is almost 0, and the optimum is reached. However, we do not always have this simple case. The model can be complex and solving the differential equations is needed. See example 2</p>

<h2>Example 2, a complex model</h2>

<p>I worked on this dataset a long time ago and haven't got time to finish running the following script myself. (You might need hours to finish running.) 
<img src=""http://i.stack.imgur.com/jmzlD.png"" alt=""Fitted plot for Example""></p>

<pre><code>data(BCS2)
ex2 &lt;- mkinmod.full(Parent= list(type = ""SFO"",to = c( ""Met1"", ""Met2"",""Met4"", ""Met5""),
                                 k = list(ini = 0.1,fixed = 0,lower = 0,upper = Inf),
                                 M0 = list(ini = 100,fixed = 0,lower = 0,upper = Inf),
                                 FF = list(ini = c(.1,.1,.1,.1),fixed = c(0,0,0,0),lower = c(0,0,0,0),upper = c(1,1,1,1))),
                    Met1 = list(type = ""SFO"",to = c(""Met3"", ""Met4"")),
                    Met2 = list(type = ""SFO"",to = c(""Met3"")),
                    Met3 = list(type = ""SFO"" ),
                    Met4 = list(type = ""SFO"", to = c(""Met5"")),
                    Met5 = list(type = ""SFO""),
                    data=BCS2)
ex2$diffs
Fit2 &lt;- NULL
alglist &lt;- c(""L-BFGS-B"",""Marq"", ""Port"",""spg"",""solnp"")
for(i in 1:5) {
  Fit2[[i]] &lt;- mkinfit.full(ex2,plot = TRUE, quiet= TRUE,ctr = kingui.control(method = alglist[i],submethod = 'Port',maxIter = 100,tolerance = 1E-06, odesolver = 'lsoda'))
  }
names(Fit) &lt;- alglist
(lapply(Fit, function(x) x$par))
unlist(lapply(Fit, function(x) x$ssr))
</code></pre>

<p>This is an example where you will see warning messages like:</p>

<pre><code>DLSODA-  At T (=R1) and step size H (=R2), the    
  corrector convergence failed repeatedly     
  or with ABS(H) = HMIN   
In above message, R = 
[1] 0.000000e+00 2.289412e-09
</code></pre>

<h1>The original question</h1>

<p>Many of the methods used in Matlab Optimization Toolbox solvers are based on trust regions. According to the CRAN Task View page, only packages <strong>trust</strong>, <strong>trustOptim</strong>, <strong>minqa</strong>  have the trust-region based methods implemented. However, <code>trust</code> and <code>trustOptim</code> require gradient and hessian. <code>bobyqa</code> in <strong>minqa</strong> seems not the one I am looking for. From my personal experience, the trust-region-reflective algorithm in Matlab often performs better compared to the algorithms I tried in R. So I tried to find a similar implemetation of this algorithm in R.</p>

<p>I have asked a related question here: <a href=""http://stackoverflow.com/questions/13703344/is-there-an-r-function-using-the-same-algorithm-implemented-in-the-lsqnonlin-f"">R function to search for a function</a></p>

<p>The answer Matthew Plourde provided gives a function <code>lsqnonlin</code> with the same function name in Matlab but does not have the trust-region-reflective algorithm implemented. I edited the old one and asked a new question here because I think Matthew Plourde's answer is in general very helpful to R users who are looking for a function.</p>

<p>I did a search again and have no luck. Are there still some functions/packages out there which implement similar matlab functions. If not, I wonder if it is allowed that I tranlate the Matlab function directly into R and use it for my own purpose. </p>
"
201,Error in reading .dat file in python,"<p>I tried to read a .dat file in python using the snippet below:</p>

<pre><code>old = numpy.loadtxt('C:\Users\Downloads\new.dat',delimiter=',')
</code></pre>

<p>This gives the following error:</p>

<pre><code>[Errno 22] invalid mode ('U') or filename:'C:\\Users\\Downloads\new.dat'
</code></pre>

<p>However, it worked for many other .dat files. Can anyone tell me what the problem is?</p>
"
202,Intersecting points of curve!,"<p>I am drawing an arc within a square or a polygon with the maximum possible radius inside.
When the arc is drawn within the boundary of the square or polygon,
the arc length will intersect at few points on the square or polygon.</p>

<p>How to find the coordinates of the intersecting points of arc at the periphery of the square/ polygon?</p>
"
203,Calculating modelview matrix for 2D camera using Eigen,"<p>I'm trying to calculate modelview matrix of my 2D camera but I can't get the formula right. I use the Affine3f transform class so the matrix is compatible with OpenGL. This is closest that I did get by trial and error. This code rotates and scales the camera ok, but if I apply translation and rotation at same time the camera movement gets messed up: camera moves in rotated fashion, which is not what I want. (And this probaly due to fact I first apply the rotation matrix and then translation)</p>

<pre><code>Eigen::Affine3f modelview;
modelview.setIdentity();
modelview.translate(Eigen::Vector3f(camera_offset_x, camera_offset_y, 0.0f));
modelview.scale(Eigen::Vector3f(camera_zoom_x, camera_zoom_y, 0.0f));
modelview.rotate(Eigen::AngleAxisf(camera_angle, Eigen::Vector3f::UnitZ()));
modelview.translate(Eigen::Vector3f(camera_x, camera_y, 0.0f));
[loadmatrix_to_gl]
</code></pre>

<p>What I want is that camera would rotate and scale around offset position in screenspace {(0,0) is middle of the screen in this case} and then be positioned along the global xy-axes in worldspace {(0,0) is also initialy at middle of the screen} to the final position. How would I do this?</p>

<p>Note that I have set up also an orthographic projection matrix, which may affect this problem.</p>
"
204,How to vectorize this operation on every row of a matrix in R,"<p>I have a matrix filled with TRUE/FALSE values and I am trying to find the index position of the first TRUE value on each row (or return NA if there is no TRUE value in the row). The following code gets the job done, but it uses an apply() call, which I believe is just a wrapper around a for loop. I'm working with some large datasets and performance is suffering. Is there a faster way?</p>

<pre><code>&gt; x &lt;- matrix(rep(c(F,T,T),10), nrow=10)
&gt; x
       [,1]  [,2]  [,3]
 [1,] FALSE  TRUE  TRUE
 [2,]  TRUE  TRUE FALSE
 [3,]  TRUE FALSE  TRUE
 [4,] FALSE  TRUE  TRUE
 [5,]  TRUE  TRUE FALSE
 [6,]  TRUE FALSE  TRUE
 [7,] FALSE  TRUE  TRUE
 [8,]  TRUE  TRUE FALSE
 [9,]  TRUE FALSE  TRUE
[10,] FALSE  TRUE  TRUE

&gt; apply(x,1,function(y) which(y)[1])
 [1] 2 1 1 2 1 1 2 1 1 2
</code></pre>
"
205,Check if point is outside of curve,"<p>Sorry if this belongs on a maths forum. I have an image rounded corners within HTML5s canvas element, I want to make the rounded corners transparent. I have the following code:</p>

<pre><code>    var cornersImgData = tempContext.getImageData(0, 0, img.width, img.height);
    var topLeft = getPixel(cornersImgData, 0, 0);
    var topRight = getPixel(cornersImgData, cornersImgData.width - 1, 0);
    var bottomLeft = getPixel(cornersImgData, 0, cornersImgData.height - 1);
    // Check that the rounded corners have actually been applied (e.g. make sure the user hasn't just clicked the button and then undo)
    var bottomRight = getPixel(cornersImgData, cornersImgData.width - 1, cornersImgData.height - 1);
    if (('rgb(' + topLeft[0] + ', ' + topLeft[1] + ', ' + topLeft[2] + ')' == _roundedCornersColour) ||
        ('rgb(' + topRight[0] + ', ' + topRight[1] + ', ' + topRight[2] + ')' == _roundedCornersColour) ||
        ('rgb(' + bottomLeft[0] + ', ' + bottomLeft[1] + ', ' + bottomLeft[2] + ')' == _roundedCornersColour) ||
        ('rgb(' + bottomRight[0] + ', ' + bottomRight[1] + ', ' + bottomRight[2] + ')' == _roundedCornersColour))
    {
        for (var x = 0; x &lt; cornersImgData.width; x++)
        {
            for (var y = 0; y &lt; cornersImgData.height; y++)
            {
                var colour = getPixel(cornersImgData, x, y);
                if ('rgb(' + colour[0] + ', ' + colour[1] + ', ' + colour[2] + ')' == _roundedCornersColour)
                {
                    setPixel(cornersImgData, x, y, colour[0], colour[1], colour[2], 0);
                }
            }
        }
    }
</code></pre>

<p>This works but because I am replacing every instance of _roundedCornersColour I sometimes end up replacing a few pixels within the image itself. My high school maths is a little rusty and I can't figure out the best way to determine if x and y fall outside of where the corner should be. Can anyone help please?</p>

<p>Joe</p>
"
206,Webscraping Issues Involving Clicking (Using R),"<p>I am trying to webscrape the following website:</p>

<p><a href=""http://www.healthgrades.com/hospital-directory/california-ca-san-mateo/affiliated-physicians-HGSTED418D46050070"" rel=""nofollow"">http://www.healthgrades.com/hospital-directory/california-ca-san-mateo/affiliated-physicians-HGSTED418D46050070</a></p>

<p>I am using R to webscrape the website. Particularly, I am trying to copy all of doctor's names and specialties from this website. However, the main issue that I am dealing with is that the url link does not change when I press the arrow/next button. I can not use any basic techniques to webscrape this page. How can I solve this problem? It would be nice to have all of the data that I am collecting in one data matrix/spreadsheet.</p>
"
207,Statistical tools for programmers,"<p>I'm trying to evaluate the purchase of a statistical tool. This will be used in part by non-programming users (doing clinical studies) and in part by programmers, so I'm trying to find a good compromise between usability and automation. Of course, cost is an issue, but if I can build a solid case, we could probably buy a commercial package, so we're not totally limited to free options.</p>

<p>So far, our options are:</p>

<ul>
<li>Statistica (which some non-programmers already know)</li>
<li>Matlab Statistics toolbox (programmers already use matlab)</li>
<li>R language (would need a UI for non-programmers)</li>
<li>Hack something into Excel (not fun, but that's what non-programmers do right now)</li>
<li>?...</li>
</ul>

<p>What else is out there? What's the industry standard? What kind of distinctive features should I look for? What would you recommend, and why?</p>

<p>Ideally, we'd like a tool that can run both on Linux and Windows machines.</p>

<p>(I work in medical imaging, so we do both biostatistics, and software engineering statistics)</p>
"
208,R: counting frequency to use in levelplot,"<p>I have point series in a data.frame with duplication. I would like to plot them using level plot, and use as a Z frequency of x, y (in example how many times for x = 1 there was y = 2). How can I do this? Well it is easy for me to explain myself with SQL syntax:</p>

<pre><code> SELECT x, y, count(*) from data_frame GROUP BY x, y
</code></pre>

<p>:)</p>
"
209,discriminant analysis cross validation,"<p>Ive just tried the code for the discriminant analysis, as given here: <a href=""http://www.statmethods.net/advstats/discriminant.html"" rel=""nofollow"">http://www.statmethods.net/advstats/discriminant.html</a></p>

<p>If using this:    </p>

<pre><code>fit &lt;- lda(G ~ x1 + x2 + x3, data=mydata, na.action=""na.omit"", CV=TRUE) 
</code></pre>

<p>followed by this:  </p>

<pre><code>ct &lt;- table(mydata$G, fit$class) 
</code></pre>

<p>and  the dataset contains NAs (which mine did) the table cannot be produced as the two vectors are different lengths. The $G is the entire vector including NAs but the $class has the NAs removed, and therefore is of a different length. </p>

<p>Is there a way to remove the data points of the $G vector which were removed, before using the table() command?</p>
"
210,Creating a matrix for a billboarded quad?,"<p>I'm trying to come up with a proper algorithm to create a matrix which gets the quad to face the camer straight up but I'm having difficulty.</p>

<p>The quad I'm drawing is facing downwards, here's the code I have so far:</p>

<pre><code>D3DXVECTOR3 pos;
pos = D3DXVECTOR3(-2.0f, 6.0f, 0.1f);
D3DXMATRIX transform;
D3DXMatrixTranslation(&amp;transform, pos.x, pos.y, pos.z);

D3DXVECTOR3 axis = D3DXVECTOR3(0, -1, 0);
D3DXVECTOR3 quadtocam = pos - EmCameraManager::Get().GetPosition();

D3DXVec3Normalize(&amp;quadtocam,&amp;quadtocam);

D3DXVECTOR3 ortho;
D3DXVec3Cross(&amp;ortho, &amp;axis, &amp;quadtocam);

float rotAngle = acos(D3DXVec3Dot(&amp;axis,&amp;quadtocam));
D3DXMATRIX rot;
D3DXMatrixRotationAxis(&amp;rot,&amp;ortho,rotAngle);

transform = rot*transform;
</code></pre>

<p>This works when it comes to making the quad face the camera however it doesn't stay up right when facing it from all angles.</p>

<p>In this screen shot: <a href=""http://imgur.com/hFmzc.png"" rel=""nofollow"">http://imgur.com/hFmzc.png</a> On the left the quad is being viewed straight on (vector is 0,0,1), in the other its being viewed from an arbitrary angle.</p>

<p>Both times it's facing the camera, however when from an arbitrary angle its tilting along its local z axis. I'm not sure how to fix it and was wondering what the next step here would be?</p>
"
211,What's the fastest algorithm to represent a prime as sum of two squares?,"<p>I could use two loops to check for all combinations of two integers that less than <code>p</code> prime, but it's very inefficient. Is there a better algorithm to approach this problem? Any idea?  </p>

<p>Where <code>p mod 4 = 1</code>.</p>

<p>Thanks,</p>
"
212,Help with convergence in distribution,"<p>$Y$ is a random variable with $$M(t) = \frac{1}{(2-\exp(t))^s}.$$</p>

<p>Does $$\frac{Y-E(Y)}{\sqrt{\operatorname{Var}(Y)}}$$ converge in distribution as $s$ tends to infinity?</p>

<p>I let $Z = \frac{Y-E(Y)}{\sqrt{\operatorname{Var}(Y)}}$.
Differentiating the MGF of $Y$ and let $t = 0$ we have $$E(Y) = s,$$$$E(Y^2) = s^2 +2s,$$$$\operatorname{Var}(Y) = 2s.$$
Thus Mgf of Z:
 \begin{align*}E(\exp(tz)) &amp;= E\left(\exp\left(\frac{t(y-s)}{\sqrt{2s}}\right)\right)\\
&amp;=E\left(\exp\left(\frac{ty}{\sqrt{2s}}\right)\exp\left(\frac{-ts}{\sqrt{2s}}\right)\right)\\
&amp;=E\left(\exp\left(\frac{ty}{\sqrt{2s}}\right)\exp\left(\frac{-ts}{\sqrt{2s}}\right)\right)\\
&amp;=\exp\left(\frac{-ts}{\sqrt{2s}}\right)\frac{1}{\left(2-\exp\left(\frac{t}{\sqrt{2s}}\right)\right)^s}.
\end{align*}
But I'm not sure how to proceed - seems to me it tends to $0$?</p>
"
213,"R> Values in one column between values in another, return values in new dataframe","<p>I have data with the following format:</p>

<pre><code>structure(list(cat = structure(c(1L, 2L, 3L, 1L, 2L, 2L, 3L, 
3L, 3L, 3L, 1L, 2L), .Label = c(""A"", ""B"", ""C""), class = ""factor""), 
ID = structure(c(1L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 2L, 
3L, 4L), .Label = c(""s1"", ""s10"", ""s11"", ""s12"", ""s2"", ""s3"", 
""s4"", ""s5"", ""s6"", ""s7"", ""s8"", ""s9""), class = ""factor""), val = c(150, 
750, 950, 104, 726, 797, 890, 912, 994, 1004, 199, 704), 
LWR = c(100, 700, 900, NA, NA, NA, NA, NA, NA, NA, NA, NA
), UPP = c(200, 800, 1000, NA, NA, NA, NA, NA, NA, NA, NA, 
NA)), .Names = c(""cat"", ""ID"", ""val"", ""LWR"", ""UPP""), row.names = c(NA, 
-12L), class = ""data.frame"")
</code></pre>

<p>Which looks like:</p>

<pre><code>    cat ID  val LWR  UPP
1    A  s1  150 100  200
2    B  s2  750 700  800
3    C  s3  950 900 1000
4    A  s4  104  NA   NA
5    B  s5  726  NA   NA
6    B  s6  797  NA   NA
7    C  s7  890  NA   NA
8    C  s8  912  NA   NA
9    C  s9  994  NA   NA
10   C s10 1004  NA   NA
11   A s11  199  NA   NA
12   B s12  704  NA   NA
</code></pre>

<p>What I want to do is find a value in the val column having the same cat that lies closest to either the LWR or UPP values.  It's probably easiest to understand by looking at the desired output:</p>

<pre><code>  cat id val LWR  UPP  LS NLWR  US NUPP
1   A s1 150 100  200  s4  104 s11  199
2   B s2 750 700  800 s12  704  s6  797
3   C s3 950 900 1000  s8  912  s9  994
</code></pre>

<p>The new coloums (LS and NLWR/US and NUPP) are the same as the ids and vals from the extracted rows, just given new column names.  I've tried to run this using various forms of ""which"" and then reforming the data but haven't had any luck.  Is there a direct way to do this, or will it always take multiple steps?</p>
"
214,Finding the last transaction for each id,"<p>I have the following data frame:</p>

<pre><code>id&lt;-c(1,1,1,1,1,3,3,3,3)
period&lt;-c(""calib"",""calib"",""calib"",""valid"",""valid"",""calib"",""calib"",""calib"",""valid"")
date&lt;-c(""11-11-07"",""11-11-07"",""23-11-07"",""12-12-08"",""17-12-08"",""11-11-07"",""23-11-07"",""23-11-07"",""16-01-08"")
time&lt;-c(12,13,14,11,23,15,12,18,14)
df&lt;-data.frame(id,period,time,date)
df$date2&lt;-as.Date(as.character(df$date), format = ""%d-%m-%y"")


id period time     date      date2
 1  calib   12 11-11-07 2007-11-11
 1  calib   13 11-11-07 2007-11-11
 1  calib   14 23-11-07 2007-11-23
 1  valid   11 12-12-08 2008-12-12
 1  valid   23 17-12-08 2008-12-17
 3  calib   15 11-11-07 2007-11-11
 3  calib   12 23-11-07 2007-11-23
 3  calib   18 23-11-07 2007-11-23
 3  valid   14 16-01-08 2008-01-16
</code></pre>

<p>I need to extract the <code>date</code> of last transaction in the <code>calib</code> period, for each <code>id</code>, and put it in a new column. If two transactions have been made in one day (similar <code>date</code>) the last transaction should be chosen based on time of transaction.
The final table that I am looking for is as follows:</p>

<pre><code>id period time     date      date2  last
 1  calib   12 11-11-07 2007-11-11   NA
 1  calib   13 11-11-07 2007-11-11   NA
 1  calib   14 23-11-07 2007-11-23 2007-11-23
 1  valid   11 12-12-08 2008-12-12   NA
 1  valid   23 17-12-08 2008-12-17   NA 
 3  calib   15 11-11-07 2007-11-11   NA
 3  calib   12 23-11-07 2007-11-23   NA
 3  calib   18 23-11-07 2007-11-23 2007-11-23
 3  valid   14 16-01-08 2008-01-16   NA
</code></pre>

<p>Can anyone help me with this, please?!</p>
"
215,Convert mixed fraction string to float in PHP,"<p>I assumed there'd be an easy way in PHP to convert a string like <code>18 5/16</code> into the float <code>18.3125</code>. I can't find a straightforward function to do it. Is there one, or do I need to write my own?</p>
"
216,Error multiplying two numpy arrays,"<p>I am getting an error that I don't quite understand with the following script. I thought I would be able to multiple the two numpy arrays happliy but I keep getting this error:</p>

<pre><code>TypeError: unsupported operand type(s) for *: 'numpy.ndarray' and 'numpy.ndarray'
</code></pre>

<p>The script is below:</p>

<pre><code>def currents_to_resistance(Istack1, Istack2, A_list, x_list):

    #Error Calcs
    Videal_R1 = (1.380648e-23 * (27+273.15)) / (1.6021766e-19)
    print Videal_R1
    Istack1 = np.array(Istack1)
    Istack2 = np.array(Istack2)
    print Istack1
    print Istack2   

    g = Istack1*Istack2
    print g 
</code></pre>

<p>The print Istack1 Istack2 before the multiply come out as</p>

<pre><code>['0.0005789047' '0.0005743839' '0.0005699334' '0.000565551' '0.0005612346'
 '0.0005569839' '0.0005527969' '0.0005486719' '0.000544608' '0.0005406044'
 '0.0005366572' '0.000532768' '0.000528934' '0.0005251549' '0.0005214295'
 '0.0005177562' '0.0005141338' '0.0005105614' '0.000507039' '0.0005035643'
 '0.0005001368' '0.0004967555' '0.0004934193' '0.0004901279' '0.0004868796'
 '0.0004836736']
['0.000608027' '0.0006080265' '0.0006080267' '0.0006080267' '0.0006080261'
 '0.0006080261' '0.0006080262' '0.0006080261' '0.0006080263' '0.0006080272'
 '0.0006080262' '0.0006080262' '0.0006080257' '0.0006080256' '0.0006080258'
 '0.0006080256' '0.0006080252' '0.0006080247' '0.000608025' '0.0006080249'
 '0.000608025' '0.0006080251' '0.0006080249' '0.0006080254' '0.0006080251'
 '0.0006080247']
</code></pre>

<p>I call the function using</p>

<pre><code>Re_list = currents_to_resistance(full_list[i][0],full_list[i][1], temp_A, temp_x)
</code></pre>

<p>What am I missing here?</p>
"
217,Learning a representation from a set of vectors,"<p>I'm currently dealing with the following problem: I have a set of feature vectors (real-valued) describing different instances of a common entity (such as an object or an event). Using these vectors, I would like to learn a common representation (a vector) for this entity (be it in the same vector space or a reduced one). </p>

<p>The most straightforward solution would be to use an arithmetic average. However, I was wondering if you could suggest some other solutions too?</p>
"
218,"Fast solution of dense linear system of fixed dimension (N=9), symmetric, positive-semidefinite","<p>Which algorithm you would recommend for fast solution of dense linear system of fixed dimension (N=9) (matrix is symmetric, positive-semidefinite)?</p>

<ul>
<li>Gaussian elimination</li>
<li>LU decomposition</li>
<li>Cholesky decomposition</li>
<li>etc?</li>
</ul>

<p>Types are 32 and 64 bits floating points.</p>

<p>Such systems will be solved millions of times, so algorithm should be rather fast with respect to dimension (n=9).</p>

<p>P.S. examples of <strong>robust</strong> C++ implementations for proposed algorithm are appreciated.</p>

<blockquote>
  <p>1) What do you mean by ""solved million of times""? Same coefficient matrix with a million of different right hand terms, or a million of distinct matrices?</p>
</blockquote>

<p>Million of distinct matrices.</p>

<blockquote>
  <p>2) Positive _semi_definite means that matrix can be singular (to machine precision). How would you like to deal with this case? Just raise an error, or try to return some sensible answer?</p>
</blockquote>

<p>Raising error is OK.</p>
"
219,Calculating the value of a unit in a game,"<p>I'm currently working on a game where each unit has a value for health, shields and agility as well as a certain number of six types of weapon. </p>

<p>e.g. Unit B has 2 lasers, 2 heavy lasers, and 1 missile launcher, no ion blaster, no heavy ion blaster, no nuclear missiles, 5 health, 2 shield, and 40 agility. </p>

<p>I have gone through a good 20 different algorithms trying to get MS Excel to balance these all these factors to give the ship a score. My current algorithm works great for some of the smaller units, but gets very unbalanced with the bigger units</p>

<p>Each ship has a damage value, where each type of weaponry is multiplied by the damage of that weapon and added together.</p>

<pre><code>damage = weapon1*damage1 + weapon2*damage2 + ...
</code></pre>

<p>The damage value is multiplied by health + 2*shield + 2*agility. Agility and shield are multiplied to give them weight over health (a unit cant lose health if it can't be hit). I also subtract the cost of the unit. So my current equation for one of my units looks like:</p>

<pre><code>value = damage*(health + 2*shield + 2*agility) - 3*cost
</code></pre>

<p>Here are some examples:</p>

<ul>
<li>Unit 1 - 1 laser, 1 health, 93 agility, and costs 1. Total score is 233.</li>
<li>Unit 2 - 2 lasers, a missile launcher, and 3 health, but only 76 agility. Score is 200. </li>
<li>Unit 3 - 6 lasers, 30 health and 15 shields, 37 agility - scores 585. </li>
</ul>

<p>I want the scores of unit 3 to be higher, but the scores for units 1 and 2 are pretty good. Can anyone suggest a better equation that will smooth out the values?</p>
"
220,How to solve a general recurrence?,"<p>Is there a way to solve a general recurrence relation of the form</p>

<pre><code>a(n)=a(n-1) * a(n-2)....
</code></pre>

<p>I mean I can use the matrix method to solve a relation of the form</p>

<pre><code>F(n)=a1*F(n-1) + a2*F(n-2).......+ ak*F(n-k)
</code></pre>

<p>but what to do when there is a <code>'*'</code> sign instead of <code>'+'</code></p>
"
221,SQL: Calculating system load statistics,"<p>I have a table like this that stores messages coming through a system:</p>

<pre><code>Message
-------
ID (bigint)
CreateDate (datetime)
Data (varchar(255))
</code></pre>

<p>I've been asked to calculate the messages saved per second at peak load.  The only data I really have to work with is the CreateDate.  The load on the system is not constant, there are times when we get a ton of traffic, and times when we get little traffic.  I'm thinking there are two parts to this problem:  1. Determine ranges of time that are considered peak load, 2. Calculate the average messages per second during these times.</p>

<p>Is this the right approach?  Are there things in SQL that can help with this?  Any tips would be greatly appreciated.</p>
"
222,What's that elegant modulo that I can't figure out?,"<p>What's a good way to determine the following.</p>

<p>You have a table of game players, in an array of size N. Each round, each player takes a turn.</p>

<p>You know the index of the player that should go first, and each player will take a turn ascending the array, and looping back to 0 when it hits the last index. For example, if player at index 3 went first, then 4 would go second, and 2 would go last.</p>

<p>How do you calculate the index of the player that goes last in the round?</p>

<p>Here's one way:</p>

<pre><code>var startPosition = 3;
var numberOfPlayers = 10;

for (var i=0;i&lt;numberOfPlayers;i++) {
  startPosition++;
  if (startPosition == numberOfPlayers) startPosition = 0;
}
</code></pre>
"
223,Algorithm to calculate trajectories from vector field,"<p>I have a two-dimensional vector field, i.e., for each point <code>(x, y)</code> I have a vector <code>(u, v)</code>, whereas <code>u</code> and <code>v</code> are functions of <code>x</code> and <code>y</code>.</p>

<p>This vector field canonically defines a set of trajectories, i.e. a set of paths a particle would take if it follows along the vector field. In the following image, the vector field is depicted in red, and there are four trajectories which are partly visible, depicted in dark red:</p>

<p><img src=""http://i.stack.imgur.com/u78WF.gif"" alt=""trajectories""></p>

<p>I need an algorithm which efficiently calculates some trajectories for a given vector field. The trajectories must satisfy some kind of minimum denseness in the plane (for every point in the plane we must have a 'nearby' trajectory), or some other condition to get a reasonable set of trajectories.</p>

<p>I could not find anything useful on Google on this, and Stackexchange doesn't seem to handle the topic either.</p>

<p>Before I start devising such an algorithm by myself: <strong>Are there any known algorithms for this problem? What is their name, for which keywords do I have to search?</strong></p>
"
224,How to Interpret Predict Result of SVM in R?,"<p>I'm new to R and I'm using the <code>e1071</code> package for SVM classification in R.</p>

<p>I used the following code:</p>

<pre><code>data &lt;- loadNumerical()

model &lt;- svm(data[,-ncol(data)], data[,ncol(data)], gamma=10)

print(predict(model, data[c(1:20),-ncol(data)]))
</code></pre>

<p>The <code>loadNumerical</code> is for loading data, and the data are of the form(first 8 columns are input and the last column is classification) :</p>

<pre><code>   [,1] [,2] [,3] [,4] [,5] [,6] [,7]      [,8] [,9]
1    39    1   -1   43   -1    1    0 0.9050497    0
2    23   -1   -1   30   -1   -1    0 1.6624974    1
3    50   -1   -1   49    1    1    2 1.5571429    0
4    46   -1    1   19   -1   -1    0 1.3523685    0
5    36    1    1   29   -1    1    1 1.3812029    1
6    27   -1   -1   19    1    1    0 1.9403649    0
7    36   -1   -1   25   -1    1    0 2.3360004    0
8    41    1    1   23    1   -1    1 2.4899738    0
9    21   -1   -1   18    1   -1    2 1.2989637    1
10   39   -1    1   21   -1   -1    1 1.6121595    0
</code></pre>

<p>The number of rows in the data is 500.</p>

<p>As shown in the code above, I tested the first 20 rows for prediction. And the output is:</p>

<pre><code>         1          2          3          4          5          6          7 
0.04906014 0.88230392 0.04910760 0.04910719 0.87302217 0.04898187 0.04909523 
         8          9         10         11         12         13         14 
0.04909199 0.87224979 0.04913189 0.04893709 0.87812890 0.04909588 0.04910999 
        15         16         17         18         19         20 
0.89837037 0.04903778 0.04914173 0.04897789 0.87572114 0.87001066 
</code></pre>

<p>I can tell intuitively from the result that when the result is close to 0, it means 0 class, and if it's close to 1 it's in the 1 class.</p>

<p>But my question is how can I <strong>precisely</strong> interpret the result: is there a threshold <em>s</em> I can use so that values below <em>s</em> are classified as 0 and values above <em>s</em> are classified as 1 ?</p>

<p>If there exists such <em>s</em>, how can I derive it ?</p>
"
225,Automatic text translation,"<p>What tools or web services are available for machine text translation.</p>

<p>For example</p>

<pre><code>ENGLISH TEXT &gt; SERVER or LIB &gt; GERMAN TEXT
</code></pre>

<p>Libraries are also acceptable. </p>

<p>Is <a href=""http://code.google.com/apis/ajaxlanguage/"" rel=""nofollow"">Google language API</a> the only one ? </p>
"
226,Linear algebra package over the integers,"<p>I've recently ran into the following problem. Given a list of vectors (here I mean tuple) all with integer entries, is there a package (language isn't too much an issue, the faster the better, so I guess C) to very quickly determine when another integer vector is in the span of the original list? I need to do this arithmetic over the integers (no division). I'm sure there is one, but wanted to circumvent the lengthy literature review. </p>
"
227,asymptotic order of the variance of the maximum of iid standard Gaussian,"<p>Suppose that $X_1,\cdots,X_n$ are iid standard Gaussian. $X_{(n)}$ is the maximum of $(X_1,\cdots,X_n)$, how can I find the asymptotic order of $VAR[X_{(n)}]$?</p>

<p>The density function of $X_{(n)}$ can be obtained as follows:
$
P(X_{(n)}&lt;t)=\prod P(X_i&lt; t)=[\Phi(t)]^n,
$
So the density is $f(t)=n\phi(t)[\Phi(t)]^{n-1}$.</p>

<p>But it's unclear to me about the approximation of the integral:</p>

<p>$\int_{-\infty}^\infty t^2n\phi(t)[\Phi(t)]^{n-1} dt$ which is the second moment.</p>

<p>I know that the first moment can be bounded by 
$$
E(X_{(n)})\le \sqrt{2\log n}-\frac{\log\log n+\log 4\pi - 2\gamma}{2\sqrt{2\log n}}.
$$</p>
"
228,Time series with multiple dependent variables,"<p>I have got a dataframe with following columns
  product,date,sales</p>

<p>e.g
 data</p>

<pre><code>        product date      sales
         A      1/1/2011  100 
         A      2/1/2011  102
         ..     .. 
         B      1/1/2011   50 
         B      2/1/2011   50
</code></pre>

<p>I have done forecasting using one variable but dont know how to do when there are multiple dependent variables
I want to plot the time series plot of all products including their forecasted value
using the trellis plot</p>
"
229,Rows With Blank Entries in R,"<p>I have a 721 x 26 dataframe. Some rows have entries that are blank. It's not NULL
or NA but just empty like the following. How can I delete those rows that have these kind of entries?</p>

<pre><code>1         Y    N        Y          N            86.8
2         N    N        Y          N            50.0
3                                               76.8
4         N    N        Y          N            46.6
5         Y    Y        Y          Y            30.0
</code></pre>
"
230,Growing matrices in R from NULL,"<p>What is the problem with initializing a matrix object to NULL and then growing it with cbind() and rbind() ?
In case the number of rows and columns are not known a priori, is it not necessary to grow from NULL?</p>

<p>Edit: My question was prompted by the need to understand memory efficient ways of writing R code. The matrix context is more general and I'm probably looking for suggestions about efficient ways to handle other data objects as well.
Apologize for being too abstract/generic, but I did not really have a specific problem in mind.</p>
"
231,How to write an intersects for Shapes in android,"<p>I have an written an Object called Shape which has a Point representing the topLeftCorner and a Dimension with represents its width and height. 
To get the topRightCorner I can simply add the width to the topLeftPoint.x. I use to rotate them on a certain degree around their center. The problem after the rotation is, that my <code>intersects(Shape)</code> method fails, because it does not honor the rotation of the Shapes. The rotation will be the same for each Shape. My current implementation looks like this inside my Shape Object:</p>

<pre><code>public boolean intersects(Shape s){
    // functions returning a Point of shape s
    return intersects(s.topLeft())
        || intersects(s.topRight())
        || intersects(s.bottomLeft())
        || intersects(s.bottomRight())
        || intersects(s.leftCenter())
        || intersects(s.rightCenter())
        || intersects(s.center());
}

public boolean intersects(Point p){
    return p.x &gt;= leftX()
        &amp;&amp; p.x &lt;= rightX()
        &amp;&amp; p.y &gt;= topY()
        &amp;&amp; p.y &lt;= bottomY();
}
</code></pre>

<p>Basically I need functions like <code>rotatedLeftX()</code> or <code>rotatedTopRight()</code> to work properly. Also for that calculation I think it doesn't matter when the topLeft point before a rotation of ie 90 will turn into topRight... </p>

<p>I already read <a href=""http://stackoverflow.com/questions/9971230/calculate-rotated-rectangle-size-from-known-bounding-box-coordinates"">this</a> and <a href=""http://stackoverflow.com/questions/9447749/find-corners-of-a-rotated-rectangle"">this</a> Question here, but do not understand it fully. </p>
"
232,XIRR Calculation,"<p>How do I calculate Excel's <code>XIRR</code> function using C#?</p>
"
233,ggplot2 - highlight min/max bar,"<p>Without adding an extra column to the data.frame, is there a built-in way to highlight the min/max bar?  In the following example, I'd like the Joe bar to be green (max) and the John bar to be red (min).</p>

<p>I'm sure this has been asked before, but I couldn't find when searching:</p>

<pre><code>data= data.frame( Name = c(""Joe"",""Jane"", ""John"") , Value = c(3,2,1) )
ggplot(data=data)+geom_bar(aes_string(x=""Name"",y=""Value""), stat=""identity"" )
</code></pre>
"
234,Plotting predefined density functions using ggplot and R,"<p>I have three data sets of different lengths and I would like to plot density functions of all three on the same plot. This is straight forward with base graphics:</p>

<pre><code>n &lt;- c(rnorm(10000), rnorm(10000))
a &lt;- c(rnorm(10001), rnorm(10001, 0, 2))
p &lt;- c(rnorm(10002), rnorm(10002, 2, .5))

plot(density(n))
lines(density(a))
lines(density(p))
</code></pre>

<p>Which gives me something like this:</p>

<p><img src=""http://www.cerebralmastication.com/wp-content/uploads/2009/10/density.png"" alt=""alt text"" /></p>

<p>But I really want to do this with GGPLOT2 because I want to add other features that are only available with GGPLOT2. It seems that GGPLOT really wants to take my empirical data and calculate the density for me. And it gives me a bunch of lip because my data sets are of different lengths. So how do I get these three densities to plot in GGPLOT2?</p>
"
235,HTML5 canvas coordinates are giving strange angles,"<p>I want to be able to orient something toward the mouse on an HTML5 canvas. But when I use Math.atan2 and the other trig functions, the directions get messed up. It rotates in the opposite direction that it should and it's usually off by 90 degrees.</p>

<p>It will probably be easier if you see it for yourself. Here's the javascript:</p>

<pre><code>var mouseX=0;
var mouseY=0;
var canvas = document.getElementById(""world"");
var context = canvas.getContext(""2d"");

function mouseMoveHandler(event) {
    mouseX = event.clientX;
    mouseY = event.clientY;
}

function windowResizeHandler() {
    canvas.width = window.innerWidth;
    canvas.height = window.innerHeight;
}

function loop() {
    // Clear Screen
    context.clearRect(0,0,canvas.width,canvas.height);

    // Calculate the angle to the mouse
    a = Math.atan2(mouseX-canvas.width/2,mouseY-canvas.height/2);

    // Draw a line in the direction of the mouse
    context.beginPath();
    context.fillStyle = ""#000000"";
    context.moveTo(canvas.width/2+10, canvas.height/2);
    context.lineTo(canvas.width/2-10, canvas.height/2);
    context.lineTo(canvas.width/2+Math.cos(a)*100, canvas.height/2+Math.sin(a)*100);
    context.fill();
}

document.addEventListener('mousemove', mouseMoveHandler, false);
window.addEventListener('resize', windowResizeHandler, false);
windowResizeHandler();
setInterval(this.loop, 1000 / 30 );
</code></pre>

<p>And here's the HTML:</p>

<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Test&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;canvas id='world'&gt;&lt;/canvas&gt;

&lt;script type=""text/javascript"" src=""test.js""&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>You can see it in action here: <a href=""http://sidefofx.com/projects/stackOverflowQuestion/"" rel=""nofollow"">http://sidefofx.com/projects/stackOverflowQuestion/</a></p>

<p>How can I make the line point in the direction of the mouse?</p>
"
236,R randomForest for classification,"<p>I am trying to do classification with randomForest, but I am repeatedly getting an error message for which there seems to be no apparent solution (randomForest has worked well for me doing regression in the past).  I have pasted my code below.  'success' is a factor, all of the dependent variables are numbers.  Any suggestions as to how to run this classification properly?  </p>

<pre><code>&gt; rf_model&lt;-randomForest(success~.,data=data.train,xtest=data.test[,2:9],ytest=data.test[,1],importance=TRUE,proximity=TRUE)

Error in randomForest.default(m, y, ...) : 
  NA/NaN/Inf in foreign function call (arg 1)
</code></pre>

<p>also, here is a sample of the dataset:</p>

<blockquote>
  <p>head(data)</p>
</blockquote>

<pre><code>success duration  goal reward_count updates_count comments_count backers_count     min_reward_level max_reward_level
True 20.00000  1500           10            14              2            68                1             1000
True 30.00000  3000           10             4              3            48                5             1000
True 24.40323 14000           23             6             10           540                5             1250
True 31.95833 30000            9            17              7           173                1            10000
True 28.13211  4000           10            23             97          2936               10              550
True 30.00000  6000           16            16            130          2043               25              500
</code></pre>
"
237,R: serialize objects to text file and back again,"<p>I have a process in R that creates a bunch of objects, serializes them, and puts them into plain text files. This seemed like a really good way to handle things since I am working with Hadoop and all output needs to stream through stdin and stdout. </p>

<p>The problem I am left with is how to read these objects out of the text file and back into R on my desktop machine. Here's a working example that illustrates the challenge:</p>

<p>Let's create a tmp file and write a single object into it. This object is just a vector:</p>

<pre><code>outCon &lt;- file(""c:/tmp"", ""w"")
mychars &lt;- rawToChar(serialize(1:10, NULL, ascii=T))
cat(mychars, file=outCon)
close(outCon)
</code></pre>

<p>The mychars object looks like this:</p>

<pre><code>&gt; mychars
[1] ""A\n2\n133633\n131840\n13\n10\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n""
</code></pre>

<p>when written to the text file it looks like this:</p>

<pre><code>A
2
133633
131840
13
10
1
2
3
4
5
6
7
8
9
10
</code></pre>

<p>I'm probably overlooking something terribly obvious, but how do I read this file back into R and unserialize the object? When I try scan() or readLines() both want to treat the new line characters as record delimiters and I end up with a vector where each element is a row from the text file. What I really want is a text string with the whole contents of the file. Then I can unserialize the string. </p>

<p>Perl will read line breaks back into a string, but I can't figure out how to override the way R treats line breaks. </p>
"
238,"creating rectangular mesh (list of line segments) from a 2D array of points, in numpy","<p>I have an MxNx2 array of 2D points, where each point represents the center of a measured property of a grid. The graphical representation is below, with white points being the positions:</p>

<p><img src=""http://i.stack.imgur.com/Y6RjU.jpg"" alt=""enter image description here""></p>

<p>The point structure is like this (shape: MxNx2):</p>

<pre><code>[[[xij, yij], [xij, yij], ...]],
 [[xij, yij], [xij, yij], ...]],
 [[xij, yij], [xij, yij], ...]],
 [ ..., ...., ............... ],
 [[xij, yij], [xij, yij], ...]]]
</code></pre>

<p>The desired output would be like this:</p>

<pre><code>[[[x1, x2], [y1, y2]],
 [[x1, x2], [y1, y2]],
 ....................,
 [[x1, x2], [y1, y2]]
</code></pre>

<p>So that I could plot every segment one by one (using each pair of x,y positions) like this:</p>

<p><img src=""http://i.stack.imgur.com/FmPnC.png"" alt=""enter image description here""></p>

<p>I have trying something similar to:</p>

<pre><code>segments = []
for row in xrange(a.shape[0] - 1):
    for col in xrange(a.shape[1] - 1):
        here = a[row, col]
        below = a[row+1, col]
        right = a[row, col+1]
        segments.extend(((here, right), (here, below)))
</code></pre>

<p>but that leaves the right and bottom edges uncovered. Also, I suspect this is a somewhat ""dumb"", non-vectorized, brute-force way of doing it, it seems to be a common enough problem to have perhaps a mesh-creating function for it.</p>

<p>Any suggestion is welcome!</p>
"
239,How to use XTS period.apply() using TTR indicator functions?,"<p>I can't seem to use TTR indicator functions direclty with period.apply() from XTS. Please help me figure out what I'm doing wrong.</p>

<pre><code>&gt; require(TTR)
&gt; require(quantmod)
&gt; require(xts)
&gt; data(sample_matrix)

&gt; period.apply(sample_matrix, endpoints(sample_matrix,""weeks""), RSI)

Error in EMA(c(NA, 0.190714286249097, 0.190459011271606, 0, 0, 0, NA,  : 
  Invalid 'n'
</code></pre>

<p>I also tried to <code>as.xts(sample_matrix)</code> first but it doesn't help.</p>
"
240,Behavior of range in python and arange in numpy,"<p>Can someone please explain the '-5' below.
I'm somewhat new to numpy, but this seems bizarre</p>

<pre><code>In [112]: an_int=9

In [113]: an_int/2
Out[113]: 4

In [114]: range(-an_int/2,an_int/2)
Out[114]: [-5, -4, -3, -2, -1, 0, 1, 2, 3]

In [115]: arange(-an_int/2,an_int/2)
Out[115]: array([-5, -4, -3, -2, -1,  0,  1,  2,  3])

In [116]: range(-4,4)
Out[116]: [-4, -3, -2, -1, 0, 1, 2, 3]
</code></pre>
"
241,Extracting bin properties of a self organizing map in R,"<p>I am trying to cluster large amount of data with SOM in R programming. The sample code is here :</p>

<pre><code>&gt; library(""kohonen"")
&gt; data(""wines"")
&gt; wines.sc &lt;- scale(wines)
&gt; set.seed(7)
&gt; wine.som &lt;- som(data = wines.sc, grid = somgrid(5, 4, ""hexagonal""))
&gt; plot(wine.som, main = ""Wine data"")
</code></pre>

<p>Now I have two questions:</p>

<ol>
<li>How can I extract the bin properties to find out each row of data belongs to which bin? (for example, I want to know each wine data row belongs to which bin). </li>
<li>How can I save the SOM and test it on the new dataset? </li>
</ol>
"
242,ggplot2 Color Scale Over Affected by Outliers,"<p>I'm having difficulty with a few outliers making the color scale useless.</p>

<p>My data has a Length variable that is based in a range, but will usually have a few much larger values. The below example data has 95 values between 500 and 1500, and 5 values over 50,000. The resulting color legends tend to use 10k, 20k, ... 70k for the color changes when I want to see color changes between 500 and 1500. Really, anything over around 1300 should be the same solid color (probably median +/- mad), but I don't know where to define that.</p>

<p>I'm open to any ggplot solution, but ideally lower values would be red, middle white, and higher blue (low is bad). In my own dataset, date is an actual date with as.POSIXct() in the ggplot aes(), but doesn't seem to affect the example.</p>

<pre><code>#example data
date &lt;- sample(x=1:10,size=100,replace=T)
stateabbr &lt;- sample(x=1:50,size=100,replace=T)
Length &lt;- c(sample(x=500:1500,size=95,replace=T),60000,55000,70000,50000,65000)
x &lt;- data.frame(date=date,stateabbr=stateabbr,Length=Length)

#main plot
(g &lt;- ggplot(data=x,aes(x=date,y=factor(stateabbr))) +
  geom_point(aes(color=as.numeric(as.character(Length))),alpha=3/4,size=4) + 
  #scale_x_datetime(labels=date_format(""%m/%d"")) + 
  opts(title=""Date and State"") + xlab(""Date"") + ylab(""State""))

#problem
g + scale_color_gradient2(""Length"",midpoint=median(x$Length))
</code></pre>

<p>Adding trans=""log"" or ""sqrt"" doesn't quite do the trick either.</p>

<p>Thank you for your help!</p>
"
243,Performance difference between built in magnitude function and this code,"<p>Is there a big difference between using the DirectX vector magnitude math function as opposed to the following code? </p>

<pre><code>float hyp = sqrt(pow(globalVector.x, 2) + pow(globalVector.y, 2))
</code></pre>
"
244,easiest way to generate base 3 numbers,"<p>I am trying to solve a mathematical problem programatically. Having a way to generate base 3 numbers would make it very easy to solve the problem. Do you know of any language that has in-built support for working with numbers of non-trivial base? It would be equally great if you could point me to some tool that can help me generate a sequence of base 3 numbers</p>
"
245,Is the death penalty an effective deterrent?,"<p>There is a lot of clutter from anti-death penalty sites, which makes it hard to find all the research.  They often cite statistics like those explained in <a href=""http://www.deathpenaltyinfo.org/deterrence-states-without-death-penalty-have-had-consistently-lower-murder-rates"" rel=""nofollow"">this website</a>, including the chart below that shows that murder rates in states without the death penalty are lower than those in states with the death penalty.</p>

<p><img src=""http://i.stack.imgur.com/dt5r7.jpg"" alt=""enter image description here""></p>

<p>Is a straight comparison of murder rates a fair way to gauge the effectiveness of the death penalty as a deterrent?  Might the murder rates be even higher in states that currently have the death penalty if they abandoned the practice?  Aren't there likely to be other factors that are contributing to higher murder rates (such as what each state considers to be ""murder"", or poverty levels, especially in the typically poorer southern states that carry out the majority of executions in the US)?  Is there any evidence for death penalty effectiveness that attempts to control for these other variables?  What about as a deterrent for other crimes besides murder?</p>

<p>Unless you're on here from China or Iran, I imagine this is a pretty uniquely US question. However, many European countries currently without the death penalty once had such laws, and perhaps the arguments used in those older debates would have merit.</p>

<p>UPDATE: I think ideally the answer to this question should involve some crime statistics from a place that had the death penalty, and then abolished it, or vice versa.  Ideally modeled in a way to try and control for other factors (as much as possible).</p>
"
246,R: polynomial shortcut notation in nls() formula,"<p>With the linear model function lm() polynomial formulas can contain a shortcut notation like this:</p>

<pre><code>m &lt;- lm(y ~ poly(x,3))
</code></pre>

<p>this is a shortcut that keeps the user from having to create x^2 and x^3 variables or typing them in the formula like <code>I(x^2) + I(x^3)</code>. Is there comparable notation for the nonlinear function <code>nls()</code>? </p>
"
247,java software for simplifying math expressions in contrast to evaluation,"<p>I just finished coding a symbolic differential calculus solver in the Java programming language. It works fine except for the fact that due to the algorithm I used in accomplishing the task, there are so many brackets and the expression is not in its most simple form.
For example, evaluating</p>

<pre><code>    diff(x-x^2)=(-((x^2)*(2/x))+1)
</code></pre>

<p>instead of</p>

<pre><code>    1-2*x.
</code></pre>

<p>The output is correct but is not simplified and this affects my optimization goals for the system. For this reason, subsequent stages of differentiation on this un-simplified output quickly produces output strings that gets large at a very high rate and makes it hard to evaluate higher derivatives. Please what free Java library is available for simplifying these output? I'm sure some people could say..why not code something by yourself..if you could code the differential calculus evaluator..but the truth is that I've had to code so many parsers and I need something reliable to get the job done for me. Thank you so much.</p>
"
248,making running mean of numbers in a row,"<p>I have the following table    </p>

<pre><code>&gt;anna2
     name   from      to       result
     11     66607     66841       5
     11     66846     67048       6 
     11     67409     68216       7
     11     69025     69289       12
     11     70172     70560       45
</code></pre>

<p>what I want is to create column which will have averages of the result in a row....</p>

<p>what I mean is that I want to have an average of the values 5,6,7 then the 6,7,12 then the 7,12,45 then the 12,45.....</p>

<p>BUT the mean of the 5,6,7 i want to be allocated to the 66846-67048 the mean of the 6,7,12 i want to be allocated in to the 67409 68216</p>

<p>so every time i want to have the mean to the center of the 3 values that i have used to calculate that because then i need to make a plot where my x is going to be the from-start and the y the mean value</p>

<p>how can i do that?</p>

<p>thank you in advance</p>

<p>Best regards
Anna</p>
"
249,Parallel computing in R on Windows using doSNOW: How to transfer results from clusters back to master,"<p>When using clusters in R on Windows I been trying to find a simple way to transfer results from a cluster to the master. If the results is an array or a simple number the .combine option of  foreach / %dopar% statement takes care of this, but if the result is a complex object lets such a randomForest model, how to transfer the whole model from the slave cluster back to the master?</p>

<p>I try: assing with <code>env=.Global</code> but it does not work on my Windows 7.</p>

<p>At the end I work around by saving the object to file. Then the master can recover the object. If someone knows a more elegant way or why assing does not work I appreciate comments.</p>

<p>sample code:</p>

<pre><code>print("" paralelize with 8 cores ------------------------------"")
library(doSNOW)
cl&lt;-makeCluster(8)
registerDoSNOW(cl)
clusterEvalQ(cl, library(randomForest))
clusterExport(cl, ""x"")
clusterExport(cl, ""y"")
clusterExport(cl, ""x.selected"")

makeModel &lt;- function(i){
  m &lt;- randomForest(x,x.selected[i,],mtry=250,sampsize=3200,ntree = 3000,do.trace=TRUE) 
  eval(parse(text = paste(""model_"",i,"" &lt;- m"",sep="""")))
  eval(parse( text =paste(""save(model_"", i, "", file =\""model_"", i, "".Rdata\"")"",sep="""" ) ))
}

foreach(i = 1:length(x.selected[,1]),.verbose = TRUE ) %dopar% makeModel(i)
stopCluster(cl)

foreach(i = 1:length(x.selected[,1]),.verbose = TRUE ) %do% 
load(paste(""model_"",i,"".RData"",sep=""""))
</code></pre>
"
250,C++ integer floor function,"<p>I want to implement greatest integer function. [The ""greatest integer function"" is a <a href=""http://www.google.com/search?q=greatest%20integer%20function"" rel=""nofollow"">quite</a> <a href=""http://books.google.com/books?q=greatest+integer+function"" rel=""nofollow"">standard name</a> for what is also known as the floor function.]</p>

<pre><code>int x = 5/3;
</code></pre>

<p>My question is with greater numbers could there be a loss of precision as 5/3 would produce a double?</p>

<p>EDIT: Greatest integer function is integer less than or equal to X.
Example:</p>

<pre><code>4.5 = 4
4 = 4
3.2 = 3
3 = 3
</code></pre>

<p>What I want to know is 5/3 going to produce a double? Because if so I will have loss of precision when converting to int.</p>

<p>Hope this makes sense.</p>
"
251,Evaluating 'combinatorial' sum,"<p>Help me please to calculate the following sum. I have seen such kind of formulas in the papers related to combinatorics, specifically 'trees'. I am curious how to calculate or approximate this sum:
Let $n \in N$, $q\geq 2$
$$
\sum_{m=-n}^n m^q {n \choose (m+n)/2}=\Gamma(n+1)\sum_{m=-n}^nm^q\frac{1}{\Gamma(n/2-m/2+1)\Gamma(n/2+m/2+1)}
$$</p>
"
252,Cannot view graphic output properly (forest plot),"<p>I am running a meta analysis in R using the meta package. The only problem is that I am combinining a large number of studies, so when I try to view the forest plot, I cannot see the entirety of it. If I try export it it also cuts off parts of the plot.</p>

<p>An example of my data is as follows</p>

<pre><code>title&lt;-c(""study1"",""study2"",""study3"",""study4"",""study5"",""study6"",""study7"",""study8"",""study9"",""study10"",""study11"",""study12"",""study13"",""study14"",""study15"",""study16"",""study17"",""study18"",""study19"",""study20"")
ES&lt;-c(45,47,108,76,-9.5,16,25.1,36.4,44,34,65.5,45,53,31.4,26,-10,45,19.1,28.8,36)
seES&lt;-c(104.5,6.4,31.8,32.3,15.3,5.9,11.8,7.6,11.7,7.6,14.7,5.1,2.5,13.2,15.3,-5.1,7.6,9.7,7.6,8.1)
indtreat&lt;-c(""drug 1"",""drug 1"",""drug 1"",""drug 2"",""drug 3"",""drug 4"",""drug 5"",""drug 6"",""drug 2"",""drug 7"",""drug 8"",""drug 8"",""drug 2"",""drug 7"",""drug 6"",""drug 6"",""drug 9"",""drug 2"",""drug 8"",""drug 10"")

data&lt;-cbind(title,ES,seES,indtreat)
data&lt;-as.data.frame(data)

alltreat_meta&lt;-metagen(data$ES,data$seES,studlab=data$title,sm=""MD"",label.e=""Active treatment"",
label.c=""placebo"",byvar=data$indtreat, print.byvar=FALSE)
forest(alltreat_meta)
</code></pre>

<p>(This is giving me the error ""non-numeric value for TE or seTE"", which I'm not too sure why?)</p>

<p>The problem is that when I run the line</p>

<pre><code>forest(alltreat_meta)
</code></pre>

<p>I cannot see the full forest plot, and am not sure how to zoom out or scroll up and down this plot. Does anyone know how to work around this problem?</p>
"
253,Automating R Script,"<p>I have written an R script that pulls some data from a database, performs several operations on it and post the output to a new database. </p>

<p>I would like this script to run every day at a specific time but I can not find any way to do this effectively. </p>

<p>Can anyone recommend a resource I could look at to solve this issue? I am running this script on a Windows machine.</p>
"
254,"Looking for a tool similar to ""Codealike""","<p>Does anyone know if there are any tools in existence similar to <a href=""http://www.codealike.com"" rel=""nofollow"">Codealike</a> where one could review stats on the work performed on a daily, weekly, monthly and yearly basis? I think codealike is a great tool, but it does not seem to work with my setup in Visual Studio 2010.</p>
"
255,How to periodize years into dates?,"<p>I'd like to go from this:</p>

<pre><code>  years
 -------
  1994
  2001
   .
   .
</code></pre>

<p>To this:</p>

<pre><code>int dates
 ------
  8793    # 1994-01-28
  8824    # 1994-02-28
  8852    # 1994-03-28
  8883    # 1994-04-28
  8913    # 1994-05-28
  8944    # 1994-06-28
  8974    # 1994-07-28
  9005    # 1994-08-28
  9036    # 1994-09-28
  9066    # 1994-10-28
  9097    # 1994-11-28
  9127    # 1994-12-28
 11350    # 2001-01-28
 11381    # 2001-02-28
 11409    # 2001-03-28
 11440    # 2001-04-28
 11470    # 2001-05-28
 11501    # 2001-06-28
 11531    # 2001-07-28
 11562    # 2001-08-28
 11593    # 2001-09-28
 11623    # 2001-10-28
 11654    # 2001-11-28
 11684    # 2001-12-28
   .
   .
</code></pre>

<p>i.e. periodizing each year into 12 dates (the 28th each month of that year) as 1970-base integers.</p>

<p>What is the most efficient way of doing this?</p>

<p>My attempt is painfully slow!</p>

<pre><code>require(data.table)

# Sample data
dt &lt;- data.table(year=c(1994,2001)) # edit

# Create results table
data &lt;- data.table(dates=integer())

for (i in 1:12) {
    temp &lt;- dt
    temp$dates &lt;- as.integer(as.Date(paste(temp$year, ""-"", sprintf( ""%02d"",i),""-28"", sep="""")))
    temp &lt;- subset(temp, select=dates)
    data &lt;- rbind(temp,data)
}

# Sort
data &lt;- data[with(data, order(dates)),]
</code></pre>
"
256,Changing coloumn names for all the files in the working directory,"<p>I am trying to learn how to work with multiple files, I have 5 sample csv files in the working directory that I am reading with the following codes:</p>

<pre><code>j = list.files()
d = lapply(j, read.csv, skip=6)
</code></pre>

<p>each files has 27 coloumns and I am trying to set coloumn name for each file, I know how to set coloumn name for a individual file, for example : </p>

<pre><code>colnames(data) = c(""type"",""date"",""v1"",""v2"",""v3"",""v4"",""v5"",""v6"",""v7"",""v8"",""v9"",""v10"",""v11"",""v12"",""v13"",""v14"",""v15"",""v16"",""v17"",""v18"",""v19"",""v20"",""v21"",""v22"",""v23"",""v24"",""total"")
</code></pre>

<p>I am just wondering how can I set for all the the files in the directory? </p>

<p>many thanks,
Ayan </p>
"
257,Google Combinatorial Optimization interview problem,"<p>I got asked this question on a interview for google a couple of weeks ago, I didn't quite get the answer and I was wondering if anyone here could help me out.</p>

<p>You have an array with <strong>n</strong> elements. The elements are either 0 or 1.
You want to <strong>split the array into k <em>contiguous</em> subarrays</strong>. The size of each subarray can vary between ceil(n/2k) and floor(3n/2k). You can assume that k &lt;&lt; n.
After you split the array into k subarrays. One element of each subarray will be randomly selected. </p>

<p>Devise an algorithm for maximizing the sum of the randomly selected elements from the k subarrays.
Basically means that we will want to split the array in such way such that the sum of all the expected values for the elements selected from each subarray is maximum. </p>

<p>You can assume that n is a power of 2.</p>

<pre><code>Example:

Array: [0,0,1,1,0,0,1,1,0,1,1,0]
n = 12
k = 3
Size of subarrays can be: 2,3,4,5,6

Possible subarrays [0,0,1] [1,0,0,1] [1,0,1,1,0]
Expected Value of the sum of the elements randomly selected from the subarrays: 1/3 + 2/4 + 3/5 = 43/30 ~ 1.4333333 

Optimal split: [0,0,1,1,0,0][1,1][0,1,1,0]
Expected value of optimal split: 1/3 + 1 + 1/2 = 11/6 ~ 1.83333333
</code></pre>
"
258,R iGraph Heatmap in Vertex,"<p>I'm quite new to R and stuck on a question. 
Would it be possible to print a heatmap on a vertex in iGraph?
I know I can do a colored square or circle. But would a small heatmap be possible?
This is the code that draws my current graph,:</p>

<pre><code>    # create graph
graph &lt;- graph.data.frame(network[,1:2])
vertex_names &lt;- get.vertex.attribute(graph,""name"")


# define node attributes
V(graph)$label.font &lt;- 1
V(graph)$label.font[which(element_types[vertex_names,""type""]==""PRIMARIES"")] &lt;- 2
V(graph)$label.font[which(element_types[vertex_names,""type""]==""REACTION"")] &lt;- 2

V(graph)$label &lt;- element_types[vertex_names,""label""]
V(graph)$color &lt;- element_types[vertex_names,""color""]
V(graph)$size &lt;- as.integer(element_types[vertex_names,""size""]*20)
V(graph)$label.cex &lt;- element_types[vertex_names,""weight""]

V(graph)$frame.color &lt;- ""gray""
V(graph)$frame.color[which(element_types[vertex_names,""type""]==""PRIMARIES"")] &lt;- ""white""
V(graph)$frame.color[which(element_types[vertex_names,""type""]==""PATHWAY"")] &lt;- ""white""
V(graph)$frame.color[which(element_types[vertex_names,""type""]==""PRODUCTS"")] &lt;- ""white""
V(graph)$frame.width &lt;- 10

V(graph)$shape &lt;- ""square""
V(graph)$shape[which(element_types[vertex_names,""type""]==""REACTION"")] &lt;- ""circle""

V(graph)$label.color &lt;- ""black""
V(graph)$label.color[which(element_types[vertex_names,""type""]==""PRIMARIES"")] &lt;- ""darkred""
V(graph)$label.color[which(element_types[vertex_names,""type""]==""PATHWAY"")] &lt;- ""darkgreen""
V(graph)$label.color[which(element_types[vertex_names,""type""]==""REACTION"")] &lt;- ""darkorange3""

E(graph)$color &lt;- ""red""
E(graph)$color[which(network[,3]==""out"")] &lt;- ""blue""
E(graph)$color[which(network[,3]==""external"")] &lt;- ""darkgreen""
E(graph)$arrow.size &lt;- 0.5

layout &lt;- layout.auto(graph)
plot.igraph(graph,layout=layout,main=pathways[pathway_id,""COMMON-NAME""])
</code></pre>

<p>furthermore I have matrices in a list that can be drawn into a heatmap.
These matrices look like this:</p>

<pre><code>[[1]]
     TotalSNP HighSNP ModerateSNP PromotorSNP
[1,]        1       0           0           1
[2,]        3       0           2           1
[3,]        5       0           2           3
[4,]        1       0           0           1
[5,]        7       0           4           3
[6,]        3       0           3           0
[7,]        4       0           1           3
[8,]        3       0           2           1

[[2]]
     TotalSNP HighSNP ModerateSNP PromotorSNP
[1,]        3       0           1           2
[2,]        0       0           0           0

[[3]]
     TotalSNP HighSNP ModerateSNP PromotorSNP
[1,]        0       0           0           0
[2,]        0       0           0           0
</code></pre>

<p>Does anyone know if it's possible to draw these matrices as heatmaps on the vertex???
Thank you in advance!
cheers,</p>

<p>Some toy data. </p>

<pre><code>    FinalList &lt;- list(structure(c(1, 3, 5, 1, 7, 3, 4, 3, 0, 0, 0, 0, 0, 0, 0, 
0, 0, 2, 2, 0, 4, 3, 1, 2, 1, 1, 3, 1, 3, 0, 3, 1), .Dim = c(8L, 
4L), .Dimnames = list(NULL, c(""TotalSNP"", ""HighSNP"", ""ModerateSNP"", 
""PromotorSNP""))), structure(c(3, 0, 0, 0, 1, 0, 2, 0), .Dim = c(2L, 
4L), .Dimnames = list(NULL, c(""TotalSNP"", ""HighSNP"", ""ModerateSNP"", 
""PromotorSNP""))), structure(c(0, 0, 0, 0, 0, 0, 0, 0), .Dim = c(2L, 
4L), .Dimnames = list(NULL, c(""TotalSNP"", ""HighSNP"", ""ModerateSNP"", 
""PromotorSNP""))))
</code></pre>

<p>`</p>

<p>The size of the matrices can vary, It's always 4 columns but the number of rows can vary from 0 rows till 10. </p>
"
259,How to proceed with spoj LGIC?,"<p>I am trying to solve this problem <a href=""http://www.spoj.pl/problems/LGIC/"" rel=""nofollow"">http://www.spoj.pl/problems/LGIC/</a>. I just can't figure out how is this sequence advancing.</p>

<p>By lagarange's it is too complex to solve for such a great range.</p>

<p>The farthest I could get was with factorials</p>

<pre><code>  1! = 1   &amp; a1=2
  2! = 2   &amp; a2=4
  3! = 6   &amp; a3=11
  4! = 24  &amp; a4=36
  5! = 120 &amp; a5=147
  6! = 720 &amp; a6=778
</code></pre>

<p>Please guide me someone..</p>
"
260,find all combinations of 3 Keys in Object,"<p>Given an object with <strong>n</strong> keys, need to find all combinations of every 3 keys (<strong>nC3</strong> in math)</p>

<p>is this the most efficient way?</p>

<pre><code>var x = {};  // object

x['00'] = [1, 7, 9];
x['01'] = [1, 9];
x['02'] = [6, 8];
x['03'] = [1, 7];
x['04'] = [1, 5, 8];
x['05'] = [4, 6, 8, 9];

var triples = [],
    c = [0,0,0]; // counter. keep track of the indexes when looping over objects

for(var i in x){
    c[0]++;
    c[1] = 0;

    for(var j in x){ // loop to compare this x[i] array to all other arrays
        c[1]++;
        if( c[1] &lt; c[0]+1 ) continue;
        c[2] = 0;

        for(var k in x){
            c[2]++;
            if( c[2] &lt; c[1]+1 ) continue;

            triples.push( [i,j,k] );
        }
    }
}

console.dir(triples);
</code></pre>
"
261,What's the correct terminology for something that isn't quite classification nor regression?,"<p>Let's say that I have a problem that is basicly classification. That is, given some input and a number of possible output classes, find the correct class for the given input. Neural networks and decision trees are some of the algorithms that may be used to solve such problems. These algorithms typically only emit a single result however: the resulting classification.</p>

<p>Now what if I weren't only interested in one classification, but in the <em>posterior probabilities</em> that the input belongs to each of the classes. I.E., instead of the answer ""This input belongs in class A"", I want the answer ""This input belongs to class A with 80%, class B with 15% and class C with 5%"".</p>

<p>My question is not on how to obtain these posterior probabilities, but rather on the correct terminology to describe the process of finding them. You could call it regression, since we are now trying to estimate a number of real valued numbers, but I am not quite sure if that's right. I feel it's not exactly classification either, it's something in between the two.</p>

<p><strong>Is there a word that describes the process of finding the class conditional posterior probabilities that some input belongs in each of the possible output classes?</strong></p>

<p>P.S. I'm not exactly sure if this question is enough of a programming question, but since it's about machine learning and machine learning generally involves a decent amount of programming, let's give it a shot.</p>
"
262,MCMC Metropolis Hastings,"<p>Does anyone know a webpage or a document where I can find a practical example of implementation of the Metropolis-Hastings algorithm, with some thoughts about burn-in time and how to construct the transition matrix?</p>
"
263,Python 2.7 vs. 3.3 Performance,"<p>As of now, how does the overall performance / efficiency of Python 2.7 compare to 3.3?</p>

<p>I use Python for academic research, so am always concerned with reducing the time to run experiments, since waiting for experiments to finish tends to waste a lot of time.</p>

<p>I'm most interested in a comparison for simple numpy matrices and fundamental language features:</p>

<ul>
<li>Basic data structures including dictionaries, lists, etc</li>
<li>Method invocations</li>
<li>Object creation overhead / memory usage</li>
<li>Basic file I/O</li>
<li>Basic matrix operations in numpy (dot product with large float matrices)</li>
</ul>

<p>Similar questions have appeared for older versions of Python 3 (3.1, 3.2) and have generally said performance was better (if only slightly) in 2.7, but based on <a href=""http://mail.python.org/pipermail/python-dev/2012-October/121923.html"" rel=""nofollow"" title=""this mailing list"">this e-mail from the Python-dev mailing list</a>, it looks like Python 3.3 may now be superior 2.7.</p>
"
264,Create a dataframe with the results of a simple linear regression,"<p>I have a time serie of individual body condition index. I want to determine, for each individual, the evolution of the body condition index over time. In other words, I want to create a linear regression for each individual and then gather all the results (intercept and slope) into a dataframe that would look like this:</p>

<p>First column : Individual id</p>

<p>Second column : Intercept</p>

<p>Thirs column: Slope</p>

<p>This is what I tried (with only a sample of my dataset), but I only get a list of all the results and I don't know how to reunite them into a single dataframe:</p>

<pre><code>individual &lt;- c(1,1,6,8,8,9,9,9,12,12)
day &lt;- c(4,17,12,12,17,3,9,22,13,20)
condition &lt;- c(0.72, 0.72, 0.67, 0.73, 0.76, 0.65, 0.68, 0.78, 0.73, 0.71)       
test &lt;- data.frame(individual, day, condition)
ind.id &lt;- unique(test$individual)
ind.list &lt;- lapply(1:length(ind.id), function(i){ subset(test, test$individual==ind.id[i])})
lms &lt;- lapply(ind.list, lm, formula=condition~day)
</code></pre>

<p>Thank you!</p>
"
265,What does size really mean in geom_point?,"<p>In both plots the points look different, but why?</p>

<pre><code>mya &lt;- data.frame(a=1:100)

ggplot() +
  geom_path(data=mya, aes(x=a, y=a, colour=2, size=seq(0.1,10,0.1))) +
  geom_point(data=mya, aes(x=a, y=a, colour=1, size=1)) +
  theme_bw() +
  theme(text=element_text(size=11))

ggplot() +
  geom_path(data=mya, aes(x=a, y=a, colour=2, size=1)) +
  geom_point(data=mya, aes(x=a, y=a, colour=1, size=1)) +
  theme_bw() +
  theme(text=element_text(size=11))
</code></pre>

<p><code>?aes_linetype_size_shape</code> explains ...</p>

<pre><code> # Size examples
 # Should be specified with a numerical value (in millimetres),
 # or from a variable source
</code></pre>

<p>But in my code it looks different.</p>
"
266,Best way to reduce consecutive NAs to single NA,"<p>I need to reduce the consecutive NA's in a vector to a single NA, without touching the other values.<br>
So, for example, given a vector like this:</p>

<pre><code>NA NA  8  7 NA NA NA NA NA  3  3 NA -1  4
</code></pre>

<p>what I need to get, is the following result:</p>

<pre><code>NA  8  7 NA  3  3 NA -1  4
</code></pre>

<p>Currently, I'm using the following function:</p>

<pre><code>reduceConsecutiveNA2One &lt;- function(vect){
  enc &lt;- rle(is.na(vect))

  # helper func
  tmpFun &lt;- function(i){
    if(enc$values[i]){
      data.frame(L=c(enc$lengths[i]-1, 1), V=c(TRUE,FALSE))
    }else{
      data.frame(L=enc$lengths[i], V=enc$values[i])
    }
  }

  Df &lt;- do.call(rbind.data.frame,lapply(1:length(enc$lengths),FUN=tmpFun))

  return(vect[rep.int(!Df$V,Df$L)])
}
</code></pre>

<p>and it seems to work fine, but probably there's a simpler/faster way to accomplish this task.</p>

<p>Any suggestions ?</p>

<p>Thanks in advance.</p>
"
267,Can I transform this Perl Script to make it read from a file?,"<p>I want to get the median of a specific column of data using a Perl Script,I got a script that reads, values from an array within the script,.</p>

<pre><code>my (@vals, $med);
@vals =(12, 23, 34, 21, 66,66, 34, 87);
print ""UNSORTED: @vals\n""; #sort data points 
@vals = sort(@vals); 
print ""SORTED: @vals\n""; #test to see if there are an even number of data points 
if( @vals % 2 == 0) { #if pair then: 
$sum = $vals[(@vals/2)-1] + $vals[(@vals/2)]; 
$med = $sum/2; 
print ""The median value is $med\n""; 
} 
else {                       #if odd then: 
print ""The median value is $vals[@vals/2]\n""; 
} 
exit;
</code></pre>

<p>Can I transform this somehow to make it read the data from a file of several columns and calculate the median for a chosen column? like typing  ./median.pl 1 column_numbers.tbl  on the shell command.
I tried this, but the file data.txt has only one column</p>

<pre><code>my (@vals, $med, $sum1, @numbers, @sorted);
open (COLUMN, ""&lt; data.txt"") || die ""Can not open file : $! "";
my @not_sorted = &lt;COLUMN&gt;;                  
close (COLUMN);
@sorted = sort { $a &lt;=&gt; $b } @not_sorted;  
if (@vals % 2 == 0) {  
$med = ($sorted[int($N/2)]);             
print ""MEDIAN = $med\n"";
}
else {  
$sum1 = $vals[(@vals/2)-1] + $vals[(@vals/2)]; 
$med = $sum1/2;
print ""MEDIAN = $vals[@vals/2]\n"";
};
</code></pre>

<p>Thanks for help.</p>
"
268,Draw a point a set distance away from a base point,"<p>I'm trying to figure out an algorithm for finding a random point a set distance away from a base point. So for example:</p>

<p><img src=""http://i34.tinypic.com/2e4vxao.png"" alt=""alt text""></p>

<p>This could just be basic maths and my brain not working yet (forgive me, haven't had my coffee yet :) ), but I've been trying to work this out on paper and I'm not getting anywhere.</p>

<p>Thanks,</p>

<p>Niall.</p>
"
269,An adaboost function,"<p>I am trying to implement adaboost with trees according to the algorithm of AdaBoost.M1 like the following<img src=""http://i.stack.imgur.com/LvKol.jpg"" alt=""enter image description here""></p>

<p>The training and testing data is generated in this way:</p>

<pre><code>p &lt;- 10
N.train &lt;- 2000
X.train &lt;- matrix(rnorm(p*N.train),nrow=N.train)
y.train &lt;- ifelse(apply(X.train^2,1,sum)&gt;qchisq(0.5,p),-1,1)
train &lt;- data.frame(y=y.train,X=X.train)
N.test &lt;- 10000 
X.test &lt;- matrix(rnorm(p*N.test),nrow=N.test)
y.test &lt;- ifelse(apply(X.test^2,1,sum)&gt;qchisq(0.5,p),-1,1)
test &lt;- data.frame(y=y.test,X=X.test)
plot(X.train[,1],X.train[,2],col=y.train+2)
</code></pre>

<p>And My function looks like:</p>

<pre><code>errorCalculation &lt;- function(fit_data, original_data) {
    cor = 0
    err = 0
    for(i in 1:length(fit_data)) {
        if (fit_data[i] == original_data[i]) {
            cor = cor + 1
        }
        else {
            err = err + 1
        }   
    }
    error = err/(err + cor) 
    return (error)
}



adaboost_m &lt;- function(train_x,train_y,test_x,test_y,M){
    N=length(train_y)
    wi=rep(1/N,N)
    model.control &lt;- rpart.control(minsplit =5, xval=10, cp=0)

    err_m &lt;- c(1:M)
    err_train &lt;- c(1:M)
    err_test &lt;- c(1:M)

    err_out_test &lt;- c(1:M)
    err_out_train &lt;- c(1:M)

    names(train_y) &lt;- c(""train_y"")
    train_x &lt;- data.frame(train_x)
    test_x &lt;- data.frame(test_x)        
    alpha &lt;- rep(1:M)
    y_out_test &lt;- rep(0,N)
    y_out_train &lt;- rep(0,N)
    for (i in 1:M){
        for (d in 1:N){
            train_x[d,] &lt;- train_x[d,]*wi[d]
        }

        dat1 &lt;- cbind(train_x,train_y)
        dat1 &lt;- data.frame(dat1)
        gm &lt;- rpart(train_y~., data= dat1, method=""class"", control=model.control)
        pred_train &lt;- predict(gm,train_x, type=""class"")
        pred_train &lt;- as.numeric(as.vector(pred_train))
        sigma_wI=0
        sigma_w=0
        for (b in 1:N) {
            pred_y = pred_train[b]
            train_y1 = train_y[b]
            if (pred_y==train_y1){
            I_i=0   
            }
            else{I_i=1}
            sigma_wI=sigma_wI+wi[b]*I_i
            sigma_w=sigma_w+wi[b]
        }
        err = sigma_wI/sigma_w
        err_m[i]=err
        alpha_m=log((1-err)/err)
        for (h in 1:N) {
            pred_y1 = pred_train[h]
            train_y11 = train_y[h]
            if (pred_y1==train_y11){
            I_j=0   
            }
            else{I_j=1}
            wi[h]= wi[h]*exp(alpha_m*I_j)
        }

        pred_output &lt;- predict(gm, test_x, type =""class"")
        pred_output &lt;- as.numeric(as.vector(pred_output))
        y_out_test &lt;- sign (y_out_test+alpha_m*pred_output)

        pred_train &lt;- predict (gm, train_x, type=""class"")
        pred_train &lt;- as.numeric(as.vector(pred_train))     
        y_out_train &lt;- sign (y_out_train+alpha_m*pred_train)


        err_m &lt;- errorCalculation(y_out_test, test_y)
        err_test &lt;- errorCalculation(y_out_test, test_y)
        err_train &lt;- errorCalculation(y_out_train, train_y)

#       err_output[i] &lt;- err_m
        err_out_test[i] &lt;- err_m
        err_out_train[i] &lt;- err_train
    }
#   return (err_out_test)
    return(list(err_out_test,err_out_train))
}
</code></pre>

<p>Now I face a problem:
The test error calculated by this function is about 50% and showing no trend of decreasing while the boost iterations increase.
Does anyone knows how to solve this?</p>
"
270,Reverse Bivariate Interpolation,"<p>I am given a table, indexed on 2 dimensions (x,y), with values(not necessarily ordered, though I think it isn't a horrendously unsafe assumption) z given, such that f(x, y) = z</p>

<p>So given an x and y, I interpolate to find a z value. Now given an x value(or y I suppose, not really important) and a z value, I need to find that y value that corresponds to the data. Is it possible to do this without the knowledge of an ordering in the z values of the table? If there is an ordering to the z values of the table is it possible? In my head, given ordering, it should be possible to find a unique solution, but I don't know how I can do it if I am not given ordering.</p>
"
271,Prolog parse postfix math expressions,"<p>I solved this my self. I'll post the solution when were past due date for my homework. </p>

<p>Okay, I'm going to build a parser or an evaluator. The de facto standard when parsing with prefix notation is to just use a stack. Add to the stack if input is a number, if it is an operator you can pop twice apply operator and put the result back on the stack. </p>

<p>The stack here would be a list, so I need to know how I can apply the operators. The input would be a string. ""(11+2*)"" This would 1+1=2*2=4. First it would read 1, and 1 to the stack. Read another 1 and add it to the stack. Now it reads ""+"", so it removes(pop) twice from the stack and apply + and puts the result back. Read 2, put 2 on the stack. Read *, pop twice and apply *. </p>

<p>Hope this makes sense. How would the predicate look like? I need one variable for the input string, one to maintain the stack, and one for the result? Three? </p>

<p>I'm especially wondering about push and pop on the stack as well as removing as I go from the input string. </p>
"
272,R finding duplicates in one column and collapsing in a second column,"<p>I have a data frame with two columns contacting character strings. in one column (named <code>probes</code>)  I have duplicated cases (that is, several cases with the same character string).  for each case in probes I want to find all the cases containing the same string, and then merge the values of all the corresponding cases in the second column (named <code>genes</code>) into a single case.
for example, if I have this structure:</p>

<pre><code>    probes  genes
1   cg00050873  TSPY4
2   cg00061679  DAZ1
3   cg00061679  DAZ4
4   cg00061679  DAZ4
</code></pre>

<p>I want to change it to this structure:</p>

<pre><code>    probes  genes
1   cg00050873  TSPY4
2   cg00061679  DAZ1 DAZ4 DAZ4
</code></pre>

<p>obviously there is no problem doing this for a single probe using which, and then paste and collapse</p>

<pre><code>ind&lt;-which(olap$probes==""cg00061679"")
genename&lt;-(olap[ind,2])
genecomb&lt;-paste(genename[1:length(genename)], collapse="" "")
</code></pre>

<p>but I'm not sure how to extract the indices of the duplicates in probes column across the whole data frame. any ideas?</p>

<p>Thanks in advance</p>
"
273,inverse of a cdf,"<p>I would like to compute the inverse cumulative density function (inverse cdf) of a given pdf. The pdf is directly given as a histogram, ie., a vector of N equally spaced components.</p>

<p>My current approach is to do :</p>

<pre><code>cdf = cumsum(pdf);
K = 3;   %// some upsampling factor
maxVal = 1;   %// just for my own usage - a scaling factor
M = length(cdf);
N = M*K;   %// increase resolution for higher accuracy
y = zeros(N, 1);
cursor = 2;
for i=1:N
   desiredF = (i-1)/(N-1)*maxVal;
   while (cursor&lt;M &amp;&amp; cdf(cursor)&lt;desiredF)
    cursor = cursor+1;
   end;    

   if (cdf(cursor)==cdf(cursor-1))
       y(i) = cursor-1;
   else        
       alpha = min(1, max(0,(desiredF - cdf(cursor-1))/(cdf(cursor)-cdf(cursor-1))));
       y(i) = ((cursor-1)*(1-alpha) + alpha*cursor )/maxVal;
   end;

end;

y = resample(y, 1, K, 0);
</code></pre>

<p>which means that I upsample with linear interpolation, inverse and downsample my histogram. This is rather an ugly code, is not very robust (if I change the upsampling factor, I can get really different results), and is uselessly slow... could anyone suggest a better approach ?</p>

<p>Note : the generalized inverse I am trying to compute (in the case the cdf is not invertible) is :   </p>

<pre><code>F^{-1}(t) = \inf{x \in R ; F(x)&gt;t }   
</code></pre>

<p>with F the cumulative density function</p>

<p>[EDIT : actually, K = 1 (ie., no upsampling) seems to give more accurate results...]</p>

<p>Thanks!</p>
"
274,Typing Mathematic Charachters on a web page,"<p>I found this interesting site, that helps me understand derivatives, <a href=""http://www.numberempire.com/derivatives.php"" rel=""nofollow"">http://www.numberempire.com/derivatives.php</a> , It seems it is writen in php, and I am wondering , how is it possible to print mathematic characters on a web page?</p>
"
275,2d outline algorithm for projected 3d mesh,"<p>Given: A 3d mesh defined with a set of vertices and triangles building up the mesh with these points.</p>

<p>Problem: Find the 2d outline of the projected arbitrarily rotated mesh on an arbitrary plane.</p>

<p>The projection is easy. The challenge lies in finding the ""hull"" of the projected triangle edges in the plane. I need some help with input/pointers on researching this algorithm. For simplicity, we can assume the 3d edges are projected straight down onto the xy plane.</p>

<p>Cheers</p>

<p>Edit: Added some pics for illustration</p>

<p>First image displays the projected edges in green with the convex hull in blue. The red lines in the second picture are the edges I'm hunting for :)</p>

<p><img src=""http://img14.imageshack.us/img14/2434/linear.gif"" alt=""alt text"" />
<img src=""http://img14.imageshack.us/img14/7647/linearred.gif"" alt=""alt text"" /></p>
"
276,weka in android?,"<p>i need to run a program that classifies certain dataset values. on the computer i am using weka to classify the same and provide it to me but i need to implement the same on android. eclipse shuts down when i run the program(weka) by giving an error such as PermGen heap space. i read several forums and found the option of changing the value in the <code>--launcher.XXMaxPermSize</code> in the eclipse.ini file but i have not found any success. i just need to use the j48 classifier from weka, is there anything else existing that would do the work for me? or is there a workaround to fix the PermGen error in eclipse?</p>

<p>please guide.</p>
"
277,why as.numeric function in R doesn't work properly?,"<p>I have these two characters and the ""as.numeric"" function doesn't work same for them. Can anyone help me why this is happening?</p>

<blockquote>
  <p>options(digits=22)</p>
  
  <p>a=""27""</p>
  
  <p>as.numeric(a)</p>
</blockquote>

<p>[1] 27.00000000000000000000</p>

<blockquote>
  <p>a=""193381411288395777""</p>
  
  <p>as.numeric(a)</p>
</blockquote>

<p>[1] 193381411288395776.0000</p>

<p>It can be seen that in the second case the last digit is not ""7"" and it is ""6"". Basically the ""as.numeric"" function decreases 1 unit from the number in the second case.</p>

<p>Any help is appreciated.</p>
"
278,plotting email flow in map using R,"<p>I want to plot email from and to in world. For example, I received the following number of emails from the following countries and I live in USA.</p>

<pre><code>recievedcountry &lt;- c(""India"", ""China"", ""France"", ""Chile"", ""Australia"", ""Chad"",
                    ""Nepal"", ""Burma"")
rfrequency &lt;- c(12, 20, 5, 2, 12, 1, 3, 2) # frequency of emails 
sendcountry &lt;- c(""Canda"", ""USA"", ""France"", ""India"", ""China"", ""Japan"")
sfrequency &lt;- c(14, 108, 12, 15, 18, 4)
</code></pre>

<p>This what I tried but I do not know how to conect with lines: </p>

<pre><code>require(fields)
world(xlim=c(-90,90),ylim=c(-180,180), xaxt = ""s"", yaxt = ""s"")
grid()
</code></pre>

<p>The following is my hypothesize model:</p>

<p><img src=""http://i.stack.imgur.com/e9HlN.jpg"" alt=""enter image description here""></p>
"
279,Normalize amplitude and phase with c#,"<p>I'm in the situation where i need to do some math related stuff in c# and for that i need some external libarys. The tool i look for should do the following actions:</p>

<ul>
<li>Process sound(wave/mp3):
<ul>
<li>Normalize the amplitude</li>
<li>Normalize the phase</li>
</ul></li>
</ul>

<p>Any idea which way to go? And is there a big difference if I should to it on mp3 instead of wav</p>

<p>Michael.</p>
"
280,Get coordinates of line-surrounding box,"<p>I've been working in JavaScript to code a line drawing system. I'd like the lines drawn to be selectable, so I've been attempting to implement line-highlighting. As you can see in the image below, I have a line (in black) with known coordinates and an equation in slope-intercept (y=mx+b). How can I calculate the corners' (circled in green) coordinates, knowing the box's radius?</p>

<p><img src=""http://i.stack.imgur.com/aVI2p.png"" alt=""line box example""></p>
"
281,learning maths for statistics,"<p>Apologies if I have posted this in the wrong place first off. </p>

<p>My work has taken me into a unexpectantly large amount of statistics. In order to really understand what I am doing I need to understand these statistical approaches. However it seems almost impossible to do that unless you speak math (I don't as I'm sure you can tell). </p>

<p>So my question is are there any free online resources for learning/ getting familiar with terminology, symbols, concepts commonly used in maths/statistics that are aimed at people with basic math skills (basic algebra is okay, can barely remember how to differentiate).</p>

<p>Thanks in advance,
Cheers,
Davy</p>

<p>Edit to say:<br>
I should mention that I am somewhat (not very) familiar with basic concepts like variance, hypothesis testing, and regression, but only on a practical level. I want understand these on the mathematical level on which they were built. Method development is something I am interested in, but I'll never be able to do, If I don't start learning more mathematics. MY problem thus far seem that every book on statistics assume you don't want to see any maths at all, or your totally familiar with giant equations, which are therefore, not explained well to someone who has no idea what (probably) commonplace symbols mean. I guess what I am really looking for is an induction to the domain specific language of maths, with a view towards stats.</p>
"
282,data() command in RStudio,"<p>I have just realized that the  <code>data()</code> command does not work in RStudio.
It normally gives you a list of all datasets that are available to you (depending on which packages you are using).</p>

<p>Does anyone know of a workaround?</p>

<p>A very bad workaround it this:</p>

<pre><code>x &lt;- edit(data())
</code></pre>

<p>at least it does give you the list after several path and other listings.</p>
"
283,Sourcing a file,"<p>How can I source a file in the R console? Every time I try <code>(File, Source File, Select file)</code> it comes back with the error message: </p>

<pre><code>Error in source(""/Users/valbruno11/Desktop/MarkMatrix.txt"") : 
  /Users/valbruno11/Desktop/MarkMatrix.txt:2:3: unexpected symbol
1: 
2: R version
 ^
</code></pre>

<p>Any advice would be appreciated.</p>
"
284,calculating time difference in R,"<p>I have a data with more that 3 million records having start.time and end.time as two of the variables. The first 10 obs are as follows:</p>

<pre><code>   start.date start.time   end.date end.time
1  2012-07-13   15:01:32 2012-07-13 15:02:42
2  2012-07-05   18:26:31 2012-07-05 18:27:19
3  2012-07-14   20:23:21 2012-07-14 20:24:11
4  2012-07-29   16:09:54 2012-07-29 16:10:48
5  2012-07-21   14:58:32 2012-07-21 15:00:17
6  2012-07-04   15:36:31 2012-07-04 15:37:11
7  2012-07-22   18:28:31 2012-07-22 18:28:50
8  2012-07-09   21:08:42 2012-07-09 21:09:02
9  2012-07-05   09:44:52 2012-07-05 09:45:05
10 2012-07-02   18:50:47 2012-07-02 18:51:38
</code></pre>

<p>I need to calculate the difference between start.time and end.time.</p>

<p>I used the following code:</p>

<pre><code>mbehave11$diff.time &lt;- difftime(mbehave11$end.time, mbehave11$start.time, units=""secs"")
</code></pre>

<p>But I am getting this error:</p>

<pre><code>Error in as.POSIXlt.character(x, tz, ...) : 
  character string is not in a standard unambiguous format
In addition: Warning messages:
1: In is.na.POSIXlt(strptime(xx, f &lt;- ""%Y-%m-%d %H:%M:%OS"", tz = tz)) :
  Reached total allocation of 1535Mb: see help(memory.size)
</code></pre>

<p>I need urgent help in this. Please please somebody help me.</p>

<p>Thanks</p>
"
285,Solving the water jug problem,"<p>While reading through some <a href=""http://ocw.mit.edu/NR/rdonlyres/Electrical-Engineering-and-Computer-Science/6-042JFall-2005/E7AB2626-FB19-480E-95F5-A9C23C2BD885/0/ln6.pdf"" rel=""nofollow"">lecture notes</a> on preliminary number theory, I came across the solution to 
water jug problem (<strong>with two jugs)</strong> which is summed as thus:</p>

<p>Using the property of the G.C.D of two numbers that GCD(a,b) is the smallest possible linear combination of a and b, and hence a certain quantity Q is only measurable by the 2 jugs, iff Q is a n*GCD(a,b), since Q=sA + tB, where:</p>

<pre><code>n = a positive integer
A = capacity of jug A
B=  capacity of jug B
</code></pre>

<p>And, then the method to the solution is discussed</p>

<p>Another model of the solution is to model the various <em>states</em> as a state-space search problem as often resorted to  in Artificial Intelligence.</p>

<p>My question is: What other known methods exist which models the solution, and how? Google didn't throw up much.</p>
"
286,color co-occurrence histogram,"<p>I went through the following paper: </p>

<p><a href=""http://research.microsoft.com/pubs/68706/cvpr1.pdf"" rel=""nofollow"">http://research.microsoft.com/pubs/68706/cvpr1.pdf</a></p>

<p>which is about color co-occurrence histogram for object recognition. It has the advantage of adding spacial information comparing to the color histograms. </p>

<p>I didnt understand how to effectively implement it. </p>

<p>Does anybody knows where can I get the code for this or some guides to implement it using matlab? </p>
"
287,Lower bound on sample size needed for e-net,"<p>Say we have a probability distribution over a set X, where $|X| = d$, and a class of subsets of X, $H \subseteq 2^X$.  </p>

<p>We wish to find an $\epsilon$-net for X.  What is the lower bound on the number of independent samples needed to create an $\epsilon$-net for X?</p>

<p>The lower bound that we are looking for is $\Omega(\delta/\epsilon)$.  I know that our ""worst-case"" probability distribution will assign a probability of $1-\epsilon$ to one element in X, and $\epsilon/(d-1)$ probability to the other elements in X.  I'm confused about how to proceed from here, however.</p>
"
288,How to export Instrument's CPU Monitor's statistics to be usable in Excel or Numbers?,"<p>I require statistics of CPU Load of my iPhone apps. I am trying to use Instrument to see the CPU Load, but all I see in Instrument program is rendered graphs. I need these statistics data in raw numbers so that I can put all of them on graphs using Excel or Numbers. Is there a way to export these data in such a way? Or do I need other programs to accomplish this?</p>
"
289,Probability of 2 people drawing same A and same B,"<pre><code>RED  box contains 64510 red  numbered balls: {R1,R2,...,R64510}  
BLUE box contains 65536 blue numbered balls: {B1,B2,...,B65536}  

1st person: takes a Red and a Blue, records the numbers like (Rx,By) 
            and puts them back inside.  
2nd person: does the same.  
</code></pre>

<p>1a) What is the possibility that both people will draw the same (Rx,By) ?<br>
1b) If the whole process is repeated, what is the possibility of this happening again ?  </p>

<pre><code>RED box remains as is: 64510 red  numbered balls: {R1,R2,...,R64510}  
GREEN  box contains 256 green  numbered balls: {G1,G2,...,G256}  
YELLOW box contains 256 yellow numbered balls: {Y1,Y2,...,Y256} 
</code></pre>

<p>2a) What is the possibility that both people will draw the same (Rx,Gy,Yz) ?<br>
2b) If the whole process is repeated, what is the possibility of this happening again ?  </p>

<p><em>This is not a math homework, i am designing an application protocol, and i will use the method with the smallest collision possibility.<br>
Thanks in advance :-)</em></p>
"
290,Android application statistics,"<p>Is there an API for application statistics: number of installations, number of downloads, active apps, etc.?</p>

<p>Also is there a way to know the number of the buyer? Meaning - a user was the ""799th buyer"" of a certain app.</p>
"
291,Math with table row values on Ruby on Rails,"<p>Lets supose I have a User table, and on this table I have height, weight and result rows.
In a form I input height and weight values and want to multiply those two values and store on the resulta row on database. I was trying something like that</p>

<pre><code>  def create
    @user = User.new(params[:user])
    @result = @user.height * @user.weight
    @user.result = @result
</code></pre>

<p>But its not working, what am i doing wrong?</p>
"
292,Vector direction equations,"<p>I have three vectors in 3D space, one is a light source, one is a ray and one is the point on a circle a ray hits. With this information, how can I work out the vector which points back at the light source from the point the ray hits the circle?</p>
"
293,trouble making a decision on where to invest my time with big data analyses in R,"<p>I know R, I know SQL, I use Windows, I have a budget of $0, I have a terabyte of data, I have twelve processors, I have 96GB of RAM, I am motivated to learn new software if the speed gains will pay off in the long term.</p>

<p>I need to run descriptive statistics and regressions.</p>

<p>I have too many options.  Where should I devote all of my energy? Thanks.</p>
"
294,Converting string to numeric,"<p>I've imported a test file and tried to make a histogram </p>

<pre><code>pichman &lt;- read.csv(file=""picman.txt"", header=TRUE, sep=""/t"")   
hist &lt;- as.numeric(pichman$WS)    
</code></pre>

<p>However, I get different numbers from values in my dataset. Originally I thought that this  because I had text, so I deleted the text:</p>

<pre><code>table(pichman$WS)    
ws &lt;- pichman$WS[pichman$WS!=""Down"" &amp; pichman$WS!=""NoData""]    
</code></pre>

<p>However, I am still getting very high numbers does anyone have an idea?</p>
"
295,correlation in errors,"<p>I'm not good in statistics, so please excuse my noob question.
We want to ask a question from people (say what is $2+2$). They might make mistake. We assume that they give the correct answer with the same probability $p$. In case their error rates are not correlated, the probability that they both give the wrong answer is $(1-p)^2$
However, their error rates are not independent, they are correlated. In this case, how can we reason about the accuracy of the result ? Do we need to know the distribution of answers?</p>
"
296,how to best utilize LCD display on my home server,"<p>I've got a small 14"" LCD display on my home server that i keep attached whenever i want to access the console directly (I usually connect via RDP).. this is a windows 2008R2 box for the record. </p>

<p>I'd like to keep the LCD on and have it display useful stats - e.g. CPU temp, drive space free, memory/cpu load, pagefile size,  incoming/outgoing IO, # of connections, etc etc etc. </p>

<p>Does anyone know of a screensaver or desktop customization app that will give me an aesthetically pleasing display of all this stuff?</p>
"
297,Scala and Java BigDecimal,"<p>I want to switch from Java to a scripting language for the Math based modules in my app. This is due to the readability, and functional limitations of mathy Java.</p>

<p>For e.g, in Java I have this:</p>

<pre><code>BigDecimal x = new BigDecimal(""1.1"");
BigDecimal y = new BigDecimal(""1.1"");
BigDecimal z = x.multiply(y.exp(new BigDecimal(""2""));
</code></pre>

<p>As you can see, without BigDecimal operator overloading, simple formulas get complicated real quick.</p>

<p>With doubles, this looks fine, but I need the precision.</p>

<p>I was hoping in Scala I could do this:</p>

<pre><code>var x = 1.1;
var y = 0.1;
print(x + y);
</code></pre>

<p>And by default I would get decimal-like behaviour, alas Scala doesn't use decimal calculation by default.</p>

<p>Then I do this in Scala:</p>

<pre><code>var x = BigDecimal(1.1);
var y = BigDecimal(0.1);
println(x + y);
</code></pre>

<p>And I still get an imprecise result.</p>

<p>Is there something I am not doing right in Scala?</p>

<p>Maybe I should use Groovy to maximise readability (it uses decimals by default)?</p>
"
298,Importing rpy2 modules in Enthought Canopy python,"<p>I reinstalled R and rpy2 after installing Enthought Canopy python.</p>

<p>In python I can now:</p>

<pre><code>import rpy2
</code></pre>

<p>without an error message.
However, when I want to import any specific rpy2 modules, I get the following error:</p>

<pre><code>import rpy2.robjects
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-2-bd461266c9bc&gt; in &lt;module&gt;()
----&gt; 1 import rpy2.robjects

/Users/martinkampmann/Library/Enthought/Canopy_32bit/User/lib/python2.7/site-packages/rpy2-2.3.5-py2.7-macosx-10.6-i386.egg/rpy2/robjects/__init__.py in &lt;module&gt;()
     13 import itertools
     14 from datetime import datetime
---&gt; 15 import rpy2.rinterface as rinterface
     16 import rpy2.rlike.container as rlc
     17 

/Users/martinkampmann/Library/Enthought/Canopy_32bit/User/lib/python2.7/site-packages/rpy2-2.3.5-py2.7-macosx-10.6-i386.egg/rpy2/rinterface/__init__.py in &lt;module&gt;()
     99 
    100 
--&gt; 101 from rpy2.rinterface._rinterface import *
    102 
    103 

ImportError: dlopen(/Users/martinkampmann/Library/Enthought/Canopy_32bit/User/lib/python2.7/site-packages/rpy2-2.3.5-py2.7-macosx-10.6-i386.egg/rpy2/rinterface/_rinterface.so, 2): Symbol not found: _R_BaseEnv
  Referenced from: /Users/martinkampmann/Library/Enthought/Canopy_32bit/User/lib/python2.7/site-packages/rpy2-2.3.5-py2.7-macosx-10.6-i386.egg/rpy2/rinterface/_rinterface.so
  Expected in: flat namespace
 in /Users/martinkampmann/Library/Enthought/Canopy_32bit/User/lib/python2.7/site-packages/rpy2-2.3.5-py2.7-macosx-10.6-i386.egg/rpy2/rinterface/_rinterface.so
</code></pre>
"
299,numpy read CSV file where some fields have commas?,"<p>I'm trying to read a CSV file using <code>numpy.recfromcsv(...)</code> where some of the fields have commas in them. The fields that have commas in them are surrounded by quotes i.e., <code>""value1, value2""</code>. Numpy see's the quoted field as two different fields and it doesn't work very well. The command I'm using right now is</p>

<pre><code>    data = numpy.recfromcsv(dataFilename, delimiter=',', autstrip=True)
</code></pre>

<p>I found this question</p>

<blockquote>
  <p><a href=""http://stackoverflow.com/questions/8311900/python-read-csv-file-with-comma-within-fields"">Python: read CSV file with comma within fields</a></p>
</blockquote>

<p>But it doesn't use <code>numpy</code>, which I'd really love to use.
So I'm hoping there are at least one of a few options here:</p>

<ol>
<li>What are some options to <code>numpy.recfromcsv(...)</code> that will allow me to read a quoted field as one field instead of multiple comma separated fields?</li>
<li>Should I format my CSV file differently?</li>
<li>(alternatively, but not ideally) Read CSV as in quoted question, with extra steps to create <code>numpy</code> array.</li>
</ol>

<p>Please advise.</p>
"
300,What is the distribution of $\bar x$?,"<blockquote>
  <p>Let $x_1,x_2,\ldots,x_n$ be a random sample from a normal distribution with mean $\mu$ and and variance $\sigma^2$. show that $E[\bar x]=\mu$ and $V[\bar x]=\sigma^2 /n$, where $\bar x = \frac1n\sum_i x_i$ is the arithmetic mean of the $x_i$.</p>
</blockquote>
"
301,Split a transform matrix into orhhogonal matrix and scale matrix,"<p>If I have a matrix from scale, translate, and rotation transform. I want to split this matrix to two matrix. One is rotation+translation matrix, the other is scale matrix. </p>

<p>Because I want to compute the correct normal vector transform, so I only need orthogonal matrix to do the computation for surface normal vector</p>

<p>Any ideas?</p>
"
302,Exposing C++ class with Rcpp,"<p>I've been playing around with Rcpp and a couple of questions are currently popping up...</p>

<p>From my understanding, if you want to expose a C++ class to R you need to write partial template specializations for Rcpp::wrap and Rcpp::as. I looked how this was done in the Rcpp::Date class and I have the following questions:
- In Date.h we have:</p>

<pre><code>// template specialisation for wrap() on the date
// OK as explained in docs for non intrusive 
// partial template specialization
template &lt;&gt; SEXP wrap&lt;Rcpp::Date&gt;(const Rcpp::Date &amp;date);
</code></pre>

<p>Further down the header you have the following code:</p>

<pre><code>template&lt;&gt; inline SEXP wrap_extra_steps&lt;Rcpp::Date&gt;( SEXP x ){
Rf_setAttrib( x, R_ClassSymbol, Rf_mkString( ""Date"" ) ) ;
return x ;
}
</code></pre>

<p>What is the wrap_extra_steps supposed to do? Is it required? Also in Date.cpp wrap method is implemented as follows:</p>

<pre><code>template &lt;&gt; SEXP wrap(const Date &amp;date) {
   return internal::new_date_object( date.getDate() ) ;
}
</code></pre>

<p>With the internal::new_date_object implemented as:</p>

<pre><code>SEXP new_date_object( double d){
   SEXP x = PROTECT(Rf_ScalarReal( d ) ) ;
   Rf_setAttrib(x, R_ClassSymbol, Rf_mkString(""Date""));
   UNPROTECT(1);
   return x;
}
</code></pre>

<p>OK I understand that an SEXP is created and returned to R, but I don't get the whole part with PROTECT(), Rf_setAttrib, UNPROTECT...what's happening here?</p>

<p>Thanks!     </p>
"
303,R - Logical operators,"<p>Please can anyone advise how I can turn the following statement into one that will do the same thing but NOT using ifelse please?  </p>

<pre><code>&lt;-ifelse(y&gt;=50, 0.2*x+0.8*y, ifelse(y&lt;50 &amp; x&gt;70, y+10, ifelse(y&lt;50 &amp; x&lt;70, y))) 

x=80
y=60
</code></pre>

<p>So I the final code should give an answer of 64 - selecting the first condition.  I will then test it to ensure the other 3 conditions give the correct result for varying values of x and y</p>

<p>Thanks a lot.</p>
"
304,Find the day of a week in R,"<p>Let's say that I have a date in R and it's formatted as follows.</p>

<pre><code>   date      
2012-02-01 
2012-02-01
2012-02-02
</code></pre>

<p>Is there any way in R to add another column with the day of the week associated with the date? The dataset is really large, so it would not make sense to go through manually and make the changes.</p>

<pre><code>df = data.frame(date=c(""2012-02-01"", ""2012-02-01"", ""2012-02-02"")) 
</code></pre>

<p>So after adding the days, it would end up looking like:</p>

<pre><code>   date       day
2012-02-01   Wednesday
2012-02-01   Wednesday
2012-02-02   Thursday
</code></pre>

<p>Is this possible? Can anyone point me to a package that will allow me to do this?
Just trying to automatically generate the day by the date.</p>
"
305,temporary tables using RMySQL,"<p>Is there a way to create a temporary table using the <code>RMySQL</code> package?  If so what is the correct way to do it?  In particular I am trying to write a dataframe from my R session to the temporary table.  I have several processes running in parallel and I don't want to worry about name conflicts, that's why I want to make them temporary so they are only visible to each individual session.  The solution should somehow involve <code>dbWritetable</code> and not <code>dbSendQuery(""create temporary table tbl;"")</code>.</p>

<p>NOTE: I found some stuff on the net that suggests creating a temporary table manually using <code>dbSendQuery(con, ""create temporary table x (x int)"")</code> and then simply overriding it with <code>dbWriteTable()</code>.  This does not work.</p>
"
306,R ggplot2 Graph: Month-Year bar graph plot faceted and filled on year; with data input as date in char format,"<p>Environment: Win 7 HP, R v2.15.1</p>

<p>What I wish to get to: </p>

<ul>
<li>Plot y (numeric) vs x (date) with </li>
<li>labels month-year abbrev, sorted mon+year, las2 vertically aligned</li>
<li>colours filled by year </li>
<li>facet-grid by year</li>
</ul>

<p>I have tried different approaches after reading up different threads in this forum, but unable to get what I need. Need help. Attaching sample data and results.</p>

<p>MySample Data</p>

<pre><code>x &lt;- c(""04-01-10"",""05-01-10"",""06-01-10"",""07-01-10"",""08-01-10"",""09-01-10"",""10-01-10"",""11-01-10"",""12-01-10"",""01-01-11"",""02-01-11"",""03-01-11"",""04-01-11"",""05-01-11"",""06-01-11"",""07-01-11"",""08-01-11"",""09-01-11"",""10-01-11"",""11-01-11"",""12-01-11"",""01-01-12"",""02-01-12"",""03-01-12"",""04-01-12"",""05-01-12"",""06-01-12"")
y &lt;- c(120,210,130,160,190,210,80,70,110,120,140,160,130,200,110,180,210,200,90,60,100,100,120,170,100,180,120)
</code></pre>

<p>x is date (character) in mm-dd-yy format tz:IST (Calcutta / Asia)
data has only single y value per month which is on the start date of the month</p>

<p>Convert to Data Frame</p>

<pre><code>MySample &lt;- data.frame(x) ## convert to dataframe 
MySample$y &lt;- y
</code></pre>

<p>load required libraries</p>

<pre><code>require(lubridate) 
require(ggplot2)
</code></pre>

<p>MySample Base Plot</p>

<p>1) Plot x vs y</p>

<pre><code>    ggplot(MySample, aes(MySample$x, MySample$y)) + 
        geom_bar(y=MySample$y,stat=""identity"") 
</code></pre>

<p>Gave me base plot results</p>

<p>2) Plot x vs y + fill=year</p>

<pre><code>    ggplot(MySample, aes(MySample$x, MySample$y, fill=year(MySample$x))) + 
        geom_bar(y=MySample$y,stat=""identity"")
</code></pre>

<p>gave me fills but have 5 fill years with 2010,2010.5,2011,2011.5,2012</p>

<p>I have tried different approaches but running into one error or another.</p>

<p>3) Plot x vs y + fill=year + facet_grid(year)</p>

<pre><code>    ggplot(MySample, aes(x, y, fill=year(x))) + 
        geom_bar(y=MySample$y,stat=""identity"") + 
        facet_grid(. ~ year(MySample$x)) 
</code></pre>

<p>Get : Error in layout_base(data, cols, drop = drop) : 
      At least one layer must contain all variables used for facetting</p>

<p>4) Plot x vs y + fill=year + facet_grid(year) + labels-month (abbr)</p>

<pre><code>    ggplot(MySample, aes(x, y, fill=year(x))) + 
        geom_bar(y=MySample$y,stat=""identity"") + 
        scale_x_date(labels=month(MySample$x,label=TRUE,abbr=TRUE))
</code></pre>

<p>Get : Error in scale_labels.continuous(scale, major) : Breaks and labels are different lengths</p>

<p>I'm stuck and need help to move forward.
Need solutions to address the following requirements: </p>

<ol>
<li>3 fill years only - 2010,2011,2012</li>
<li>xlabels - %b%y format; sorted on month-year sequence; las2 positioned (vertical)</li>
<li>facet_grid by year with only that year's xlabels and bars in the appropriate facet-grid</li>
</ol>
"
307,How are numpy arrays implemented internally?,"<p>Suppose I use numpy arrays (e.g. <code>numpy.ndarray</code>) to store large, sparse matrices (i.e., most of the entries are 0): Do the zero entries actually occupy memory? Does numpy support sparse arrays, and if yes, which <a href=""http://en.wikipedia.org/wiki/Sparse_matrix#Storing_a_sparse_matrix"" rel=""nofollow"">storage format</a> is used?</p>
"
308,Find probability given frequency for event,"<p>Lets say we have three channels with probability of gender based view like this:  </p>

<p>chA->men = 0.3<br>
chB->men = 0.7<br>
chC->men = 0.8  </p>

<p>In a week sample,a user views total 1000 impressions.<br>
Out of which the views on channels were:<br>
chA= 10<br>
chB= 100<br>
chC=50  </p>

<p>Ques: What is the probability that user is a man?  </p>

<p>Thanks,</p>
"
309,Fastest way to reshape variable values as columns,"<p>I have a dataset with about 3 million rows and the following structure:</p>

<pre><code>PatientID| Year | PrimaryConditionGroup
---------------------------------------
1        | Y1   | TRAUMA
1        | Y1   | PREGNANCY
2        | Y2   | SEIZURE
3        | Y1   | TRAUMA
</code></pre>

<p>Being fairly new to R, I have some trouble finding the right way to reshape the data into the structure outlined below:</p>

<pre><code>PatientID| Year | TRAUMA | PREGNANCY | SEIZURE
----------------------------------------------
1        | Y1   | 1      | 1         | 0
2        | Y2   | 0      | 0         | 1
3        | Y1   | 1      | 0         | 1
</code></pre>

<p>My question is: What is the fastest/most elegant way to create a data.frame, where the values of PrimaryConditionGroup become columns, grouped by PatientID and Year (counting the number of occurences)?</p>
"
310,reading a big xls file into R,"<p>I have an excel file with ~10000 rows and ~250 columns, currently I am using RODBC to do the importing:</p>

<pre><code>channel &lt;- odbcConnectExcel(xls.file=""s:/demo.xls"")
demo &lt;- sqlFetch(channel,""Sheet_1"")
odbcClose(channel)
</code></pre>

<p>But this way is a bit slow (I need a minute or two to import them), and the excel is originally encrypted, I need to remove the password to work on it, which is something that I prefer not to, I wonder if there is any better way (i.e. import faster, and capable of importing encrypted excel files)</p>

<p>Thanks.</p>
"
311,Find reverse percentage,"<p>I have a function that sets a bootstrap progress bar based on a number it finds in a div .days-due. It works fine but the progress bar is opposite width that I want. </p>

<p>How can I reverse the progBarValue number?</p>

<pre><code>function daysUntil(year, month, day) {
  var now = new Date(),
      dateEnd = new Date(year, month - 1, day), // months are zero-based
      days = (dateEnd - now) / 1000/60/60/24;   // convert milliseconds to days

  return Math.round(days);
}

// find percentage to due date
$('#paging1 ul li').each(function () {

    var monthDue = $(this).find('.month').text();
    var dayDue = $(this).find('.day').text();
    var yearDue = $(this).find('.year').text();

    $(this).find('.days-due').text(daysUntil(yearDue, monthDue, dayDue));

    // progress bar
    // find number of days until due date
    var progBarValue = $(this).find('.days-due').text();
    // limit days due to no more than 100%
    progBarValue = progBarValue &gt; 100 ? 100 : progBarValue;
    // set progress bar width
    $(this).find('.bar').width(progBarValue +""%"");

});
</code></pre>
"
312,Jquery Trigonmetry?,"<p>I have a need to calculate the adjacent angle of a right angle triangle formed by a rectangle that is dictated by two sets of points. My plan is to use offset() on two elements to get absolute position co-ordinates, then determine the internal angle from the bottom left corner of this implied box so that i may rotate a thin rectangle element using css rotate.</p>

<p>Possible? </p>

<p>The only part i wouldn't know how to do right off the bat is the trigonometry syntax, and whether i need an external library for that.</p>

<p>*Note: I do understand that css rotate would rotate around the center of the element, so i would have to shift the rectangle over to compensate.</p>
"
313,RMySQL dbWriteTable to a table with a MySQL reserved word as name,"<p>I am having trouble with the dbWriteTable command from the RMySQL package. I have to append records to a table name which is called 'order', a reserved word in MySQL.</p>

<pre><code>dbWriteTable(connection, ""`order`"", df, append = T)
</code></pre>

<p>Give as error:</p>

<blockquote>
  <p>Warning message:
  In mysqlWriteTable(conn, name, value, ...) :
   could not create table: aborting mysqlWriteTable</p>
</blockquote>

<p>Other queries like SELECT work fine as long as I put order between back ticks.</p>

<p>Any ideas how the execute the dbWriteTable command? And renaming the table is unfortunately no option.  </p>
"
314,setting spacing between grouped bar plots in matplotlib,"<p>I'm trying to make a grouped bar plot in matplotlib, following the example in the gallery. I use the following:</p>

<pre><code>import matplotlib.pyplot as plt
plt.figure(figsize=(7,7), dpi=300)
xticks = [0.1, 1.1]
groups = [[1.04, 0.96],
          [1.69, 4.02]]
group_labels = [""G1"", ""G2""]
num_items = len(group_labels)
ind = arange(num_items)
width = 0.1
s = plt.subplot(1,1,1)
for num, vals in enumerate(groups):
    print ""plotting: "", vals
    group_len = len(vals)
    gene_rects = plt.bar(ind, vals, width,
                         align=""center"")
    ind = ind + width
num_groups = len(group_labels)
# Make label centered with respect to group of bars
# Is there a less complicated way?
offset = (num_groups / 2.) * width
xticks = arange(num_groups) + offset
s.set_xticks(xticks)
print ""xticks: "", xticks
plt.xlim([0 - width, max(xticks) + (num_groups * width)])
s.set_xticklabels(group_labels)
</code></pre>

<p><img src=""http://i.stack.imgur.com/tPn37.png"" alt=""enter image description here""></p>

<p>My questions are:</p>

<ol>
<li><p>How can I control the space between the groups of bars? Right now the spacing is huge and it looks silly. Note that I do not want to make the bars wider - I want them to have the same width, but be closer together.</p></li>
<li><p>How can I get the labels to be centered below the groups of bars? I tried to come up with some arithmetic calculations to position the xlabels in the right place (see code above) but it's still slightly off... it feels a bit like writing a plotting library rather than using one. How can this be fixed? (Is there a wrapper or built in utility for matplotlib where this is default behavior?)</p></li>
</ol>

<p><strong>EDIT:</strong> Reply to @mlgill: thank you for your answer. Your code is certainly much more elegant but still has the same issue, namely that the width of the bars and the spacing between the groups are not controlled separately. Your graph looks correct but the bars are far too wide -- it looks like an Excel graph -- and I wanted to make the bar thinner.</p>

<p>Width and margin are now linked, so if I try:</p>

<pre><code>margin = 0.60
width = (1.-2.*margin)/num_items
</code></pre>

<p>It makes the bar skinnier, but brings the group far apart, so the plot again does not look right.</p>

<p>How can I make a grouped bar plot function that takes two parameters: the width of each bar, and the spacing between the bar groups, and plots it correctly like your code did, i.e. with the x-axis labels centered below the groups?</p>

<p>I think that since the user has to compute specific low-level layout quantities like margin and width, we are still basically writing a plotting library :) </p>
"
315,Bar plot mean from two or more files in R,"<p>I have two text files, A.txt and B.txt (perhaps more in the future). Each text file looks like this (but with different values):</p>

<pre><code>1
2
5
6
7
</code></pre>

<p>I would like to plot the average of each row in the text files, for example, combine the text files into a matrix that looks something like this:</p>

<pre><code>1 5
2 7
5 5
6 9
7 7
</code></pre>

<p>And then make a bar plot of 5 bars, each representing the average for each row (1+5/2, 2+7/2, etc.). I would also like to plot error bars representing the standard error for each bar graph (assuming I have more than one text file).</p>

<p>I'm able to do the following for a single text file:</p>

<pre><code>my.data &lt;- read.table('A.txt')
barplot(t(my.data))
</code></pre>

<p>Which will graph one text file. But not sure how to take the two text files, combine them, graph the mean, and produce error bars. Any suggestions or links to resources?</p>
"
316,How do I plot a graph of values against non-continuous dates using R?,"<p>I have a list of about 50 values and corresponding non-continuous dates in a pdf of which I need to make a time series graph in R. How do I do this? </p>

<p>No response can be too detailed or basic. Thanks.</p>
"
317,Bernoulli Random Variable,"<p>Let $X$ be a Bernoulli random variable with probability of success $p$. Answer the following questions. </p>

<ul>
<li>(i) Derive the formulas for the mean, the variance, and the standard deviation of X.</li>
<li>(ii) Find the third and fourth moments of $X$.</li>
</ul>
"
318,Giving sampling distribution of population total estimator [Homework],"<p>Given sampling data below,</p>

<p><img src=""http://i.stack.imgur.com/eokKD.png"" alt=""sampling data""></p>

<p>Considering a Simple Random Sampling(SRS without replacement) of size 4, what is the sampling distribution of the estimators for total of y's? t1 and t2 where <br>
i) t1 is the estimate of population total of y using Simple Random Sampling <br>
ii) t2 is the estimate of population total of y using Ratio estimator</p>

<p>I would begin by obtaining sampling distribution of y's,</p>

<p><img src=""http://i.stack.imgur.com/psBbX.png"" alt=""sampling distribution of yi""></p>

<p>followed by their totals,</p>

<p><img src=""http://i.stack.imgur.com/ZrafF.png"" alt=""sampling distribution of totals""></p>

<p>how do i obtain the p(ti) for both SRS and Ratio estimators? Thanks for reading through my questions. Looking forward to great advices.</p>
"
319,how to insert the gap back into a time series so that there is no gap in the result time series?,"<p>We have retailer data stored in a database, if there is no sale for one item, then there is no row for that item in the database. So after fetch the data from database to R, there will be same gap in the result time series. Is there any way the remove the time series in R (insert back the missing data with value of 0)? </p>
"
320,Javascript: Math.random,"<p>If num parameter is 52, how many possible return values are there? is it 52 or 53? If I understand this correctly, Math.random uses random values from 0 to 1 inclusive. If so, then 0 is a possible return value and so is 52. This results in 53 possible return values. Is this correct? Reason I ask is that a book that I'm learning from uses this code for a deck of cards. I wonder if num should equal 51 ? </p>

<p>Thanks ...</p>

<pre><code>function getRandom(num) {
    var my_num = Math.floor(Math.random * num);
    return my_num;
};
</code></pre>
"
321,Row aggregation when values are close enough in a column,"<p>I have a dataframe with 2 columns</p>

<pre><code>        time     x
  1306247226     5
  1306247236    10
  1306248127    20
  1306248187    36
  1306249248    28
  1306249258    24
  1306249259    20
  ...
</code></pre>

<p>I'd like to aggregate the rows whose values in the 'time' column are close enough
(eg. let's say their difference is less than 60.) and sum their 'x' values in the aggregated row. The 'time value in the aggregated row will be the one of the first row of the aggregation. ('time' is an unix timestamp)</p>

<p>The goal is to have as output of this example:</p>

<pre><code>        time     x
  1306247226    15
  1306248127    20
  1306248187    36
  1306249248    72
  ...
</code></pre>

<p>The dataset is quite big, a 'for' loop will take a long time... but if it is the only option I can deal with it and wait.
Any idea?</p>

<p>Thanks a lot!</p>
"
322,An efficient way to grow a data frame using a by function,"<p>I need to do some analysis on vehicles that are identified by their <code>ID</code>. The results of this analysis will include some <code>numeric</code>, <code>factor</code>, and <code>logical</code> information. All the data used in the analysis is in one data frame, so that the function goes like this:</p>

<pre><code>Results &lt;- by(Data, Data$ID, Function)
</code></pre>

<p>Where <code>Function</code> is designed to give output like this:</p>

<pre><code>Function &lt;- function(DF) {
                          ## Do stuff...
                          return(c(23.2, as.factor(""SuperFast""), TRUE))
                         }
</code></pre>

<p>What's been great about this approach so far is that in addition to being quite fast (taking ~1 min where a <code>for</code> loop took hours), it's easy to put in <code>data.frame</code> format by:</p>

<pre><code>as.data.frame(do.call(""rbind"", Results))
</code></pre>

<p>But of course, <code>c</code> in <code>Function</code> <strong>and</strong> <code>""rbind""</code> in <code>do.call</code> coerce everything into the same object type. To resolve this, I've been making <code>Function</code> spit out a character vector (like <code>as.character(23.2, ""SuperFast"", TRUE)</code> and then changing object types manually at the end. </p>

<p>Is there (1) a way to return something that can be a <code>row</code> in a dataframe that has different object types or (2) a better method than using <code>by</code> and <code>c</code> (for rows)?</p>

<p>Just for kicks, here's something that can be used for Data:</p>

<pre><code>Data &lt;- data.frame(ID=c(1,2,2,3))
</code></pre>
"
323,Creating an ID variable based on 3 common variables in R,"<p>I have the following dataframe (below) which is a subset of my full dataset. I need to define the same ID for each entries with the same LAT/LONG and landed date. I previously ordered LANDEDDATE from the earliest to latest date. </p>

<pre><code> &gt; dput(df2)
structure(list(LATITUDE = c(43.35, 43.35, 43.35, 43.35, 43.35, 
43.35, 43.35, 43.35, 43.5166, 43.5166, 43.5166, 43.5166, 43.5166, 
43.5166, 43.5166, 42.9833, 42.9833, 42.9833, 42.9666, 42.9666
), LONGITUDE = c(-60.6163, -60.6165, -60.7167, -60.7166, -60.7163, 
-60.716, -60.7169, -60.7166, -59.9169, -59.9168, -59.9169, -59.9166, 
-59.9166, -59.916, -59.916, -61.8333, -61.8333, -61.8333, -61.9161, 
-61.9161), LANDEDDATE = structure(c(11171, 11171, 11183, 11183, 
11183, 11183, 11183, 11183, 11192, 11192, 11192, 11192, 11192, 
11192, 11192, 11210, 11210, 11210, 11210, 11210), class = ""Date""), 
    sppCODE = c(251L, 251L, 251L, 251L, 251L, 251L, 251L, 251L, 
    251L, 251L, 251L, 251L, 251L, 256L, 251L, 256L, 252L, 251L, 
    251L, 252L), LIVEW = c(0.337, 0.471, 0.238, 0.772, 0.178, 
    0.416, 0.535, 0.356, 0.442, 0.663, 0.442, 0.497, 0.276, 0.032, 
    0.828, 0.035, 0.011, 1.224, 1.025, 0.072), SPECIES = structure(c(7L, 
    7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 
    7L, 7L, 7L, 7L), .Label = c(""Albacore Tuna"", ""Bigeye Tuna"", 
    ""Bluefin Tuna"", ""Mako"", ""Porbeagle"", ""Shark, UNSP"", ""Swordfish"", 
    ""Tuna, UNSP"", ""White Marlin"", ""Yellowfin Tuna""), class = ""factor"")), .Names = c(""LATITUDE"", 
""LONGITUDE"", ""LANDEDDATE"", ""sppCODE"", ""LIVEW"", ""SPECIES""), row.names = c(19L, 
20L, 13L, 14L, 15L, 16L, 17L, 18L, 6L, 7L, 8L, 9L, 10L, 11L, 
12L, 1L, 2L, 3L, 4L, 5L), class = ""data.frame"")
</code></pre>

<p>This is what I've done so far but I can't figure out the rest. </p>

<pre><code>    df2$setID&lt;-""NA"" #I created an empty setID.
    &gt; head(df2)
   LATITUDE LONGITUDE LANDEDDATE sppCODE LIVEW   SPECIES setID
19    43.35  -60.6163 2000-08-02     251 0.337 Swordfish    NA
20    43.35  -60.6165 2000-08-02     251 0.471 Swordfish    NA
13    43.35  -60.7167 2000-08-14     251 0.238 Swordfish    NA
14    43.35  -60.7166 2000-08-14     251 0.772 Swordfish    NA
15    43.35  -60.7163 2000-08-14     251 0.178 Swordfish    NA
16    43.35  -60.7160 2000-08-14     251 0.416 Swordfish    NA
unique&lt;-df2[which(!duplicated(df2[,1:3])),] #This is each entry that are NOT duplicate
unique2$setID&lt;-1:13 # Ranked from 1:13
&gt; head(unique) #looks like that
   LATITUDE LONGITUDE LANDEDDATE sppCODE LIVEW   SPECIES setID
19    43.35  -60.6163 2000-08-02     251 0.337 Swordfish     1
20    43.35  -60.6165 2000-08-02     251 0.471 Swordfish     2
13    43.35  -60.7167 2000-08-14     251 0.238 Swordfish     3
14    43.35  -60.7166 2000-08-14     251 0.772 Swordfish     4
15    43.35  -60.7163 2000-08-14     251 0.178 Swordfish     5
16    43.35  -60.7160 2000-08-14     251 0.416 Swordfish     6
rep&lt;-df2[which(duplicated(df2[,1:3])),] #This is all my replicates
</code></pre>

<p>I need to allocate the setID of my unique dataframe to the corresponding sets (replicates with same LAT/LONG and LANDEDDATE) in the rep dataframe. Any advices would be appreciated!</p>
"
324,Automatic addition using R,"<p>I have this matrix:</p>

<pre><code>&gt; y
     [,1] [,2] [,3] [,4]
[1,]    1    8   36  180
[2,]    5   24   21   32
[3,]    9   40    6   48
[4,]   13   56   45  180
[5,]   17   72  117   28
</code></pre>

<p>and this vector:</p>

<pre><code>&gt; x
[1] 10 25 34 41 59
</code></pre>

<p>I should SUM the elements of the row and then subtract the result with the respective value on the x vector.</p>

<p>Example:</p>

<pre><code>10 - (1 + 8 + 36 + 180)
25 - (5 + 24 + 21 + 32)
34 - (9 + 40 + 6 + 48)
41 - (13 + 56 + 45 + 180)
59 - (17 + 72 + 117 + 28)
</code></pre>

<p>How can I create a vector with the results of those computations?</p>

<p>Thanks</p>
"
325,A question about statistics...,"<blockquote>
  <p>Let X1, X2, , Xn be a random sample from a Poisson() distribution. Let (X-bar) be their sample mean and $S2$ their sample variance. 
  a) Show that $\frac{\sqrt{n}[\bar{X}-\lambda]}{\sqrt{\bar{X}}}$ and $\frac{\sqrt{n}[\bar{X}-\lambda]}{S}$ both have a standard normal limiting distribution. 
  b) Find the limiting distribution of $\sqrt{n}[\bar{X}-\lambda]^2$<br>
  c) Find the limiting distribution of $\sqrt{n}[\bar{X}^2-\lambda^2]$</p>
</blockquote>

<p>a) For $\frac{\sqrt{n}[\bar{X}-\lambda]}{S}$, we know that $\frac{\sqrt{n}[\bar{X}-\lambda]}{S}$ = $(\frac{\sqrt{n}[\bar{X}-\lambda]}{\sigma})(\frac{\sigma}{S})$. Since 
$\frac{\sqrt{n}[\bar{X}-\lambda]}{\sigma}$ appraches N(0,1) in distribution by CLT, and since $(\frac{\sigma}{S})$ appraches 1 in probability, the whole thing approaches N(0,1).</p>

<p>For 
$\frac{\sqrt{n}[\bar{X}-\lambda]}{\sqrt{\bar{X}}}$, we get the same result...since the mean is equal to the variance in a poisson distribution.</p>

<p>b) I'm a little confused about this one...</p>

<p>c) From a theorem in my textbook, I know that if $\sqrt{n}(X_n - \theta) \rightarrow N(0,\sigma^2)$ and if there is a differentiable function g(x) at theta where the derivative at theta is not zero...then $\sqrt{n}(g(X_n)-g(\theta)) \rightarrow N(0,\sigma^2(g'(\theta))^2)$.</p>

<p>So I just need to use this theorem, right? And in this case g(x)=x^2. </p>

<p>Do you think my answer for a), and c) are correct? Also, can you give me a hint for b)?</p>

<p>Thanks in advance </p>
"
326,"Numpy vectorize, using lists as arguments","<p>The numpy <code>vectorize</code> function is useful, but it doesn't behave well when the function arguments are lists rather then scalars. As an example:</p>

<pre><code>import numpy as np

def f(x, A):
    print ""type(A)=%s, A=%s""%(type(A),A)
    return sum(A)/x

X = np.linspace(1,2,10)
P = [1,2,3]

f2 = np.vectorize(f)

f(X,P)
f2(X,P)
</code></pre>

<p>Gives:</p>

<pre><code>type(A)=&lt;type 'list'&gt;, A=[1, 2, 3]
type(A)=&lt;type 'numpy.int64'&gt;, A=1

Traceback (most recent call last):
  File ""vectorize.py"", line 14, in &lt;module&gt;
    f2(X,P)
  File ""/usr/local/lib/python2.6/dist-packages/numpy/lib/function_base.py"", line 1824, in __call__
    theout = self.thefunc(*newargs)
  File ""vectorize.py"", line 5, in f
    return sum(A)/x
TypeError: 'numpy.int64' object is not iterable
</code></pre>

<p>I understand that the function f works <em>just fine</em> without <code>vectorize</code>ing it, but I'd like to know how to (in general) vectorize a function whose arguments take in lists rather than a scalar.</p>
"
327,How do I flip a polygon by flipping the array?,"<p>I have 2 arrays of ints I am using to create a polygon (that looks like a fish). What do I need to do to the arrays to flip the polygon horizontally?</p>

<pre><code>x = new int[]
   { 0, 18, 24, 30, 48, 60, 60, 54, 60, 48, 30, 24, 0 };
y = new int[]
   { 0, 18, 6, 0, 0, 12, 18, 24, 24, 36, 36, 30, 36 };
</code></pre>
"
328,Python: Decimals with trigonometric functions,"<p>I'm having a little problem, take a look:</p>

<pre><code>&gt;&gt;&gt; import math
&gt;&gt;&gt; math.sin(math.pi)
1.2246467991473532e-16
</code></pre>

<p>This is not what I learnt in my Calculus class (It was 0, actually)</p>

<p>So, now, my question:</p>

<p>I need to perform some heavy trigonometric calculus with Python. What library can I use to get correct values?</p>

<p>Can I use Decimal?</p>

<p>EDIT:</p>

<p>Sorry, What I mean is other thing.</p>

<p>What I want is some way to do:</p>

<pre><code>&gt;&gt;&gt; awesome_lib.sin(180)
0
</code></pre>

<p>or this:</p>

<pre><code>&gt;&gt;&gt; awesome_lib.sin(Decimal(""180""))
0
</code></pre>

<p>I need a libraray that performs good trigonometric calculus. Everybody knows that sin 180 is 0, I need a library that can do that too.</p>
"
329,What does ...=... do in R?,"<p>What does <code>... = ...</code> mean as a function parameter?  I saw this in some R source code.  I understand that <code>...</code> is additional arguments, but not sure what the equals does?</p>
"
330,Values instead <NA> in cut () with quantile (),"<p>I have a dataframe: </p>

<pre><code>a &lt;- matrix(c(1,2,3,4), 2,2)
colnames(a) &lt;- c(""a"", ""b"")
df &lt;- as.data.frame(a)

&gt; df
  a b
1 1 3
2 2 4
</code></pre>

<p>First, I calculate quartilies of ""a"" column: </p>

<pre><code>&gt; quantile (df$a)
  0%  25%  50%  75% 100% 
1.00 1.25 1.50 1.75 2.00 
</code></pre>

<p>Then, I would like to categorize column ""b"" using quartiles from column ""a"": </p>

<pre><code>&gt; cat.b&lt;-cut(df$b, quantile (df$a,))
&gt; cat.b  
[1] &lt;NA&gt; &lt;NA&gt;  
Levels: (1,1.25] (1.25,1.5] (1.5,1.75] (1.75,2]
</code></pre>

<p>As you can see, R gives NA for both ""b"" values as they are above the highest quartile of ""a"". </p>

<p>However, I would like the resulting ""cat.b"" vector to look like: </p>

<pre><code>&gt; cat.b
[1]
""&gt;2""    ""&gt;2""  
</code></pre>

<p>Could you please tell me how to do it in R. </p>

<p>Thank you</p>
"
331,"Working out the ""best"" score based on quantity and ratio","<p>I'm not an expert with mathematics so I hope I'm posting in the right place!</p>

<p>I've got a lot of products which can be rated good, bad and OK by a user. What I'm having trouble with is finding which is rated the ""best""- similar to Amazon's sort by rating.</p>

<p>Getting the average rating won't work as I want a product with 99 good ratings and 1 bad rating to rank higher than a product with just 2 good ratings.
Any ideas on what type of formula to use?</p>

<p>-</p>

<p><em><strong>ANSWER, SORT OF:</em></strong></p>

<p>Thanks to Raskolnikov's comment I've found out what I need is either the Wilson Score Interval (which at the minute is way too complicated) or the Bayesian rating, which can be found here- <a href=""http://www.thebroth.com/blog/118/bayesian-rating"" rel=""nofollow"">http://www.thebroth.com/blog/118/bayesian-rating</a>.</p>

<p>Now I know at least what the formula I want is called, I can figure out how to get what I want.</p>
"
332,1 sample space with 2 probability example?,"<p>I am a freshman here with the following question, sorry for my gramar I don't know the exact matematical probability terms in english. </p>

<p>So, if I have a sample space, can I define in this space 2 probability? </p>

<p>I know that, the probability function fit 3 condition.</p>

<ol>
<li>$P(a) \ge 0$  </li>
<li>$P(\Omega) = 1$  </li>
<li>$P\left(\bigcup_{j\in J}A_j\right) =\sum_{j\in J}P(A_j)$</li>
</ol>

<p>The following probabilities function fit the conditions: probability as geometrie, probability as counting, probability as algebra. 
Example: probability as counting P(event) = of successful outcomes / of total outcomes</p>

<p>So can I define a 2 function on the same sample space? If not why, how to prove it? If anyone can give me examples, I would be very grateful.</p>

<p>I want to understand this with 100% percent.</p>

<p>Thank you very much.</p>
"
333,comparing row in numpy array,"<p>I have a 2d numpy array of bools, and I'd like to know how many unique rows my data set contains and the frequency of each row. The only way I could solve this problem is by converting my whole data set into a string and then do the comparison, but surely there must be a better way to do this. Any help is appreciated.</p>

<pre><code>def getUniqueHaplotypes(self,data):
nHap=data.shape[0]
unique=dict() 
for i in range(nHap):
    s = """".join([str(j) for j in data[i]])
    if unique.has_key(s):
        unique[s]+=1
    else:
        unique[s] = 1

return unique
</code></pre>
"
334,"Who uses R with multicore, SNOW or CUDA package for resource intense computing?","<p>who of you in this forum uses R (http://www.r-project.org/) with the multicore, SNOW or CUDA packages, so for advanced calculations that need more power than a workstation CPU? On which hardware do you compute these scripts? At home/ at work or do you have data center access somewhere?</p>

<p>The background of these questions is the following: I am currently writing my M.Sc. thesis about R and High-Performance-Computing and need a strong knowledge about who actually uses R. I read that R had 1 million users in 2008, bu thats more or less the only user statistics I could find on this topic - so I hope for your answers!</p>

<p>Sincerely Heinrich</p>
"
335,NaNs as key in dictionaries,"<p>Can anyone explain the following behaviour to me?</p>

<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; {np.nan: 5}[np.nan]
5
&gt;&gt;&gt; {float64(np.nan): 5}[float64(np.nan)]
KeyError: nan
</code></pre>

<p>Why does it work in the first case, but not in the second?
Additionally, I found that the following DOES work:</p>

<pre><code>&gt;&gt;&gt; a ={a: 5}[a]
float64(np.nan)
</code></pre>
"
336,R: Pass by reference,"<p>Can you pass by reference with ""R"" ?
for example, in the following code:</p>

<pre><code>setClass(""MyClass"",
    representation(
    name=""character""
    ))


instance1 &lt;-new(""MyClass"",name=""Hello1"")
instance2 &lt;-new(""MyClass"",name=""Hello2"")

array = c(instance1,instance2)

instance1
array

instance1@name=""World!""

instance1
array
</code></pre>

<p>the output is</p>

<pre><code>&gt; instance1
An object of class MyClass
Slot ""name"":
[1] ""World!""

&gt; array
[[1]]
An object of class MyClass
Slot ""name"":
[1] ""Hello1""


[[2]]
An object of class MyClass
Slot ""name"":
[1] ""Hello2""
</code></pre>

<p>but I wish it was</p>

<pre><code>&gt; instance1
An object of class MyClass
Slot ""name"":
[1] ""World!""

&gt; array
[[1]]
An object of class MyClass
Slot ""name"":
[1] ""World!""


[[2]]
An object of class MyClass
Slot ""name"":
[1] ""Hello2""
</code></pre>

<p>is it possible ?</p>

<p>Thanks</p>

<p>Pierre</p>
"
337,Algo for a stable 'download-time-remaining' in a download window,"<p>While displaying the download status in a window, I have information like:</p>

<p>1) Total file size (f)</p>

<p>2) Downloaded file size (f')</p>

<p>3) Current download speed (s)</p>

<p>A naive time-remaining calculation would be (f-f')/(s), but this value is way-to-shaky (6m remaining / 2h remaining / 5m remaining! deja vu?! :)</p>

<p>Would there be a calculation which is both stabler and not extremely wrong (showing 1h even when the download is about to complete)?</p>
"
338,Difficulty loading maxent package in R,"<p>I'm trying to load up the maxent package in R (<a href=""http://cran.r-project.org/web/packages/maxent/index.html"" rel=""nofollow"">http://cran.r-project.org/web/packages/maxent/index.html</a>) but it keeps returning the error:</p>

<pre><code>Error: package maxent is not installed for 'arch=x86_64'
</code></pre>

<p>The above is what happens when attempting just to load it from the CRAN repository.  Specifically, when trying to build from the source, it goes:</p>

<pre><code>&gt; install.packages('maxent_1.3.3.tar.gz', repos = NULL, type = 'source')
Warning in install.packages :
  package maxent_1.3.3.tar.gz is not available (for R version 2.15.2)
    Installing package(s) into     /Library/Frameworks/R.framework/Versions/2.15/Resources/library
(as lib is unspecified)
* installing *source* package maxent ...
** package maxent successfully unpacked and MD5 sums checked
** libs
*** arch - i386
sh: make: command not found
ERROR: compilation failed for package maxent
* removing /Library/Frameworks/R.framework/Versions/2.15/Resources/library/maxent
* restoring previous     /Library/Frameworks/R.framework/Versions/2.15/Resources/library/maxent
</code></pre>

<p>I've tried building the library from the source, as well as using the Mac OSX binary.  Below is the information regarding my version:</p>

<pre><code>platform       x86_64-apple-darwin9.8.0     
arch           x86_64                       
os             darwin9.8.0                  
system         x86_64, darwin9.8.0          
status                                      
major          2                            
minor          15.2                         
year           2012                         
month          10                           
day            26                           
svn rev        61015                        
language       R                            
version.string R version 2.15.2 (2012-10-26)
nickname       Trick or Treat
</code></pre>

<p>I'm doing all this in RStudio (Version 0.97.312).  Has anyone else experienced this problem?  Going into terminal to try to build the library hasn't worked, either.            </p>
"
339,First bracketed assignment is as time-consuming as full assignment?,"<p>Regarding this answer in:
<a href=""http://stackoverflow.com/questions/15759117/what-exactly-is-copy-on-modify-semantics-in-r-and-where-is-the-canonical-source/16370240#16370240"">What exactly is copy-on-modify semantics in R, and where is the canonical source?</a></p>

<p>We can see that, at the first time a vector is altered with <code>'[&lt;-'</code>, R copies the entire vector even if only a single entry is to be modifed. At the second time, however, the vector is altered ""in place"". This is noticeable without inspecting the address of the objects if we measure the time to create and modify a large vector:</p>

<pre><code>&gt; system.time(a &lt;- rep(1L, 10^8))
   user  system elapsed 
   0.15    0.17    0.31 
&gt; system.time(a[222L] &lt;- 111L)
   user  system elapsed 
   0.26    0.08    0.34 
&gt; system.time(a[333L] &lt;- 111L)
   user  system elapsed 
      0       0       0
</code></pre>

<p>Note that there is no change of type/storage.mode.</p>

<p>So the question is: why is it not possible to optimize the first bracket assignment as well? In what situation this kind of behaviour (full copy at first modification) is actually needed?</p>

<p>EDIT: (spoiler!) As explained in the accepted answer below, this is nothing but an artifact of enclosing the first assignment in a <code>system.time</code> function call. This causes R to mark the memory space bound to <code>a</code> as possibly referring to more than one symbol, thus requiring duplication when changed. If we remove the enclosing calls, the vector is modified in place from the very first bracket assignment.</p>

<p>Thanks Martin for in-depth solution!</p>
"
340,"javascript matrix exponential, ala cv::Rodrigues( )?","<p>Does anyone know of an existing code that does matrix exponential in javascript ?</p>

<p>Or, if not, what are ""best practices"" for writing something like that? I am used to C++ &amp; Matlab, just trying my hand at hacking some WebGL stuff in the last few days ...</p>
"
341,Understanding buffer/bitrate statistics in Media Player Classic,"<p>I understand that the two numbers under bitrate are the average bitrate and the current bitrate of each stream.</p>

<p>But what are the two numbers under buffers? I suppose the second one is the amount of data loaded in memory, but what is the first number? The amount of data decoded?</p>

<p>Also, why are there a jitter and a sync offset?</p>

<p><em>(For your reference, here stream 0-6 are video track 1, audio track 1-2 and subtitle track 1-4.)</em></p>

<p><img src=""http://i.stack.imgur.com/tJJzu.png"" alt=""alt text""></p>
"
342,How do you calculate cyclomatic complexity for R functions?,"<p><a href=""https://secure.wikimedia.org/wikipedia/en/wiki/Cyclomatic_complexity"" rel=""nofollow"">Cyclomatic complexity</a> measures how many possible branches can be taken through a function.  Is there an existing function/tool to calculate it for R functions?  If not, suggestions are appreciated for the best way to write one.</p>

<p>A cheap start towards this would be to count up all the occurences of <code>if</code>, <code>ifelse</code> or <code>switch</code> within your function.  To get a real answer though, you need to understand when branches start and end, which is much harder.  Maybe some R parsing tools would get us started?</p>
"
343,Why does C++ define the norm as the euclidean norm squared,"<p>This may sound like a bit of a rhetorical question, but I ask it here for two reasons:</p>

<ol>
<li>It took me a while to figure out what C++ std::norm() was doing differently from Matlab/Octave, so others may stumble upon it here.</li>
<li>I find it odd to define the <code>norm()</code> function as being something different (though closely related) to what is generally considered to be the norm (or L2-norm, or euclidean norm, etc. etc.)</li>
</ol>

<p>Specifically the C++ standard library defines <code>norm()</code> for complex numbers to be the square of the modulus (or absolute value), where the modulus is sqrt(a^2 + b^2) when the complex number is in the form a + i*b.</p>

<p>This goes against my understanding of the norm, which when specified as the euclidean norm (which corresponds to the modulus used here), is the square root of the sum of squares. I'll reference <a href=""http://mathworld.wolfram.com/ComplexModulus.html"" rel=""nofollow"">Mathworld's definition of the complex modulus</a>.</p>

<p>Is this something others have run into? I found it as a result of porting some signal processing code from Octave to C++, and the only other place I found reference to this difference was on the GCC mailing list (can't post the link due to 1-link limit).</p>
"
344,R Language: How to jump to next top level loop?,"<p>There is 2 levels of loop. 
How to jump to next time of loop of top level when something happen in the sub level of loop</p>

<p>Thanks a lot!</p>

<pre><code>uuu=0
for (i in 1:100)
{
    uuu=uuu+1
    j=1000
    for(eee in 1:30)
    {
        j=j-1
        if(j&lt;990)
        {
            if j is smaller than 990 I hope start next time of i
        }
    }
}
</code></pre>
"
345,How can I scale multiple windows with fixed aspect ratio so that they cover maximum of monitor without overlapping,"<p>My question is in my title. I am trying to develop a logic so that I can scale fixed aspect ratio windows to cover most of the screen without overlapping eachother. There could be two three or four windows. There could be an alignment parameter also.</p>
"
346,Sending a selection of code from texmate 2 to R.app makes TM 2 return to line 1,"<p>When I select something at in textmate 2 and push    the defualt for sending to R.app, then TM 2 returns to line 1 after execution. I looked in to the command it self, and found that apparently $TM_LINE_NUMBER is set to zero when when something is selected. </p>

<p>Is this a bug or a feature? Is there a working way to know what the last line of a selection is?</p>
"
347,Test if a list of genes has a significant GO/KEGG pathway with using globaltest,"<p>I was wondering: can I run the globaltest package to test if a list that only contain genes is significant for a KEGG of GO pathway? The only thing I have is a geneList like this:</p>

<pre><code>geneList &lt;- c(""Prdx2"", ""Gbp1"", ""Gm10654"", ""Xaf1"", ""Hs2st1"", ""Zfp930"", ""Fam32a"", 
              ""Slc5a5"", ""Gm7110"", ""Slc35e1"")
</code></pre>

<p>Thank you!</p>
"
348,"How to solve the system of nonlinear simultaneous equations (in Matlab, in Python, or in Fortran)","<p>I am looking at a system of nonlinear simultaneous equations. The two variables are u>0 and b>0. How can I solve this problem in Matlab, in Python, or in Fortran? Thanks.</p>

<p><img src=""http://i.stack.imgur.com/flOQP.jpg"" alt=""Nonlinear simultaneous equations""></p>
"
349,backtransfrom `scale()` for plotting,"<p>I have a explanatory variable that is centered using <code>scale()</code> that is used to predict a response variable:</p>

<pre><code>d &lt;- data.frame(
  x=runif(100),
  y=rnorm(100)
)

d &lt;- within(d, s.x &lt;- scale(x))

m1 &lt;- lm(y~s.x, data=d)
</code></pre>

<p>I'd like to plot the predicted values, but using the original scale of <code>x</code> rather than the centered scale. Is there a way to sort of backtransform or reverse scale <code>s.x</code>? </p>

<p>Thanks!</p>
"
350,How to save dictionaries and arrays in the same archive (with numpy.savez),"<p>first question here. I'll try to be concise.</p>

<p>I am generating multiple arrays containing feature information for a machine learning application. As the arrays do not have equal dimensions, I store them in a dictionary rather than in an array. There are two different kinds of features, so I am using two different dictionaries.</p>

<p>I also generate labels to go with the features. These labels are stored in arrays. Additionally, there are strings containing the exact parameters used for running the script and a timestamp. </p>

<p>All in all it looks like this:</p>

<pre><code>import numpy as np    

feature1 = {}
feature2 = {}
label1 = np.array([])
label2 = np.array([])
docString = 'Commands passed to the script were...'

# features look like this:
feature1 = {'case 1': np.array([1, 2, 3, ...]),
            'case 2': np.array([2, 1, 3, ...]),
            'case 3': np.array([2, 3, 1, ...]),
            and so on... }
</code></pre>

<p>Now my goal would be to do this:</p>

<pre><code>np.savez(outputFile, 
         saveFeature1 = feature1, 
         saveFeature2 = feature2, 
         saveLabel1 = label1, 
         saveLabel2 = label2,
         saveString = docString)
</code></pre>

<p>This seemingly works (i.e. such a file is saved with no error thrown and can be loaded again). However, when I try to load for example the feature from the file again:</p>

<pre><code>loadedArchive = np.load(outFile)
loadedFeature1 = loadedArchive['saveFeature1']
loadedString = loadedArchive['saveString']
</code></pre>

<p>Then instead of getting a dictionary back, I get a numpy array of shape (0) where I don't know how to access the contents:</p>

<pre><code>In []: loadedFeature1
Out[]: 
       array({'case 1': array([1, 2, 3, ...]), 
              'case 2': array([2, 3, 1, ...]), 
              ..., }, dtype=object)
</code></pre>

<p>Also strings become arrays and get a strange datatype:</p>

<pre><code>In []: loadedString.dtype
Out[]: dtype('|S20')
</code></pre>

<p>So in short, I am assuming this is not how it is done correctly. However I would prefer not to put all variables into one big dictionary because I will retrieve them in another process and would like to just loop over the dictionary.keys() without worrying about string comparison.</p>

<p>Any ideas are greatly appreciated.
Thanks</p>
"
351,Simplifying fractions in C#,"<p>I have made a console app that adds and subtracts fractions, I have added a function to simplify:</p>

<pre><code>    public static Numbers Add(Numbers n1, Numbers n2)
    {
        int den1;
        int num1;
        int num2;
        int dsimp;
        int nsimp;
        int numtop;
        num1 = n1.Numerator * n2.Denominator;
        num2 = n2.Numerator * n1.Denominator;
        den1 = n1.Denominator * n2.Denominator;
        numtop = num2 + num1;

        if (numtop == 0)
        {
            return new Numbers(0);
        }
        if (numtop % n1.Denominator == 0)
        {
            nsimp = numtop / n1.Denominator;
            dsimp = den1 / n1.Denominator;
            return new Numbers(nsimp, dsimp);

        }

        else
        {
            return new Numbers(numtop, den1);
        }
    }
</code></pre>

<p>When I put in 1/2 + 4/8 it simplifies it all perfectly the way I tell it to, but, it gives me 8/8. This needs to be simplified to 1/1. How do I get it to simplify what has already been simplified to the lowest possible fraction?</p>
"
352,What command converts knitr R Markdown into Stack-Exchange-friendly Markdown?,"<p><strong>Motivation</strong>:
I often want to paste the results of a quick analysis using R Markdown into a StackExchange site. This includes the <a href=""http://stackoverflow.com/questions/tagged/r"">R-tag on Stack Overflow</a>, <a href=""http://stats.stackexchange.com/"">Cross Validated</a>, or even a domain specific analysis on sites like <a href=""http://cogsci.stackexchange.com/"">Cognitive Sciences Stack Exchange</a> (e.g., this quick analysis of <a href=""http://cogsci.stackexchange.com/questions/922/what-is-the-correlation-between-objective-indexes-and-aggregated-self-report-mea/1067#1067"">OECD life index data</a>).</p>

<p><strong>Problems with default conversion:</strong>
The <strong>default</strong> markdown output of <code>knitr</code> is not suitable for inclusion on StackExchange.
The main problems I can see are that</p>

<ul>
<li>images are referenced to the local hard drive</li>
<li>code chunks are not tab or space indented; rather they use github style Markdown (i.e., no indentation)</li>
</ul>

<p>I.e., the chunks look like this:</p>

<pre><code>```r
some code
```
</code></pre>

<p>and output looks like this</p>

<pre><code>```
## some output
## ...
```
</code></pre>

<p>There might also be other specific issues to consider, such as </p>

<ul>
<li>ensuring tables are included properly</li>
<li>ensuring that equations are passed correctly for sites that support MathJax like Cross Validated and Cognitive Science Stack Exchange.</li>
</ul>

<h3>Question</h3>

<p><strong>What is a good command for converting R Markdown into Markdown (or HTML) suitable for simple inclusion into Stack Exchange sites?</strong></p>

<p>I think an ideal command would be a one-liner that takes an R Markdown file and generates a file where the entire content can be pasted directly into Stack Exchange to yield a well-formatted question or answer.</p>

<p>I share <a href=""https://github.com/downloads/jeromyanglim/assorted-files/non-sig-r.rmd"" rel=""nofollow"">this simple rmd file</a> with a couple of code chunks, a figure, and an equation as a test example.</p>

<p><strong>Initial thoughts</strong>:
Hosting of images on imgur would presumably sort out the issue with images. This can be done by including the following in the R Markdown file, but it would probably be simpler if this instruction was incorporated into some one-liner command.</p>

<pre><code>``` {r }
opts_knit$set(upload.fun = imgur_upload) 
````
</code></pre>

<p>It might be worth considering whether HTML or Markdown is the better format for pasting into  StackExchange. The <code>markdown</code> package provides a lot of flexibility.</p>
"
353,ndarray comparison,"<p>If I compare two ndarrays of type float64, where one of them is empty, I get an empty array of bool:</p>

<pre><code>x = np.array([1.0,2.1]) #dtype is float64
y = np.array([])        #dtype is float64
</code></pre>

<p><code>x==y</code> returns an empty ndarray with dtype of <code>bool</code>.</p>

<p>However, if I compare two ndarrays of type <code>int32</code>, where one of them is empty, I get False:</p>

<pre><code>a = np.array([1,2])
b = np.array([], dtype='int32')
</code></pre>

<p><code>a==b</code> returns False</p>

<p>What gives? Why are the returned types different? What I'm trying to do is compare two ndarrays of type <code>float64</code>.</p>

<p>This is being done on python 2.6.4, numpy 1.6.1, Windows XP</p>

<p>EDIT: ""trying to do is compare two ndarrays of type 'float5'"" -> ""trying to compare two ndarrays of type 'float64'"".</p>
"
354,Computing Conditional Probability (Poisson Random Variables),"<p>The number of red and blue cars that go through a given intersection in an hour is a Poisson-distributed random variable with $\lambda$ = 10.  What is the probability, conditionally, that at most three cars that go through the intersection are red given that ten blue cars entered in that hour?  </p>

<p>I should note, I understand how the Poisson distribution works, but I am not sure how to do it conditionally.  </p>
"
355,Python Numpy Dictionary Map Import?,"<p>So i want to try and use a numpy array to import a 2D map file to display graphical tiles on a grid. So say I have a window that takes 5x10 tiles. I'be been able to let the '1's represent a certain PNG tile in a dictionary, but how would i import a map file with other numbers and even symbols like $ or % or @ ect inside my dictionary instead of just all the 1s?</p>

<pre><code>&gt;&gt;&gt; numpy.ones((10,5))
array([[ 1.,  1.,  1.,  1.,  1.],
       [ 1.,  1.,  1.,  1.,  1.],
       [ 1.,  1.,  1.,  1.,  1.],
       [ 1.,  1.,  1.,  1.,  1.],
       [ 1.,  1.,  1.,  1.,  1.],
       [ 1.,  1.,  1.,  1.,  1.],
       [ 1.,  1.,  1.,  1.,  1.],
       [ 1.,  1.,  1.,  1.,  1.],
       [ 1.,  1.,  1.,  1.,  1.],
       [ 1.,  1.,  1.,  1.,  1.]])
&gt;&gt;&gt;
</code></pre>
"
356,Data binning: irregular polygons to regular mesh,"<p>I have thousands of polygons stored in a table format (given their 4 corner coordinates) which represent small regions of the earth. In addition, each polygon has a data value.
The file looks for example like this:</p>

<pre><code>lat1,  lat2,  lat3,  lat4,  lon1,   lon2,   lon3,   lon4,   data
57.27, 57.72, 57.68, 58.1,  151.58, 152.06, 150.27, 150.72, 13.45
56.96, 57.41, 57.36, 57.79, 151.24, 151.72, 149.95, 150.39, 56.24
57.33, 57.75, 57.69, 58.1,  150.06, 150.51, 148.82, 149.23, 24.52
56.65, 57.09, 57.05, 57.47, 150.91, 151.38, 149.63, 150.06, 38.24
57.01, 57.44, 57.38, 57.78, 149.74, 150.18, 148.5,  148.91, 84.25
...
</code></pre>

<p>Many of the polygons intersect or overlap. Now I would like to create a n*m matrix ranging from -90 to 90 latitude and -180 to 180 longitude in steps of, for instance, 0.25x0.25 to store the (area-weighted) mean data value of all polygons that fall within each pixel.</p>

<p>So, one pixel in the regular mesh shall get the mean value of one or more polygons (or none if no polygon overlaps with the pixel). Each polygon should contribute to this mean value depending on its area fraction within this pixel.</p>

<p>Basically the regular mesh and the polygons look like this:</p>

<p><img src=""http://i.stack.imgur.com/5gUq7.png"" alt=""enter image description here""></p>

<p>If you look at pixel 2, you see that two polygons are inside this pixel. Thus, I have to take the mean data value of both polygons considering their area fractions. The result should be then stored in the regular mesh pixel.</p>

<p>I looked around the web and found no satisfactory approach for this so far. Since I am using Python/Numpy for daily work I would like to stick to it. Is this possible? The package <a href=""http://pypi.python.org/pypi/Shapely"">shapely</a> looks promising but I don't know where to begin with...
Porting everything to a postgis database is an awful amount of effort and I guess there will be quite a few obstacles in my way.</p>
"
357,How to use correlogram to estimate variance?,"<p>From a book of computer simulation, I got this two equation.</p>

<p><img src=""http://i37.tinypic.com/x2ugxz.jpg"" alt=""alt text"" /></p>

<p>The first is to calculate <a href=""http://en.wikipedia.org/wiki/Correlogram"" rel=""nofollow"">correlogram</a>, the second is how to use correlogram to estimate variance.</p>

<p>The common approach to estimate variance of observation is often not incorrect in computer simulation because observations are often related.</p>

<p>My question is, the value I calculated from my program is very big, so it could not be correct.</p>

<p>I think because r[k] is going to get near 0 when k gets greater, the second equation will give a quite large value, so maybe the equation is incorrect?</p>

<p>As you asked, here is the whole program:</p>

<pre><code>@property
def autocorrelation(self):
    n = self.packet_sent
    mean = self.mean
    waiting_times = self.waiting_times
    R = [ sum([(x - mean) ** 2 for x in waiting_times[:-1]]) / n ]
    #print R

    for k in range(1, n / 4 + 1):
        R.append(0)
        for i in range(0, n - k):
            R[k] += (waiting_times[i] - mean) * (waiting_times[i + k] - mean)
        R[k] /=  n

    auto_cor = [r / R[0] for r in R]
    return auto_cor

@property
def standard_deviation_wrong(self):
    '''This must be a wrong method'''
    s_x = self.standard_deviation_simple
    auto_cor = self.autocorrelation
    s = 0
    n = self.packet_sent
    for k, r in enumerate(auto_cor[1:]):
        s += 1 - (k + 1.0) * r / n
        #print ""%f %f %f"" % (k, r, s)
    s *= 2
    s += 1
    s = ((s_x ** 2) * s) ** 0.5
    return s
</code></pre>
"
358,Optimum pairing logic,"<p>Here's a challenge that might best suit the mathematicians..</p>

<p>I have a list of N objects, each with two variables - value,flag.
 - value is an integer 0+.
 - flag is a boolean, 0 or 1.
 - e.g. 1,0  24,1  56,0  658,1 etc..</p>

<p>The objects must be paired wherever possible, according to the following rules:</p>

<ul>
<li>Each pair must have at least one object with flag = 1.</li>
<li>If only one object in the pair has flag = 1, it must be the one with the highest value.</li>
<li>The ""value"" of a pair is equal to the highest value of the two objects.</li>
<li>The total sum of ""values"" of all pairs, plus the sum of values of unpaired objects, must be minimal (small as it possibly can be for any arrangement).</li>
</ul>

<p>Now, there is a direct application to this problem which is probably obvious, but I think I've stated it in the most general terms possible. </p>

<p><strong>The challenge:</strong>  Demonstrate an algorithm which assigns pairs to reach the lowest possible total sum.</p>

<p>Contributions are appreciated in either a programming language or your choice, mathematical notation, or simply a plain English / pseudo code approach.</p>

<p>Thanks!</p>

<p><strong>EDIT</strong> - the naive algorithm that I initially suggested is as simple as:</p>

<ul>
<li>Choose the highest priced, flagged, unmatched product.</li>
<li>Choose the next highest priced (or same price) unmatched, product.</li>
<li>Match those two products.</li>
<li>Repeat until 0 flagged or &lt;2 total products remain.</li>
</ul>

<p>Which seemed fine, until it was noted that this fails in certain situations.
e.g. A:200,1   B:150, 1   C:100,0    D:100,0
where the best solution is to pair A with C, B with D for a total of 350.</p>

<p>(If you haven't guessed the application yet, it's buy-one-get-one-free!)</p>
"
359,"C: How to wrap a float to the interval [-pi, pi)","<p>I'm looking for some nice C code that will accomplish effectively:</p>

<pre><code>while (deltaPhase &gt;= M_PI) deltaPhase -= M_TWOPI;
while (deltaPhase &lt; -M_PI) deltaPhase += M_TWOPI;
</code></pre>

<p>What are my options?</p>
"
360,R2WinBUGS - logistic regression with simulated data,"<p>I am just wondering whether anyone has some R code that uses the package R2WinBUGS to run logistic regression - ideally with simulated data to generate the 'truth' and two continous co-variates.</p>

<p>Thanks.</p>

<p>Christian</p>

<p>PS:</p>

<p>Potential code to generate artificial data (one dimensional case) and run winbugs via r2winbugs (it does not work yet).</p>

<pre><code>library(MASS)
library(R2WinBUGS)

setwd(""d:/BayesianLogisticRegression"")

n.site &lt;- 150

X1&lt;- sort(runif(n = n.site, min = -1, max =1))

xb &lt;- 0.0 + 3.0*X1 

occ.prob &lt;- 1/(1+exp(-xb))

plot(X1, occ.prob,xlab=""X1"",ylab=""occ.prob"")

true.presence &lt;- rbinom(n = n.site, size = 1, prob = occ.prob)

plot(X1, true.presence,xlab=""X1"",ylab=""true.presence"")

# combine data as data frame and save
data &lt;- data.frame(X1, true.presence)
write.matrix(data, file = ""data.txt"", sep = ""\t"")

sink(""model.txt"")
cat(""
model {

# Priors
 alpha ~ dnorm(0,0.01)
 beta ~ dnorm(0,0.01)

# Likelihood
 for (i in 1:n) {
    C[i] ~ dbin(p[i], N)        # Note p before N
    logit(p[i]) &lt;- alpha + beta *X1[i]
 }
}
"",fill=TRUE)
sink()

# Bundle data
win.data &lt;- list(mass = X1, n = length(X1))

# Inits function
inits &lt;- function(){ list(alpha=rlnorm(1), beta=rlnorm(1))}

# Parameters to estimate
params &lt;- c(""alpha"", ""beta"")

# MCMC settings
nc &lt;- 3 #Number of Chains
ni &lt;- 1200 #Number of draws from posterior
nb &lt;- 200 #Number of draws to discard as burn-in
nt &lt;- 2 Thinning rate

# Start Gibbs sampling
out &lt;- bugs(data=win.data, inits=inits, parameters.to.save=params, 
model.file=""model.txt"", n.thin=nt, n.chains=nc, n.burnin=nb, 
n.iter=ni, debug = TRUE)
</code></pre>
"
361,Matching an array to a row in Numpy,"<p>I have an array 'A' of shape(50,3) and another array 'B' of shape (1,3). </p>

<p>Actually this B is a row in A. So I need to find its row location.</p>

<p>I used <code>np.where(A==B)</code>, but it gives the locations searched <strong>element wise</strong>. For example, below is the result i got :</p>

<pre><code>&gt;&gt;&gt; np.where(A == B)
(array([ 3,  3,  3, 30, 37, 44]), array([0, 1, 2, 1, 2, 0]))
</code></pre>

<p>Actually B is the 4th row in A (in my case). But above result gives (3,0)(3,1)(3,2) and others, which are matched element-wise. </p>

<p>Instead of this, i need an answer '3' which is the answer obtained when B searched in A as a whole and it also removes others like (30,1)(37,2)... which are partial matches.</p>

<p>How can i do this in Numpy?</p>

<p>Thank you.</p>
"
362,how to use numpy.gradient working for left and right differences?,"<p>how to use numpy.gradient working for left and right. It works by default for central difference.</p>

<p>Thanks a lot.</p>
"
363,Using a matrix to rotate rectangles individually,"<p>Having a bit of a drawing complication you would call it. My math is a bit rusty when it comes to Matrices and drawing rotations on shapes. Here is a bit of code:</p>

<pre><code>private void Form1_Paint(object sender, PaintEventArgs e)
    {
        g = e.Graphics;
        g.SmoothingMode = SmoothingMode.HighQuality;
        DoRotation(e);
        g.DrawRectangle(new Pen(Color.Black), r1);
        g.DrawRectangle(new Pen(Color.Black), r2);

        // draw a line (PEN, CenterOfObject(X, Y), endpoint(X,Y) )
        g.DrawLine(new Pen(Color.Black), new Point((r1.X + 50), (r1.Y + 75)), new Point((/*r1.X + */50), (/*r1.Y - */25)));

        this.lblPoint.Text = ""X-pos: "" + r1.X + "" Y-pos: "" + r1.Y;

        //this.Invalidate();
    }
    public void DoRotation(PaintEventArgs e)
    {
        // move the rotation point to the center of object
        e.Graphics.TranslateTransform((r1.X + 50), (r1.Y + 75));
        //rotate
        e.Graphics.RotateTransform((float)rotAngle);
        //move back to the top left corner of the object
        e.Graphics.TranslateTransform(-(r1.X + 50), -(r1.Y + 75));
    }
    public void Form1_KeyDown(object sender, KeyEventArgs e)
    {
        case Keys.T:
                rotAngle += 1.0f;
    }
</code></pre>

<p>when I rotate (what I think should be r1) both r1 and r2 rotate.  I need to be able to rotate each shape individually as I add more shapes.</p>
"
364,Table like dotplot in R,"<p>I want to create a table like dotplot with ggplot but haven't found the right way to do it. I am using code like the following to plot p-values for different groups across a number of covariates:</p>

<pre><code>library(ggplot2) 

y &lt;- rep(c(""a"",""b"",""c"",""d""),2) 
p &lt;-  abs(rnorm(n=8)) 
group &lt;- rep(c(""1"",""2""),4) 
n &lt;-c(10, 20, 30, 10, 15, 12, 18, 22)
df &lt;- data.frame(p, y, group, n) 

ggplot(data = df, mapping = aes(x = p, y= y 
    ))+ geom_point(mapping=aes(y=y, x=p), stat=""identity"")
</code></pre>

<p>I want the covariates to be on the y-axis and the p-values on the x-axis.</p>

<p>The specific problems I find are two:</p>

<ol>
<li><p>First, there are different subsets (""groups"" in this data) for each combination of y and x, as reflected in different dots located on the same rows. Therefore, I would like the p-values to be plotted on separate rows. While faceting would offer a solution, it is not what I am looking for. I want the different groups to be identified within the same facet/panel (I am also avoiding colors because I am using that for something else).</p></li>
<li><p>Second, I would like a column showing the number of observations (""n"" in this data) on the left-hand side. The end result would have three columns on the left of the graph: one with the y labels spanning over different groups, another with the groups, and the last with the number of observations.</p></li>
</ol>
"
365,how to save and fetch statistics in mysql,"<p>I'd like to save statistics for clicks and showings and some another parameters per content. 
I can't use Google Analytics for this purposes, because show reports should be very specific. </p>

<p>Firstly, I need to show count of clicks, impressions, anything else per hour, date, etc.
Number of tracking records more than 1 billion and request to show count of clicks takes > 1 second. </p>

<p>Can you please advise any article or book where I can learn such type of task.</p>

<p>Thanks for advance!</p>
"
366,Recalculate column only for highest date in each category,"<p>I want to recalculate a column in my <code>data.table</code> only for certain rows, depending on a <code>Condition</code>, the category (<code>Cat</code>) and the <code>Date</code>.</p>

<p>A row may qualify to be recalculated only if <code>Condition==TRUE</code>. Among all rows with <code>Condition==TRUE</code>, only the rows with the highest <code>Date</code> for the respective <code>Cat</code> should be selected.</p>

<p>A simplified example:</p>

<pre><code>     DF = data.frame(Cat=rep(c(""A"",""B"",""C""),each=3), Date=rep(c(""01-08-2013"",""01-07-2013"",""01-04-2013""),3),
            Condition=c(TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE),
            Data1=c(1:9), Data2=rep(c(1:3),3), Result=c(1:1))
     DF$Date = as.Date(DF$Date , ""%m-%d-%Y"")
     DT = data.table(DF)
     DT

        Cat       Date Condition Data1 Data2 Result
     1:   A 2013-01-08      TRUE     1     1      1
     2:   A 2013-01-07      TRUE     2     2      1
     3:   A 2013-01-04     FALSE     3     3      1
     4:   B 2013-01-08     FALSE     4     1      1
     5:   B 2013-01-07     FALSE     5     2      1
     6:   B 2013-01-04     FALSE     6     3      1
     7:   C 2013-01-08     FALSE     7     1      1
     8:   C 2013-01-07     FALSE     8     2      1
     9:   C 2013-01-04      TRUE     9     3      1
</code></pre>

<p>I found out how to extract the <code>Cat</code>'s and <code>Date</code>'s of the rows, for which the <code>Result</code> must be recalculated:</p>

<pre><code>    setkey(DT, Condition, Cat, Date)
    DT[J(TRUE), max(Date), by=Cat]

       Cat         V1
    1:   A 2013-01-08
    2:   C 2013-01-04
</code></pre>

<p>However, I don't know how to calculate a new <code>Result</code> for these rows.
In this simplified example, the new <code>Result</code> should be <code>Data1+Data2</code>.</p>

<p><strong>Edit:</strong><br>
Inspired by eddi's answer, I came up with two more possible solutions:</p>

<p>Approach using <code>.I</code>:</p>

<pre><code>    DT[DT[Condition==TRUE , .I[which.max(Date)], by=Cat][[2]], Result:=Data1+Data2]
</code></pre>

<p>Approach using <code>.SD</code> (see eddi's note of caution):</p>

<pre><code>    max_dates=DT[Condition==TRUE , .SD[which.max(Date)], by=Cat]
    setkey(DT, Cat, Date)
    DT[max_dates, Result:=Data1 + Data2]
</code></pre>

<p>Are there any recommendations which solution to choose with regard to speed / efficiency?</p>
"
367,"What discrete distribution is completely determined by its mode and variance, is easy to sample, and has nice border properties?","<p>I need to generate random ordered unranked trees that will be used to test some computer program. I'd like to incorporate some kind of control into the generation process, so that the generated trees have (in expectation) some desired properies, like: long chains (node with 1 child, its child also has 1 child, ...); or: there are many children on level 3, but not so many on level 1; or: if a node has few children, its children are more likely to have more children, but if it has many children, its children are more likely to have less; etc.</p>

<p>I've chosen this approach: define a tree as a prefix closed set of strings over some alphabet $\Sigma$ such that if $Sc$ is in the set ($S \in \Sigma^{*}$, $c \in \Sigma$), then for all $b \le c, b \in \Sigma$ $Sb$ is in the set. I fix $\Sigma$, and to generate a tree, I generate a few strings over this alphabet, treat them as nodes, and then generate all other nodes from them (by prefix closing the set, etc.)</p>

<p>The node string generation process is done symbol-by-symbol, with symbol on level $l$ generated as follows:</p>

<ol>
<li>Sample Bernoulli distribution with parameter $1\over d$ to see if we should stop. (That way, expected length of the generated strings is $d$.)</li>
<li>Sample symbol from the distribution $p(s | l, t) = p(s|l)p(s|t)$ where $s\in\Sigma$ is the symbol being generated, $t\in\Sigma$ is the previous generated symbol. Here, $p(s|l)$ and $p(s|t)$ are parameters of the generation process.</li>
</ol>

<p>I'd like the user of this generation process to specify discrete distribution $p(s|l)$ over symbols from $\Sigma$ for each $l$ using just two parameters: the expected symbol, and variance. (Same for $p(s|t)$ for each $t$.) The motivation is this: say, our symbols are $1,2,...,9$, then symbol $s$ at level $l$ determines the number of nodes at level $l$ under the already-generated node $S$. If we define $p(s|l)$ to be sharply distributed around $3$, in the generated trees nodes at level $l-1$ will in most cases have 3 children, and so on.</p>

<p>Now, to the question. I need a family of functions that will be my $p(s|l)$ and $p(s|t)$, with the following properties:</p>

<ol>
<li>The function is completely determined by 2 parameters, the first being its mode and the second being variance or something else that has the meaning of how sharp the values are distributed around the mean.</li>
<li>The distribution is easy to sample from. I don't want to depend on some library for that, nor write lots of complicated code. (The overall task is of low priority, but of high curiosity for me.)</li>
<li>The distribution behaves ""as expected"" if the mode is close to the beginning or end of the alphabet. Specifically, assume again that our symbols are $1,2,...,9$; if the mode is $1$, the function should be strictly decreasing, and if the mode is $5$, it should be symmetric.</li>
</ol>

<p>Beta-binomial looked similar to what I wanted, but I don't know if there is an easy way to sample from it, and also I'm not that familiar with it to come up with a good second parameter. I'd like to know if there is a simple, well-known distribution with such properties.</p>
"
368,Box plots showing relationship in R,"<p>I am using a regression model for some data whose explanatory variables can only be <code>1,2,3,4 and 5</code>. There are 5 explanatory variables and a dependent variable. For example,</p>

<pre><code>set.seed(2)
x1 &lt;- sample(rep(1:5,2))
x2 &lt;- sample(rep(1:5,2))
x3 &lt;- sample(rep(1:5,2))
x4 &lt;- sample(rep(1:5,2))
x5 &lt;- sample(rep(1:5,2))
y &lt;- runif(10,-1,1)

model &lt;- lm(y~x1 + x2 + x3 + x4 + x5)
</code></pre>

<p>I want to create a box plot showing the relationship between these dependent variables  and the dependent variable. How can I do that in R?</p>

<p>I have managed to create a box plot using the code provided by @Ben. However, there are some points in the plot that I do not understand. Any idea what they are for? Here is the plot<img src=""http://i.stack.imgur.com/F3ONi.png"" alt=""enter image description here""></p>
"
369,Birth-death process invariant distribution,"<p>Let $X_n$ be a birth-death process, with birth rates $\lambda_n$ and death rates $\mu_n$ (with $\mu_o=0$ and $\lambda_{-1}=0$). How do you show that the invariant distribution $\pi_i$ is:</p>

<p>$\pi_0=\Big[ 1+ \sum_{k=0}^\infty \frac{\lambda_k\lambda_{k-1}\dots\lambda_0}{\mu_{k+1}\mu_k\dots \mu_1}\Big]^{-1}$</p>

<p>and</p>

<p>$\pi_{n+1}=\frac{\lambda_n}{\mu_{n+1}}\pi_n$?</p>

<p>I used the definition of invariant distribution, and arrived at the formula</p>

<p>$\pi_i=\frac{\pi_{i-1}\lambda_{i-1}+\pi_{i+1}\mu_{i+1}}{\lambda_i+\mu_i}$, </p>

<p>but I have no idea how to use this to prove what I'm being asked to prove.</p>
"
370,Basic probability... probability that two users share a key out of a keypool of 20?,"<p>Let's say I am distributing a bunch of encryption keys, where if two users have the same key, they can communicate.  I have 20 different keys, and I am giving each user 2.  Since encryption keys don't disappear when I hand them out, it can be considered 'with replacement', but I'm not giving the same user the same key twice.  </p>

<p>I want to know what the probability that two users can communicate under these circumstances.  Doing a bit of the math from what I know, each user has 20 * 19 = 380 permutations of keys.  The order that they get the keys in doesn't matter, though, so it's actually 190 different combinations of keys.  So, two users have 190 possible combinations of four keys, and what is the probability that one of their two keys is in common.  </p>

<p>That reasoning is about as far as I get.  How does one move forward from there?  Thanks!</p>
"
371,How do I generate normal cumulative distribution in Java? its inverse cdf?  How about lognormal?,"<p>I am brand new to Java, second day! I want generate samples with normal distribution.  I am using inverse transformation.</p>

<p>Basically, I want to find the inverse normal cumulative distribution, then find its inverse.  And generate samples.</p>

<p>My questions is: Is there a built-in function for inverse normal cdf?  Or do I have to hand code?</p>

<p>I have seen people refer to <a href=""http://commons.apache.org/math/api-1.2/org/apache/commons/math/distribution/NormalDistributionImpl.html"" rel=""nofollow"">this</a> on apache commons.  Is this a built-in?  Or do I have to download it?</p>

<p>If I have to do it myself, can you give me some tips?  If I download, doesn't my prof also have to have the ""package"" or special file installed?</p>

<p>Thanks in advance!</p>

<p>Edit:Just found I can't use libraries, also heard there is simpler way converting normal using radian.</p>
"
372,"R: How does ""gsub"" handle spaces?","<p>I have a character string <code>""ab b cde""</code>, i.e. <code>""ab[space]b[space]cde""</code>. I want to replace ""space-b"" and ""space-c"" with blank spaces, so that the output string is <code>""ab[space][space][space][space]de""</code>. I can't figure out how to get rid of the second ""b"" without deleting the first one. I have tried:</p>

<pre><code>gsub(""[\\sb,\\sc]"", "" "", ""ab b cde"", perl=T)
</code></pre>

<p>but this is giving me <code>""a[spaces]de""</code>. Any pointers?  Thanks.</p>

<p>Edit: Consider a more complicated problem: I want to convert the string <code>""akui i ii""</code> i.e. <code>""akui[space]i[space]ii""</code> to <code>""akui[spaces|""</code> by removing the <code>""space-i""</code> and <code>""space-ii""</code>.</p>
"
373,How do I extract the angle of rotation from a QTransform?,"<p>I have a QTransform object and would like to know the angle in degrees that the object is rotated by, however there is no clear example of how to do this:</p>

<p><a href=""http://doc.trolltech.com/4.4/qtransform.html#basic-matrix-operations"" rel=""nofollow"">http://doc.trolltech.com/4.4/qtransform.html#basic-matrix-operations</a></p>

<p>Setting it is easy, getting it back out again is hard.</p>
"
374,R: How can I replace let's say the 5th element within a string?,"<p>I would like to convert the a string like be33szfuhm100060 into BESZFUHM0060.</p>

<p>In order to replace the small letters with capital letters I've so far used the gsub function. </p>

<pre><code>test1=gsub(""be"",""BE"",test)
</code></pre>

<p>Is there a way to tell this function to replace the 3rd and 4th string element? If not, I would really appreciate if you could tell me another way to solve this problem. Maybe there is also a more general solution to change a string element at a certain position into a capital letter whatever the element is?</p>
"
375,Adding Pronumerals together in javascript,"<p>I want javascript to be able to interpret the following (<code>a</code> and <code>b</code> are always going to be different, so these are just an example)  </p>

<pre><code>a=(3x)+y  
b=x+(4y)  
</code></pre>

<p>and return the following</p>

<pre><code>a+b=(4x)+(5y)  
</code></pre>

<p>all variables are strings and not integers so math can not be applied to <code>a</code>,<code>b</code>,<code>x</code> or <code>y</code>  </p>

<p>I have not started on this particular instance, due to the fact that i don't know where to start.  </p>

<p>P.S. I have not had any experience with jQuery, so if possible, try and avoid it  </p>

<p>EDIT: The program is designed to help find raw materials in the game minecraft. For example if you want a diamond sword (<code>a</code>) and a diamond pickaxe (<code>b</code>), <code>a</code> requires 1 wood (<code>x</code>) and 2 diamonds (<code>y</code>), and <code>b</code> requires 1 wood (<code>x</code>) and 3 diamonds (<code>y</code>). Once i run it through this program, i would like a response saying that it requires 2 wood and 5 diamonds. Sorry for any prior confusion...</p>
"
376,X axis in Barplot in R,"<p>I want to ask a question about barplot's axes:</p>

<p>first  please see my data.</p>

<pre><code>SerNo   DOY Rain
1   350 0
2   351 0
3   352 0
4   353 0
5   354 0
6   355 0
7   356 0
8   357 0
9   358 0
10  359 0
11  360 0
12  361 0
13  362 0
14  363 0
15  364 0.7
16  365 2.7
17  1   0
18  2   0
19  3   0
20  4   2
21  5   0
22  6   0
23  7   0
24  8   0
25  9   0
26  10  0
27  11  0
28  12  0
29  13  0
30  14  0
31  15  0
32  16  0
33  17  1.8
34  18  0.8
35  19  10
36  20  0
37  21  0
38  22  0
39  23  0
40  24  0
41  25  0
42  26  0
43  27  0
44  28  0
45  29  0
46  30  6.5
47  31  0
48  32  0
49  33  0
50  34  0
51  35  0
52  36  5.8
53  37  0
54  38  0
55  39  0
56  40  0
57  41  0
58  42  0
59  43  0
60  44  0
61  45  0
62  46  2.9
63  47  0
64  48  0
</code></pre>

<p>DOY means the day of year , 1st,January  is 1,and 31st December is 365/366, Rain is the total precipitation in that day ,because this time period across the year boundary, and I want to draw a plot which x is the DOY and Y axis is the rain, when using the barplot, I can't match the DOY with the corresponding col of rain
here is my code</p>

<pre><code>rainbar&lt;-read.table(""I:/example.txt"",header=T)
rainbar
barplot(rainbar$Rain,axes=F,ylim=c(0,15))

length(rainbar$SerNo)
seq(1,length(rainbar$SerNo),1)
axis(2,seq(0,15,3),c(0,3,6,9,12,15))
axis(1,seq(1,length(rainbar$SerNo),1),rainbar$DOY)
</code></pre>

<p>the result likes this
why can't the two data fit together? even I added a column called SerNo, and the SerNo based X axis still can't match the corresponding rain day, what is the reason? how does the barplot function define its own X axis?
thank you very much</p>

<p><img src=""http://i.stack.imgur.com/kBP58.png"" alt=""enter image description here""></p>
"
377,R caret and gbm can't find ntrees input,"<p>I'm trying to train a <code>gbm</code> using the <code>caret</code> package in R.  I initially got the following error and thought it was due to lack of an input, so I created the <code>gbmGrid</code> but am still getting the same error message.</p>

<pre><code>sub4Collect1 &lt;- data.frame(testing$row_id)
&gt; 
&gt; cl &lt;- makeCluster(10, type = ""SOCK"")
&gt; registerDoSNOW(cl)
&gt; ptm &lt;- proc.time()
&gt; 
&gt; for(i in 2:7){
+ trainClass &lt;- postPrior1[,i]
+ testClass &lt;- postTest1[,i]
+ gbmGrid &lt;- expand.grid(.interaction.depth = (1:5) * 2, .n.trees = (1:5)*50, .shrinkage = .1)
+ bootControl &lt;- trainControl(number = 1)
+ set.seed(2)
+ gbmFit &lt;- train(prePrior1[,-c(2,60,61,161)], trainClass, method = ""gbm"", tuneLength = 5,
+ trControl = bootControl
+ ##, scaled = FALSE
+ , tuneGrid = gbmGrid 
+ )
+ pred1 &lt;- predict(gbmFit$finalModel, newdata = preTest1[,-c(2,60,61,161)])
+ sub4Collect1 &lt;- cbind(sub4Collect1, pred1)
+ print(i)
+ flush.console()
+ }
Iter   TrainDeviance   ValidDeviance   StepSize   Improve
     1        0.0000            -nan     0.1000    0.0000
     2        0.0000            -nan     0.1000    0.0000
     3        0.0000            -nan     0.1000    0.0000
     4        0.0000            -nan     0.1000    0.0000
     5        0.0000            -nan     0.1000    0.0000
     6        0.0000            -nan     0.1000    0.0000
     7        0.0000            -nan     0.1000    0.0000
     8        0.0000            -nan     0.1000    0.0000
     9        0.0000            -nan     0.1000    0.0000
    10        0.0000            -nan     0.1000    0.0000
    50        0.0000            -nan     0.1000    0.0000

Error in n.trees[n.trees &gt; object$n.trees] &lt;- object$n.trees : 
  argument ""n.trees"" is missing, with no default
&gt; stopCluster(cl)
&gt; timee4 &lt;- proc.time() - ptm
&gt; timee4 
   user  system elapsed 
  3.563   0.306  14.472 
</code></pre>

<p>Any suggestions?</p>
"
378,Why is it that when I have X(long) / (Y*Y*Y)(long) I get Error: Divide by Zero?,"<p>In my example X is already long and Y is a long also. I am not casting at then.</p>

<p>I really just want to divide by a number that is cubed. (using native libraries)</p>

<p>These numbers are extremely large. If I convert them to floats and do it, its value is Infinite...</p>

<pre><code>System.out.println(formatter.format(""%20d"", (X/(Y*Y*Y))));
</code></pre>

<p>Y is an extremely large number, it is not 0. X is a measurement of time in milliseconds.</p>

<p>I will post the exact code in a short while if this question doesn't get closed... I don't have access to it right this minute.</p>

<p><strong>Context:</strong> I am dealing with a big notation calculation for O(n^3).</p>

<p><strong>Error:</strong> ""Exception in thread ""main"" java.lang.ArithmeticException: / by zero""</p>

<p><strong>Answers:</strong> </p>

<blockquote>
  <p>Assuming you didn't really mean the quotes, the likely reason is that
  Y * Y * Y is greater than 2 ^ 31. It's overflowing, with a lower part
  of 0. I believe this would only happen if Y is a multiple of 2^11
  (2048) - but I'm not certain*</p>
</blockquote>

<p><em><strong>-This is the case for me, Y is a multiple of 2048, hopefully this helps with trying to find a solution.</em></strong></p>

<pre><code>    // Algorithm 3
    for( int n = 524288; n &lt;= 5000000; n *= 2 ){
        int alg = 3;
        long timing;
        maxSum = maxSubSum3( a );
        timing = getTimingInfo( n, alg );
        System.out.println(fmt.format(""%20s %20d %20d %20d %20d %20s%n"", ""Alg. 3"", n, timing, timing, timing/(n*n), ""time/(n*log(n))""));
    }
</code></pre>
"
379,Please help me on choosing right classifer,"<p>I am facing a problem on selecting correct classifier for my data-mining task.</p>

<p>I am labeling webpages using statistical method and label them using a 1-4 scale,1 being the poorest while 4 being the best.</p>

<p>Previously,I used SVM to train the system since I was using a binary(1,0) label then.But now since I switch to this 4-class label,I need to change classifier,because I think the SVM classifier will only work for two-class classification(Please correct me if I am wrong).</p>

<p>So could you please offer some suggestion here on what kind of classifier is most approriate here for my classification purpose.</p>

<p>Thanks in advance for suggestions.</p>
"
380,Rules-of-thumb doc for mathematical programming in R?,"<p>Does there exist a simple, cheatsheet-like document which compiles the best practices for mathematical computing in R? Does anyone have a short list of their best-practices? E.g., it would include items like:</p>

<ol>
<li>For large numerical vectors <code>x</code>, instead of computing <code>x^2</code>, one should compute <code>x*x</code>. This speeds up calculations.</li>
<li>To solve a system $Ax = b$, never solve $A^{-1}$ and left-multiply $b$. Lower order algorithms exist (e.g., Gaussian elimination)</li>
</ol>

<p>I did find a nice numerical analysis cheatsheet <a href=""http://www.tfinley.net/notes/cs421-cheat-sheet4.pdf"" rel=""nofollow"">here</a>. But I'm looking for something quicker, dirtier, and more specific to R. </p>
"
381,summarising multiple non-exclusive dummy variables in R into one variable,"<p>I was sent a dataset with multiple dummy variables and other variables as well. Basically what Id like to do is create summary table with summary.formula from rms. However, I do not know how to create a single variable from the multiple dummy variables and they are not mutually exclusive. Is this at all possible. Of course I could do it creating a table etc, but then I cannot use summary.formula and Id like the summary.formula output to include just the individual levels of the dummy variables.</p>

<p>edit:
to clarify: a &amp; b need to be summarized, but they are not mutually exclusive. Since age is recorded for every row I need to summarize a &amp; b into one variable for it to be used in summary.formula. Ive edited the code below so that 0 and 1 are changed into NA or a,b respectively.</p>

<p>Id like the summary.formula output to be something like this:</p>

<pre><code>h&lt;-data.frame(a=sample(c(""A"",NA),100,replace=T),b=sample(c(""B"",NA),100,replace=T),age=rnorm(100,50,25),epo=sample(c(""Y"",""N""),100,T))





library(rms)

summary.formula(epo~age####+summary variable of a &amp; b######,method=""reverse"",data=h)



#-----------------
 Descriptive Statistics by epo

+---------+--------------------------+--------------------------+
|         |N                         |Y                         |
|         |(N=56)                    |(N=44)                    |
+---------+--------------------------+--------------------------+
|age      |31.53434/48.90788/67.69096|28.63689/43.93502/57.81834|
+---------+--------------------------+--------------------------+
|sab : A  |         25% (14)         |         16% ( 7)         |
+---------+--------------------------+--------------------------+
|   B     |         27% (15)         |         32% (14)         |
+---------+--------------------------+--------------------------+
</code></pre>
"
382,Generate a set of random numbers with an average evenly distributed between two given values,"<p>1) I generate 1000 random numbers between 0 and 10 and take the average.</p>

<p>If I do the above action ""many"" times the resulting average values will be a normal distribution over 0 to 10. Correct?</p>

<p>What I want after ""many"" iterations of generating 1000 random numbers (+ some manipulation) is to produce average values between 3 and 7, distributed evenly between 3 and 7.</p>

<p>What's my approach here?</p>
"
383,max likelihood fminsearch,"<p>I used Matlab-fminsearch for a negativ max likelihood  model for a binomial distributed function. I don't get any error notice, but the parameter which I want to estimate, take always the start value. Apparently, there is a mistake. I know that I ask a totally general question. But is it possible that anybody had the same mistake and know how to deal with it?
Thanks a lot,</p>

<p>@woodchips, thank you a lot. Step by step, I've tried to do what you advised me. First of all, I actually maximized (-log(likelihood)) and this is not the problem. I think I found out the problem but I still have some questions, if I don't bother you. I have a model(param) to maximize in paramstart=p1. This model is built for (-log(likelihood(F))) and my F is a  vectorized function like F(t,Z,X,T,param,m2,m3,k,l). I have a data like (tdata,kdata,ldata),X,T are grids and Z is a function on this grid and (m1,m2,m3) are given parameters.When I want to see the value of F(tdata,Z,X,T,m1,m2,m3,kdata,ldata), I get a good output. But I think fminsearch accept that F(tdata,Z,X,T,p,m2,m3,kdata,ldata) like a constant and thatswhy I always have as estimated parameter the start value. I will be happy, if you have any advise to tweak that.</p>
"
384,"How to find BUE of N? where $X_i$ iid $ P(X_i=j)=1/N$, for $j=1, \cdots, N$","<p>Let $X_1, \cdots, X_n$ be iid with $P(X_i=j)=1/N$, for $j=1, \cdots, N$, where $N$ is an unknown positive integer.</p>

<p>I would guess that the order statistics $X_{(n)}$ is the complete sufficient statistic of $N$, but I'm having trouble in proving this.</p>

<p>Also, I understand that best unbiased estimator of $N$ could be in the form of $2E(X_1|X_{(n)})-1$, but I'm having trouble to go further.</p>

<p>Any help would be appreciated!</p>
"
385,"Midpoint sums Problem 22 - I dont get the right answer because I dont undestand what means ""xxxxx.x"" I think","<p>Hello
 I am trying to solve this exercise <a href=""http://www.cstutoringcenter.com/problems/problems.php?id=22"" rel=""nofollow"">Problem 22</a> just for reinforcing my solving skills. I've already coded the answer. The task asks for ""what is the sum of ALL the resulting y coordinates values? (Enter the number as a decimal in the form xxxxx.x (<strong>I dont understand what this means</strong>)) . My answers is 50616.0, but it is wrong.
 I hope you can help me. I don't know if I am doing wrong, or just I dont understand what the task means in the form xxxxx.x, I thinks is decimal no?</p>

<p>this is my code:</p>

<pre><code>import java.io.*;
import java.math.BigDecimal;

public class Problema22 {

public static void main(String args[]) {
    File archivo = null;
    FileReader fr = null;
    BufferedReader br = null;


    try {
        archivo = new File(""C:\\plane22.txt"");
        fr = new FileReader(archivo);
        br = new BufferedReader(fr);
        String linea;
        int index = 0;
        int num = 0;
        String num2 = """";
        BigDecimal sol = BigDecimal.valueOf(0);

        while ((linea = br.readLine()) != null) //System.out.println(linea);
        {
            //System.out.println(linea);
            int line1 = linea.indexOf("","");
            int line2 = linea.lastIndexOf("","");
            int line3 = linea.indexOf(""|"");


            String y1 = linea.substring(line1+1, line3);
            String y2 = linea.substring(line2+1, linea.length() );

            long sumys = (Integer.parseInt(y1)+ Integer.parseInt(y2))/2;

            sol = BigDecimal.valueOf(sumys).add(sol);
            //System.out.println(sol);





        }

        System.out.println(sol);

        //count((ArrayList&lt;String&gt;) arr);

    } catch (Exception e) {
        e.printStackTrace();

    }
}
</code></pre>
"
386,Generate a new numpy array of order 2 filling in each element with a random number in a certain range,"<p>I'm new to numpy. What's the best way to create a new array and fill each element with a random number within a certain range? </p>

<p>For example I want a <em>3-by-3</em> array where each element is either a <code>0</code> or a <code>1</code>.</p>
"
387,Logic Function Mod5 Problem,"<p>I am looking for a solution:</p>

<p>A= {0,1,2,3,4};</p>

<p>F(x) = 3x - 1 (mod5)</p>

<p>Could you help me to find the inverse. I am struggling with this as it seems to be not to be onto or 1to1. </p>

<p>Thank you for your help.</p>
"
388,residuals input R,"<p>I was given raw data on 1960 and 1970 ratio of women number of previous birth who will have another birth. I had to do a 2-way analysis table then a square combining table to obtain the residuals. I did not know how to compute in R, so it was done manually (by row medians and then column medians). This was my residuals for the different groups:</p>

<pre><code>P0      P1      P2      P3      P4      P5
0.186   0.122   -0.014  -0.059  -0.017  0.013
-0.004  -0.004  -0.004  0.004   0.042   0.006
-0.174  -0.125  -0.013  0.018   0.048   0.012
0.205   0.176   0.043   -0.043  -0.052  -0.053
0.004   0.004   0.004   -0.004  -0.013  -0.069
-0.040  -0.011  0.009   0.005   0.013   -0.006
</code></pre>

<p>how do I enter in R, to create a stem and leaf and residual plot. These values are the residuals for the data for 1960 row1-3 and 1970 row 1-3.</p>
"
389,iostat usage and output,"<p>When I run the <code>iostat</code> command like this:</p>

<pre><code>iostat -d
</code></pre>

<p>I get a result something like this:</p>

<pre><code>Linux 2.6.18-238.el5 (mon01)       09/03/2011

Device:            tps   Blk_read/s   Blk_wrtn/s   Blk_read   Blk_wrtn
sda               9.83        11.88       264.67   38378414  854835288
sda1              9.83        11.88       264.67   38375658  854834320
sda2              0.00         0.00         0.00       2346        968
sdb              58.40         7.96      1552.02   25716671 5012803778
sdb1             58.40         7.96      1552.02   25715861 5012803778
</code></pre>

<p>If I run it again several times waiting two to three few minutes between execution I get exactly the same set of values for <code>tps</code>, <code>Blk_read/s</code> and <code>Blk_wrtn/s</code>. The <code>man</code> page for <code>iostat</code> says:</p>

<blockquote>
  <p>The first  report  generated by the iostat command provides statistics
  concerning the time since the system was booted. Each subsequent
  report covers the time since the previous report.</p>
</blockquote>

<p>If this is the case why are my <code>tps</code>, <code>Blk_read/s</code> and <code>Blk_wrtn/s</code> values not varying in the slightest?</p>

<p>Does <em>""Each subsequent report""</em> mean between the individual reports if running:</p>

<pre><code>iostat -d 1 5
</code></pre>

<p>...rather than the last time I ran <code>iostat -d</code>?</p>

<p>I am slightly confused here.</p>
"
390,How do you print to stderr in R?,"<p>How do you print to <code>stderr</code> in <code>R</code>?</p>

<p>This would especially useful for scripts written in <code>Rscript</code>.</p>
"
391,How to find nth prime with complexity o(1),"<p>How to find nth prime number with complexity o(1)</p>
"
392,reading csv files in a for loop and assigning dataframe names,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stackoverflow.com/questions/5319839/read-multiple-csv-files-into-separate-data-frames"">Read multiple CSV files into separate data frames</a>  </p>
</blockquote>



<p>I need to read many csv files into dataframes from one folder. The csv file names are of the form <strong><em>fxpair-yyyy-mm.csv</em></strong> (e.g. AUDJPY-2009-05.csv). I want to read all csv files in and create dataframes of the form <strong><em>fxpair.yyyy.mm</em></strong></p>

<p>I am having trouble creating the dataframe names in the loop for assignment from the read.csv statements</p>

<pre><code>filenames &lt;- list.files(path=getwd())  
numfiles &lt;- length(filenames)  

#fx.data.frames to hold names that will be assigned to csv files in csv.read
fx.data.frames &lt;- gsub(pattern=""-"",x=filenames,replacement=""."")  
fx.data.frames &lt;- gsub(pattern="".csv"",x=fx.data.frames,replacement="""")

i &lt;-1  
for (i in c(1:numfiles)){  
   filenames[i] &lt;- paste("".\\"",filenames[i],sep="""")  
   fx.data.frames[i] &lt;- read.csv(filenames[i], header=FALSE)
}
</code></pre>

<p>The csv.read seems to work fine but I am not able to create the dataframe objects in the way I intend. I just want some way to name the dataframes read in the <strong><em>fxpair.yyyy.mm</em></strong> format based on the file name.</p>

<p>Am I missing something obvius? THANK YOU FOR ANY HELP!!</p>
"
393,Strange abline behaviour,"<p>I am currently trying to graph a time series of a price spread and then add an abline with a regression. Currently this is just a AR(1) because I wanted to get the plot right before starting.</p>

<p>The data is from a .XLS
and is organized as such in OpenOffice </p>

<pre><code>Date - Price1 - Price 2 - NA.(Empty) 
01.01.1982 - 1.56 - 2.53 -  
[...]
</code></pre>

<p>I am reading this as</p>

<pre><code>library(xlsx)
library(AER)
x&lt;-read.xlsx(""data.xlsx"",1)
</code></pre>

<p>then I fill the empty column like so</p>

<pre><code>x$NA.=(x$Price1-x$Price2)
</code></pre>

<p>So I now have a table in memory that produces this head()</p>

<pre><code>       Date Price1 Price2 NA.
1 1987-08-28  18.30  19.44 1.24
2 1987-08-31  18.65  19.75 1.12
</code></pre>

<p>(there is an index column before Date, I don't really need it since I plot by date but its there)</p>

<p>I then do</p>

<pre><code>plot(x$Date,x$NA.)
</code></pre>

<p>I get the correct plot. I have modified that plot command a bit to get correct grids, axis and dates and lines etc, but the problem I have also persists even with the simple plot version above, so the problem is not with my edits.</p>

<p>The problem is the following:</p>

<p>If I now try to plot a line</p>

<pre><code>abline(a=1,b=1,col=""blue"")
</code></pre>

<p>It doesn't work. The command goes through, but it does not show a line. However:</p>

<pre><code>abline(a=1,b=0,col=""blue"")
</code></pre>

<p>works as intended and shows a blue, horizontal line.</p>

<p>The problem I have is that I want to feed a regression object into the plot, for example like so</p>

<pre><code>SPRC=zoo(x$NA.,x$Date)
SPRC2=lag(SPRC, -1)
SPRC=SPRC[2:length(SPRC)]
LMO&lt;-lm(SPRC ~ SPRC2)
abline(a=LMO$coefficients[2],b=LMO$coefficients[1],col=""red"")
</code></pre>

<p>What I am trying to do is a simple AR to test things out. The regression works as intended, but the abline does not produce an output.</p>

<p>I also tried to do abline without variables - it works only if it b=0. I also tried instead to do a </p>

<pre><code>plot(SPRC)
</code></pre>

<p>and then any kind of abline, but either none shows up or it becomes a vertical line (!). Only if b=0 it becomes a horizontal line.</p>

<p>I imagine this has to do with the data object or input, but I am really lost on why it does not work. I also tried an as.Date on the Date object, but that does not change anything. All other plot commands seem to work, like adding custom grids, par, locator texts, axis etc. The problem occurs if I start a clean R session and pretty much only enter the code above.
I also tried to switch around the regression variables, the a and b values, or the order of the plot variables. It still does not work</p>

<p>Can you imagine what might be the issue?</p>

<p>Edit:
I just checked the datatypes if typeof().
typeof x is ""list"", everything else is ""double"", even if I do a <code>x$Date&lt;-as.Date(x$Date,""%d.%m.%Y"")</code></p>

<p>Edit2:
I went ahead and saved the file as csv and read it with read.csv
then I did</p>

<pre><code>plot(x$Price1)
</code></pre>

<p>and</p>

<pre><code>abline(a=40,b=1)
</code></pre>

<p>All it does is produce a vertical(!) line that is slightly turned clockwise.
Is my R broken?
<img src=""http://i.stack.imgur.com/OBh1j.png"" alt=""enter image description here""></p>

<p>(I realize the scale is off for the price - the spread is around 0. But even with an a=40, the line is identical)</p>
"
394,Filter an array in Python3 / Numpy and return indices,"<p>Is there any built-in function in Python3/Numpy which filters an array and returns indices of the elements which are left? Something similar to numpy.argsort for sorting. The filter I have is setting both min and max thresholds - all values below/above min/max have to be filtered out.</p>

<p>I've seen Python's function <a href=""http://docs.python.org/py3k/library/functions.html#filter"" rel=""nofollow"">filter</a>, but I don't see a way to extract indices using it.</p>

<p><strong>EDITED:</strong> Lots of usefull information in the answers, thank you!</p>

<p>As @SvenMarnach pointed out, mask is enough:</p>

<pre><code>mask = (min_value &lt; a) &amp; (a &lt; max_value)
</code></pre>

<p>Now I have to apply this mask to other arrays of the same shape as <code>a</code>, but not sure what is the best way to do it...</p>
"
395,"R, Right xpath expression when using XML and xpathSApply","<p>Let's say I parsed an website using below expression</p>

<pre><code>library(XML)
url.df_1 = htmlTreeParse(""http://www.appannie.com/app/android/com.king.candycrushsaga/"", useInternalNodes = T)
</code></pre>

<p>if I run below code,</p>

<pre><code>xpathSApply(url.df_1, ""//div[@class='app_content_section']/h3"", function(x) c(xmlValue(x), xmlAttrs(x)[[""href""]]))
</code></pre>

<p>I will get below - </p>

<pre><code>[1] ""Description""                      ""What's new""                      
[3] ""Permissions""                      ""More Apps by King.com All Apps  ""
[5] ""Customers Also Viewed""            ""Customers Also Installed""       
</code></pre>

<p>Now, what I'm interested in is only ""Customers Also Installed""  part. But when I run the below code,</p>

<pre><code>xpathSApply(url.df_1, ""//div[@class='app_content_section']/ul/li/a"", function(x) c(xmlValue(x), xmlAttrs(x)[[""href""]]))
</code></pre>

<p>it spits out the all the apps included in ""More Apps by King.com All Apps"" , ""Customers Also Viewed"" and ""Customers Also Installed"".  </p>

<p>So I tried,</p>

<pre><code>xpathSApply(url.df_1, ""//div[h3='Customers Also Installed'], function(x) c(xmlValue(x), xmlAttrs(x)[[""href""]]))
</code></pre>

<p>but this didn't work. So I tried</p>

<pre><code>xpathSApply(url.df_1, ""//div[contains(.,'Customers Also Installed')]"",xmlValue)
</code></pre>

<p>but this doesn't work either. (The output should be something like below-)</p>

<pre><code> [,1]                                                
[1,] ""Christmas Candy Free\n    Daniel Development\n    ""
[2,] ""/app/android/xmas.candy.free/""                     
 [,2]                                           
[1,] ""Jewel Candy Maker\n    Nutty Apps\n    ""      
[2,] ""/app/android/com.candy.maker.jewel.nuttyapps/""
 [,3]                                      
[1,] ""Pogz 2\n    Terry Paton\n    ""           
[2,] ""/app/android/com.terrypaton.unity.pogz2/""
</code></pre>

<p>Any guidance will be much appreciated! </p>
"
396,How to plot timelines with R or Gnuplot,"<p>What is the best and easy way to plot, with R or Gnuplot, timelines like in this picture:</p>

<p><img src=""http://i.stack.imgur.com/ojETX.gif"" alt=""enter image description here""></p>
"
397,C# Math.Round Error?,"<p>Ok, I realize it's early on a Sunday so I hope I'm just missing something obvious:</p>

<p>I have this function:</p>

<pre><code>private decimal CashConversion(decimal amount, decimal ratio)
{
    if (ratio == 1) return amount;

    decimal convertedAmount = amount / ratio;
    return Math.Round(convertedAmount, 2);
}
</code></pre>

<p>When I call it like this:</p>

<pre><code>decimal tax = CashConversion(96.53, 15);
</code></pre>

<p>The ""tax"" variable is equal to 6.43. However, 96.53/15 is 6.435333333333333. Rounding that to 2 places should return 6.44. Am I missing something here?</p>
"
398,Integral of a Gaussian distribution over the unit ball,"<p>Given a multivariate normal $X \sim N(\vec{0},\Sigma)$, I would like to calculate the pdf when sampling from the unit ball $(||X||_2=1)$. Specifically what is the value of the normalizing factor $Z$ ie. the integral of the gaussian over the ball.</p>
"
399,start to next without looping,"<p>I have two vectors. One is a series of starts or a sequence of id's the other is a guide to the next location from each start.  </p>

<p>The key position for my analysis is the first position. I add the corresponding number from the next vector to the position to know were to move next.</p>

<pre><code>starts &lt;- c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20)
nexts &lt;- c(4,2,1,1,3,5,1,1,3,2,10,2,3,4,1,1,1,1,4,6)
</code></pre>

<p>What I expect to have is starting at ""starts"" 1, next tells me to move 4 places, so I end up at starts 1+4, then from starts 5 next tells me to move 3 places, now I am at starts 8, the corresponding next at position 8 is 1, I move to 9, from 9 the next move is 3...</p>

<p>The final goal would be a vector like this one goal &lt;- c(5,8,9,12....). The first vector ""starts"" might not be needed really if we can make the ""jumps"" using nexts by just reading the size of the next move at each ""landing"". like this: from 1 we jump 4, to arrive at 5, from location 5 we jump 3 to position 8 from 8 we jump 1 position to 9....</p>
"
400,Fastest way to cross-tabulate two massive logical vectors in R,"<p>For two logical vectors, <code>x</code> and <code>y</code>, of length > 1E8, what is the fastest way to calculate the 2x2 cross tabulations?</p>

<p>I suspect the answer is to write it in C/C++, but I wonder if there is something in R that is already quite smart about this problem, as it's not uncommon.</p>

<p>Example code, for 300M entries (feel free to let N = 1E8 if 3E8 is too big; I chose a total size just under 2.5GB (2.4GB).  I targeted a density of 0.02, just to make it more interesting (one could use a sparse vector, if that helps, but type conversion can take time).</p>

<pre><code>set.seed(0)
N = 3E8
p = 0.02
x = sample(c(TRUE, FALSE), N, prob = c(p, 1-p), replace = TRUE)
y = sample(c(TRUE, FALSE), N, prob = c(p, 1-p), replace = TRUE)
</code></pre>

<p>Some obvious methods:</p>

<ol>
<li><code>table</code></li>
<li><code>bigtabulate</code></li>
<li>Simple logical operations (e.g. <code>sum(x &amp; y)</code>)</li>
<li>Vector multiplication (boo)</li>
<li><code>data.table</code></li>
<li>Some of the above, with <code>parallel</code> from the <code>multicore</code> package (or the new <code>parallel</code> package)</li>
</ol>

<p>I've taken a stab at the first three options (see my answer), but I feel that there must be something better and faster.</p>

<p>I find that <code>table</code> works very slowly.  <code>bigtabulate</code> seems like overkill for a pair of logical vectors.  Finally, doing the vanilla logical operations seems like a kludge, and it looks at each vector too many times (3X? 7X?), not to mention that it fills a lot of additional memory during processing, which is a massive time waster.</p>

<p>Vector multiplication is usually a bad idea, but when the vector is sparse, one may get an advantage out of storing it as such, and then using vector multiplication.</p>

<p>Feel free to vary <code>N</code> and <code>p</code>, if that will demonstrate anything interesting behavior of the tabulation functions.  :)</p>

<hr>

<p>Update 1. My first answer gives timings on three naive methods, which is the basis for believing <code>table</code> is slow.  Yet, the key thing to realize is that the ""logical"" method is grossly inefficient.  Look at what it's doing:</p>

<ul>
<li>4 logical vector operations</li>
<li>4 type conversions (logical to integer or FP - for <code>sum</code>)</li>
<li>4 vector summations</li>
<li>8 assignments (1 for the logical operation, 1 for the summation)</li>
</ul>

<p>Not only that, but it's not even compiled or parallelized.   Yet, it still beats the pants off of <code>table</code>.  Notice that <code>bigtabulate</code>, with <em>an extra type conversion</em> (<code>1 * cbind...</code>) still beats <code>table</code>.</p>

<p>Update 2.  Lest anyone point out that logical vectors in R support <code>NA</code>, and that that will be a wrench in the system for these cross tabulations (which is true in most cases), I should point out that my vectors come from <code>is.na()</code> or <code>is.finite()</code>.  :)  I've been debugging <code>NA</code> and other non-finite values - <a href=""http://stackoverflow.com/questions/9167376/how-to-force-an-error-if-non-finite-values-na-nan-or-inf-are-encountered"">they've been a headache for me recently</a>.  If you don't know whether or not all of your entries are <code>NA</code>, you could test with <code>any(is.na(yourVector))</code> - this would be wise before you adopt some of the ideas arising in this Q&amp;A.</p>

<hr>

<p>Update 3. Brandon Bertelsen asked a very reasonable question in the comments: why use so much data when a sub-sample (the initial set, after all, is a sample ;-)) might be adequate for the purposes of creating a cross-tabulation?  Not to drift too far into statistics, but the data arises from cases where the <code>TRUE</code> observations are very rare, for both variables.  One is a result of a data anomaly, the other due to a possible bug in code (possible bug because we only see the computational result - think of variable <code>x</code> as ""Garbage In"", and <code>y</code> as ""Garbage Out"".  AS a result, the question is whether the issues in the output caused by the code are solely those cases where the data is anomalous, or are there some other instances where good data goes bad?  (This is why I asked a question about <a href=""http://stackoverflow.com/questions/9167376/how-to-force-an-error-if-non-finite-values-na-nan-or-inf-are-encountered"">stopping when a <code>NaN</code>, <code>NA</code>, or <code>Inf</code> is encountered</a>.)</p>

<p>That also explains why my example has a low probability for <code>TRUE</code> values; these really occur much less than 0.1% of the time.</p>

<p>Does this suggest a different solution path?  Yes: it suggests that we may use two indices (i.e. the locations of <code>TRUE</code> in each set) and count set intersections.  I avoided set intersections because I was burned awhile back by Matlab (yes, this is R, but bear with me), which would first sort elements of a set before it does an intersection.  (I vaguely recall the complexity was even more embarrassing: like <code>O(n^2)</code> instead of <code>O(n log n)</code>.)</p>
"
401,Error in using variables in SQL statements,"<p>Im extracting some data from a table using sql select statment in R, </p>

<pre><code>query &lt;- ""select * from MyTable where TimeCol='6/29/2012 21:05' "";
result &lt;- fn$sqldf(query);
</code></pre>

<p>The above code gives correct results, but when the time value is saved in variable, it doesn't works </p>

<pre><code>mytime &lt;- ""6/29/2012 21:05"";

query &lt;- ""select * from MyTable where TimeCol = $mytime"";     # OR

query &lt;- ""select * from MyTable where TimeCol = $[mytime]"";   # OR

query &lt;- ""select * from MyTable where TimeCol = '$[mytime]' "";

result &lt;- fn$sqldf(query);
</code></pre>

<p>None of the above three lines is working</p>

<p><code>View(result)</code>   it gives the error:  invalid 'x' argument</p>
"
402,list of character vectors of unequal length to data.frame,"<p>I have a named list that looks like this:</p>

<pre><code>&gt; head(pathways)
$&lt;NA&gt;
NULL
$`2`
[1] ""hsa04610""
$`9`
[1] ""hsa00232"" ""hsa00983"" ""hsa01100""
$`10`
[1] ""hsa00232"" ""hsa00983"" ""hsa01100""
$&lt;NA&gt;
NULL
$&lt;NA&gt;
NULL
</code></pre>

<p>To describe it more formerly. The name of each list is an id number, and the entries of each element of the character vector that are elements of the list is another id number. I can filter out the <code>$&lt;NA&gt;</code> entries easily with <code>is.na()</code>, but then I want to change the rest so it would look like:</p>

<pre><code>id   another_id
2    hsa04610   
9    hsa00232   
9    hsa00983   
9    hsa01100   
10   hsa00232  
10   hsa00983
10   hsa01100


&gt; dput(test)
structure(list(`NA` = NULL, `2` = ""hsa04610"", `9` = c(""hsa00232"", 
""hsa00983"", ""hsa01100""), `10` = c(""hsa00232"", ""hsa00983"", ""hsa01100""
), `NA` = NULL, `NA` = NULL), .Names = c(NA, ""2"", ""9"", ""10"", 
NA, NA))
</code></pre>

<p>Any ideas?</p>
"
403,Get the column number in R given the column name,"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stackoverflow.com/questions/4427234/get-column-index-from-label-in-a-data-frame"">Get column index from label in a data frame</a>  </p>
</blockquote>



<p>I need to get the column number of a column given its name.</p>

<p>Supose we have the following dataframe:</p>

<pre><code>df &lt;- data.frame(a=rnorm(100),b=rnorm(100),c=rnorm(100))
</code></pre>

<p>I need a function that would work like the following:</p>

<pre><code>getColumnNumber(df,""b"")
</code></pre>

<p>And it would return </p>

<pre><code>[1] 2
</code></pre>

<p>Is there a function like that?</p>

<p>Thanks!</p>
"
404,Errors in segmented package: breakpoints confusion,"<p>Using the segmented package to create a piecewise linear regression I am seeing an error when I try to set my own breakpoints; it seems only when I try to set more than two.</p>

<p>(EDIT) Here is the code I am using:</p>

<pre><code># data
bullard &lt;- structure(list(Rt = c(0, 4.0054, 25.1858, 27.9998, 35.7259, 39.0769, 
45.1805, 45.6717, 48.3419, 51.5661, 64.1578, 66.828, 111.1613, 
114.2518, 121.8681, 146.0591, 148.8134, 164.6219, 176.522, 177.9578, 
180.8773, 187.1846, 210.5131, 211.483, 230.2598, 262.3549, 266.2318, 
303.3181, 329.4067, 335.0262, 337.8323, 343.1142, 352.2322, 367.8386, 
380.09, 388.5412, 390.4162, 395.6409), Tem = c(15.248, 15.4523, 
16.0761, 16.2013, 16.5914, 16.8777, 17.3545, 17.3877, 17.5307, 
17.7079, 18.4177, 18.575, 19.8261, 19.9731, 20.4074, 21.2622, 
21.4117, 22.1776, 23.4835, 23.6738, 23.9973, 24.4976, 25.7585, 
26.0231, 28.5495, 30.8602, 31.3067, 37.3183, 39.2858, 39.4731, 
39.6756, 39.9271, 40.6634, 42.3641, 43.9158, 44.1891, 44.3563, 
44.5837)), .Names = c(""Rt"", ""Tem""), class = ""data.frame"", row.names = c(NA, 
-38L))

library(segmented)

# create a linear model
out.lm &lt;- lm(Tem ~ Rt, data=bullard)

o&lt;-segmented(out.lm, seg.Z=~Rt, psi=list(Rt=c(200,300)), control=seg.control(display=FALSE))
</code></pre>

<p>Using the <code>psi</code> option, I have tried the following:</p>

<pre><code>psi = list(x = c(150, 300)) -- OK
psi = list(x = c(100, 200)) -- OK
psi = list(x = c(200, 300)) -- OK
psi = list(x = c(100, 300)) -- OK
psi = list(x = c(120, 150, 300)) -- error 1 below
psi = list(x = c(120, 300)) -- OK
psi = list(x = c(120, 150)) -- OK
psi = list(x = c(150, 300)) -- OK
psi = list(x = c(100, 200, 300)) -- error 2 below
</code></pre>

<p>(1) <code>Error in segmented.lm(out.lm, seg.Z = ~Rt, psi = list(Rt = c(120, 150,  : 
  only 1 datum in an interval: breakpoint(s) at the boundary or too close</code></p>

<p>(2) <code>Error in diag(Cov[id, id]) : subscript out of bounds</code></p>

<p>I have already listed my data <a href=""http://stackoverflow.com/q/12105435/857416"">at this question</a>, but as a guide the limits on the x data are about 0--400. </p>

<p>A second question that pertains to this one is: how do I actually fix the breakpoints using this segmented package? </p>
"
405,How to add table of contents to R Markdown HTML file using pandoc?,"<p><strong>How to add table of contents to R Markdown HTML file using pandoc but retain all the HTML formatting and header information?</strong></p>

<p>E.g., If I had a file called <code>test.html</code>, I tried:</p>

<pre><code>pandoc -s -S --toc test.html -o test-toc.html
</code></pre>

<p>This adds the table of contents but it removes the existing header information which makes all the formatting attractive.</p>

<p>Thus, it makes <a href=""http://cloud.github.com/downloads/jeromyanglim/assorted-files/rpubs-test.html"">this html file</a> look like <a href=""http://cloud.github.com/downloads/jeromyanglim/assorted-files/rpubs-test-toc.html"">this one</a>. I'd like to preserve the formatting.</p>
"
406,"Hebrew ""URL Encoding"" in R?","<p>I've been referred to both:</p>

<pre><code>?URLencode
#and
?curlEscape
</code></pre>

<p>I see that both functions work great for English, but fail to provide with the proper translation for Hebrew characters.</p>

<p>For example, the word</p>

<blockquote>
  <p></p>
</blockquote>

<p>(Peace, in Hebrew)
Should be this:</p>

<blockquote>
  <p>%D7%A9%D7%9C%D7%95%D7%9D</p>
</blockquote>

<p>But instead, both commands translate it to:</p>

<pre><code>URLencode("""")
%f9%ec%e5%ed
</code></pre>

<p>What do you suggest?  (write it myself, or is there something pre-made)</p>

<p>Thanks, 
Tal</p>

<p>Update: My sessionInfo:</p>

<pre><code>&gt; sessionInfo()
R version 2.12.0 (2010-10-15)
Platform: i386-pc-mingw32/i386 (32-bit)

locale:
[1] LC_COLLATE=Hebrew_Israel.1255  LC_CTYPE=Hebrew_Israel.1255   
[3] LC_MONETARY=Hebrew_Israel.1255 LC_NUMERIC=C                  
[5] LC_TIME=Hebrew_Israel.1255    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     
</code></pre>
"
407,make panels with same margins when combining ggplot and base graphics,"<p>I have generated a figure that combines ggplot and base graphics:</p>

<pre><code>t &lt;- c(1:(24*14)) 
P &lt;- 24
A &lt;- 10 
y &lt;- A*sin(2*pi*t/P)+20 
#*****************************************************************************
par(mfrow = c(2,1))
plot(y,type = ""l"",xlab = ""Time (hours)"",ylab = ""Amplitude"")
aa &lt;- par(""mai"")
plot.new()

require(gridBase)
vps &lt;- baseViewports()
pushViewport(vps$figure)
pushViewport(plotViewport(margins = aa)) ## I use 'aa' to set the margins 
#*******************************************************************************
require(ggplot2)
acz &lt;- acf(y, plot = FALSE)
acd &lt;- data.frame(Lag = acz$lag, ACF = acz$acf)
p &lt;- ggplot(acd, aes(Lag, ACF)) + geom_area(fill = ""grey"") +
  geom_hline(yintercept = c(0.05, -0.05), linetype = ""dashed"") +
  theme_bw()
grid.draw(ggplotGrob(p)) ## draw the figure
</code></pre>

<p>I use the plotViewport command and set the dimensions of the panel according to the dimensions of the first panel, obtained by par(""mai""). The figure attached shows the outcome.
<img src=""http://i.stack.imgur.com/WnTce.png"" alt=""enter image description here"">
However, the dimensions of both panels do not match, i.e. the second panel seems to be slightly wider than the first. How can I overcome this without having to manually set the margins with </p>

<pre><code>pushViewport(plotViewport(c(4,1.2,0,1.2)))
</code></pre>
"
408,android - corresponding rotated object to numeric values,"<p>I have a combination lock rotating in a 360 degrees circle.</p>

<p>The combination lock has numerical values on it, these are purely graphical.</p>

<p>I need a way to translate the image's rotation to the 0-99 values on the graphic.</p>

<p>In this first graphic, the value should be able to tell me ""0""</p>

<p><img src=""http://i48.tinypic.com/27y67b7.png""></p>

<p>In this graphic, after the user has rotated the image, the value should be able to tell me ""72""</p>

<p><img src=""http://i46.tinypic.com/2ueiogh.png""></p>

<p>Here is the code: <a href=""http://pastebin.com/YFKufZgs"">http://pastebin.com/YFKufZgs</a> , I think I need to translate some information from the matrix to a 0-99 value.</p>
"
409,R - stuck with plot() - Colouring shapefile polygons based upon a slot value,"<p>I have a shapefile showing remote areas in Australia, obtained from the Australian Bureau of Statistics:</p>

<p><a href=""http://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1270.0.55.005July%202011?OpenDocument"" rel=""nofollow"">http://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1270.0.55.005July%202011?OpenDocument</a></p>

<p>At the same URL is a PDF ""ASGS Remoteness Structure Edition 2011 PDF Maps"" - I am trying to reproduce the first map from this PDF document.</p>

<p>I have read in the shapefile and added colour information to the <code>data</code> slot:</p>

<pre><code>ra &lt;- readShapeSpatial(""RA_2011_AUST"", delete_null_obj = TRUE)
ra@data$COLOUR &lt;- ""#FFFFFF""
ra@data$COLOUR[(as.numeric(as.character(ra@data$RA_CODE11)) %% 10) == 0] &lt;- ""#006837""
ra@data$COLOUR[(as.numeric(as.character(ra@data$RA_CODE11)) %% 10) == 1] &lt;- ""#31A354""
ra@data$COLOUR[(as.numeric(as.character(ra@data$RA_CODE11)) %% 10) == 2] &lt;- ""#78C679""
ra@data$COLOUR[(as.numeric(as.character(ra@data$RA_CODE11)) %% 10) == 3] &lt;- ""#C2E699""
ra@data$COLOUR[(as.numeric(as.character(ra@data$RA_CODE11)) %% 10) == 4] &lt;- ""#FFFFCC""
</code></pre>

<p>The only thing left for me to do is plot the map! This is where I get stuck...</p>

<p><code>ra@polygons</code> is a list of 35 polygons, each of which has a slot <code>ID</code> which is an index to the data frame <code>ra@data</code>. So all I have to do is tell <code>plot()</code> to find the colour in <code>ra@data$COLOUR[ID]</code>. Well, not quite. Each of the 35 polygons (class ""Polygons"") has its own list of polygons (class ""Polygon""); in total there are 6902 polygons!!!</p>

<p>My understanding of <code>plot()</code> is that I have to pass it a vector of colours in the same order as the polygons will be plotted. Therefore I believe I will have to create a vector of length 6902 with each element holding the colour value for the associated polygon. How am I doing so far?</p>

<p>That would be easy enough if the polygons were plotted in order, but they aren't. Each of the 35 Polygons has a slot <code>plotOrder</code> which is an integer vector, so the colour vector will, presumably, have to be ordered by the values in each of these vectors.</p>

<p>At this point this all seems a bit too complex. Am I completely off track here?</p>

<p>Thanks for your advice!</p>
"
410,"R,Rename.file iteratively","<p>I'm trying to write an algorithm in R language which allows me to rename files. I want to rename all pdf files in a folder by numbers 1,2,3,4,5,... and keeping the pdf extension.
The main difficulty is the following: My folders contains files and folders and these folders contain files and folders etc.... I want that all pdf files in all these subfolders are named by numbers starting to 1 and going up to the number of files in this specific folder (following for example the alphabetic order)! Another difficulty: My folders also contain non-pdf files !</p>

<p>Here is an example:
(In the following example Folder 1 contains folder2 and 3 files. folder two contains 4 files.)</p>

<pre><code>Folder1   ""contains""   folder2     ""contains""   Bzzz.file.R
                       A.file.txt               B.file.pdf
                       Bla.file.pdf             C.file.pdf
                       C.file.pdf               Delta.file.pdf
</code></pre>

<p>... should be transform into this....</p>

<pre><code>Folder1  ""contains""  folder2   ""contains""  Bzzz.file.R
                     A.file.txt            1.pdf
                     1.pdf                 2.pdf
                     2.pdf                 3.pdf
</code></pre>

<p>The algorithm should be working whatever is the number of folders ""and how deep is the folder inception""</p>

<p>If this would be working even if there are already files called 3.pdf or other number.pdf in some folders it would be a bit better but I can actually deal without this flexibility !</p>

<p>Thanks a lot for your help !</p>

<p>I ran this code but it is not totally working. If I just run it, I get this error message: ""Error in if (is.na(pathname)) {  : the argument's length is zero"" (I had to translate this error message from french so there might have some missmatch with an error message we can get from R in english). </p>

<p>Then I realized that d is weird because it contains a directory called ""."" at the first position that I don't have in my folder (Do you have an explanation for that by the way ?!).</p>

<p>so I did d&lt;-d[-1] and rerun the big lapply I get this error message: </p>

<p>""Error in setwd(d[x]): impossible to change working directory""
And indeed the files in the first folder were renamed but only in the first folder</p>

<p>Here what list.dirs() gives:</p>

<p>"".""        ""./3.Sept"" ""./4.Oct""  ""./5.Nov""  ""./6.Dec"" </p>

<p>But I only have 4 folders. There is no folder called ""."" ! And as I said if I do d&lt;-d[-1] the code is not succeding at changing directory so that only one folder is renamed</p>
"
411,find length of sequences of identical values in a numpy array,"<p>In a pylab program (which could probably be a matlab program as well) I have a numpy array of numbers representing distances: <code>d[t]</code> is the <em>distance</em> at time <code>t</code> (and the timespan of my data is <code>len(d)</code> time units).</p>

<p>The events I'm interested in are when the distance is below a certain threshold, and I want to compute the duration of these events. It's easy to get an array of booleans with <code>b = d&lt;threshold</code>, and the problem comes down to computing the sequence of the lengths of the True-only words in <code>b</code>. But I do not know how to do that efficiently (i.e. using numpy primitives), and I resorted to walk the array and to do manual change detection (i.e. initialize counter when value goes from False to True, increase counter as long as value is True, and output the counter to the sequence when value goes back to False). But this is tremendously slow.</p>

<p><b>How to efficienly detect that sort of sequences in numpy arrays ?</b></p>

<p>Below is some python code that illustrates my problem : the fourth dot takes a very long time to appear (if not, increase the size of the array)</p>

<pre><code>from pylab import *

threshold = 7

print '.'
d = 10*rand(10000000)

print '.'

b = d&lt;threshold

print '.'

durations=[]
for i in xrange(len(b)):
    if b[i] and (i==0 or not b[i-1]):
        counter=1
    if  i&gt;0 and b[i-1] and b[i]:
        counter+=1
    if (b[i-1] and not b[i]) or i==len(b)-1:
        durations.append(counter)

print '.'
</code></pre>
"
412,How do I multiply (and divide) BCD numbers by 10^x,"<p>I have a large (12 digit) BCD number, encoded in an array of 6 bytes - each nibble is one BCD digit. I need to multiply it by 10^x, where x can be positive or negative. </p>

<p>I know it can be done by shifting left or right by nibble instead of bit, but it's a horrible implementation - especially in Javacard, which is what I'm using. Is there a better way? </p>
"
413,Variance of inverse weighted sum,"<p>Let $X$ be a random variable with variance $Var[X]=\sigma^2$. Given a number of (independent) realizations $X_i$, is it possible to estimate the variance of the inverse weighted sum:
$$
Var\left[\frac{1}{\sum_i a_iX_i}\right]
$$
? What I am actually looking for is an estimate of the variance of the above expression <em>without</em> having a particular set of realizations - just knowing the weights $a_i$ and $\overline X$ and $\sigma^2$.</p>

<p>I found the <a href=""http://en.wikipedia.org/wiki/Delta_method"" rel=""nofollow"">delta method</a>, which is essentially a kind of Taylor expansion:
$$
Var[f(\vec X)] \approx Var[X]\left(\sum_i\left(\frac{\partial}{\partial X_i}f(\vec X)\right)^2\right)\\
=Var[X]\left(\sum_i\left(-\frac{a_i}{2\left(\sum_i a_iX_i\right)^2}\right)^2\right)
$$
However, this expressions still has $X_i$ inside, which doesn't solve my problem.</p>
"
414,Heat map colors corresponding to data in R,"<p>I have a one dimensional vector of data in R and I want to find heat map colors that correspond to this data. For example:</p>

<pre><code>data = c(12,32,33,41,5)
</code></pre>

<p>I then want to find a vector of HEX colors that correspond to that vector - something like higher values have darker colors and lower values have lighter colors or something of that sort. </p>

<p>Are there any packages/functions out there that will do this?</p>

<p>Thanks!</p>
"
415,"Given an R dataframe with column A, how do I create two new columns containing all ordered combinations of A","<p>I have a data.frame with one id column (x below), and a number of variables (y1,y2 below).</p>

<pre><code>    x y1 y2
1   1 43 55
2   2 51 53
[...]
</code></pre>

<p>What I would like to generate from this is a dataframe where the first two columns cover  every ordered combination of x (except where they are equal) along with columns for each variable related to the order. The data frame header and first two rows would look like this (did this by hand, excuse errors):</p>

<pre><code>xi xj y1i y1j y2i y2j
 1  2  43  51  55  53
 2  1  51  43  53  55
[...]
</code></pre>

<p>So each row would container a source and destination (i and j) and then values for y1 at each source and destination.</p>

<p>I'm slowly learning R data manipulation, but this one is stumping me. Kudos for the one line does-it-all answer, as well as a more readable didactic answer.</p>
"
416,Select several subsets by taking different row interval and appy function to all subsets,"<p>How can I select n number of subsets from a data frame by taking every nth row for
subset 1 then nth +1 row for subset 2 then nth+3 for subset3 until nth=n</p>

<p>I have used</p>

<pre><code>subset&lt;-data[seq(nth,length,n),]
</code></pre>

<p>But this gives one subset then I have to keep changing nth from 1...n to get different
subsets.e.g using a small data(106 rows x 742 columns) set to get 10 subsets of every 10th row</p>

<pre><code> subset1&lt;-data[seq(1,106,10),]
 subset2&lt;-data[seq(2,106,10),]
 subset3&lt;-data[seq(3,106,10),]
</code></pre>

<p>Is there any way to do this better?</p>

<p>From going through the FAQ I have tried using loops like</p>

<pre><code>sub&lt;-function(data,nth,length,n){
         sub&lt;-data[seq(nth,length,n),]
         for(n in 1:(sub)){
         sub2&lt;-sub[nth,]+1,sub3&lt;-sub[nth,]+2,sub4&lt;-sub[nth,]+3) }
      su&lt;-(sub,sub2, sub3,sub4)
     return(su)
    } 
sub(data=gag11p,n=1,length=106,10)
</code></pre>

<p>This returns 3 data list with only the last variable in the data frame,I am not sure where I went wrong, also how can I just get the name of
the subset instead of a data frame as I want to apply a PLS calibration function to the subsets created</p>

<p>Please forgive and correct any mistakes since I am now learning programing and R.</p>
"
417,How to run R script line by line from linux shell?,"<p>I'm used to work on my R scripts with some GUI, so I can easily run commands line-by-line, then pause and inspect my objects as they are created and changed.</p>

<p>I currently need to work with some data on a remote server. Is it possible to run line-by-line using R console or some other application (please, not vi) that does not require real GUI?</p>
"
418,Changing whisker definition in geom_boxplot,"<p>Greetings!</p>

<p>I'm trying to use ggplot2 / geom_boxplot to produce a boxplot where the whiskers are defined as the 5 and 95th percentile instead of 0.25 - 1.5 IQR / 0.75 + IQR and outliers from those new whiskers are plotted as usual.  I can see that the geom_boxplot aesthetics include ymax / ymin, but it's not clear to me how I put values in here.  It seems like:</p>

<pre><code>stat_quantile(quantiles = c(0.05, 0.25, 0.5, 0.75, 0.95))
</code></pre>

<p>should be able to help, but I don't know how to relate the results of this stat to set the appropriate geom_boxplot() aesthetics:</p>

<pre><code>geom_boxplot(aes(ymin, lower, middle, upper, ymax))
</code></pre>

<p>I've seen other posts where people mention essentially building a boxplot-like object manually, but I'd rather keep the whole boxplot gestalt intact, just revising the meaning of two of the variables being drawn.</p>

<p>Thanks!</p>

<p>Chris</p>
"
419,getSymbols (quantmod) giving wrong dates,"<p>I'm using the quantmod package to fetch stock data. The code</p>

<pre><code>Data = getSymbols('LON:ADN',src=""google"",auto.assign=FALSE, from = '2011-08-10')
</code></pre>

<p>Results in an xts as expected, however on closer examination it shows a volume of trades for 2012-10-21 (October 21st) which was a sunday, and is therefore clearly erroneous. Several other sundays are also included. Unfortunately the errors surrounding the weekends seem to have moved the rest of the data out of alignment.</p>

<p>Has anyone experienced similar problems fetching tickers with quantmod before, and if so, are they aware of a way around them? </p>

<p>Thanks</p>
"
420,"""Approximating"" the derivative of date points in R","<p>So I have a time series of MODIS NDVI values (vegetation values from 0-1 for the non-geographic geeks), and I'm trying to approximate the derivative by using a <code>for</code> loop.</p>

<p>This is a sample of my data:</p>

<pre><code>&gt; m2001
   date  value    valnorm
1     1 0.4834 0.03460912
2    17 0.4844 0.03664495
3    33 0.5006 0.06962541
4    49 0.4796 0.02687296
5    65 0.5128 0.09446254
6    81 0.4915 0.05109935
7    97 0.4664 0.00000000
8   113 0.5345 0.13864007
9   129 0.8771 0.83611564
10  145 0.9529 0.99043160
11  161 0.9250 0.93363192
12  177 0.9450 0.97434853
13  193 0.9491 0.98269544
14  209 0.9434 0.97109121
15  225 0.9576 1.00000000
16  241 0.8992 0.88110749
17  257 0.9115 0.90614821
18  273 0.8361 0.75264658
19  289 0.5725 0.21600163
20  305 0.5188 0.10667752
21  321 0.5467 0.16347720
22  337 0.5484 0.16693811
23  353 0.5427 0.15533388
</code></pre>

<ul>
<li>Column 1 is the julian day of the pixel value</li>
<li>Column 2 is the raw NDVI value</li>
<li>Column 3 is the NDVI stretched from 0-1 (it's a normalization technique, since NDVI rarely actually gets to 1 or 0).</li>
</ul>

<p>I'm still very new to programming and R, but I think I've managed to piece together a tenuous grasp on it.  What I'm trying to do is create a new column with values that would give me some idea of the local slope of data points.</p>

<p>The function I've come up with is this:</p>

<pre><code>deriv &lt;- function(x1=1:23, x2=1){
    for (i in x1){
    i1 &lt;- c(x1[i-1], x1[i], x1[i+1])
    i2 &lt;- c(x2[i-1], x2[i], x2[i+1])
        deriv.func &lt;- lm(i2~i1, na.action=NULL)
    } return(deriv.func$coef[[2]])
}
</code></pre>

<p>What happens when I run it is this:</p>

<pre><code>&gt; deriv &lt;- function(x1=1:23, x2=1){
+ for (i in x1){
+     i1 &lt;- c(x1[i-1], x1[i], x1[i+1])
+     i2 &lt;- c(x2[i-1], x2[i], x2[i+1])
+ deriv.func &lt;- lm(i2~i1, na.action=NULL)
+ } return(deriv.func$coef[[2]])
Error: unexpected symbol in:
""deriv.func &lt;- lm(i2~i1, na.action=NULL)
} return""
&gt; }
Error: unexpected '}' in ""}""
&gt;
</code></pre>

<p>I'm not sure what I'm doing wrong, as I can get it to parse when I fill in a value for i </p>

<pre><code>&gt; i=6
&gt; x1=m2001$date
&gt; x2=m2001$valnorm
&gt;     i1 &lt;- c(x1[i-1], x1[i], x1[i+1])
&gt;     i2 &lt;- c(x2[i-1], x2[i], x2[i+1])
&gt; i1
[1] 33 49 65
&gt; i2
[1] 0.06962541 0.02687296 0.09446254
&gt; lm(i2 ~ i1)

Call:
lm(formula = i2 ~ i1)

Coefficients:
(Intercept)           i1  
  0.0256218    0.0007762  

&gt; func &lt;- lm(i2 ~ i1)
&gt; func$coef[[2]]
[1] 0.0007761604
</code></pre>

<p>Any ideas?  Thanks a ton.</p>
"
421,How do you animate a display object in an arc in AS3?,"<p>This is for a game in a flash AS3 only project.</p>

<p>the player controls a character with a gun.  By clicking on the screen the gun fires a missile in an arc to the point clicked.</p>

<p>Whats the best way to calculate the x and y co-ordinates of the missile for each frame?</p>
"
422,weird error with R when using data.table,"<p>I'm doing some small calculations and i decided to fill the data inside a <code>data.table</code> since it's much faster than <code>data.frame</code> and <code>rbind</code></p>

<p>so basically my code is something like that: </p>

<p><code>df</code> is a <code>data.frame</code> used in the calculation but it's important what does it contain.</p>

<pre><code>l=12000
dti = 1
dt = data.table(ni = 0, nj = 0, regerr = 0)
for (i in seq(1,12000,200)) {
    for (j in seq(1, 12000, 200)) {
        for (ind in 1:nrow(df)) {
            if( i+j &gt;= l/2 ){
                df[ind,]$X =  df[ind,]$pos * 2
            } else {
                df[ind,]$X = df[ind,]$pos/l
            }
        }
        for (i in 1:100) { # 100 sample
            sample(df$X,nrow(df), replace=FALSE) 
            fit=lm(X ~ gx, df)   #linear regression calculation
            regerror=sum(residuals(fit)^2)

            print(paste(i,j,regerror))
            set(dt,dti,1L,as.double(i))             
            set(dt,dti,2L,as.double(j))             
            set(dt,dti,3L,regerror)             
            dti=dti+1

        }
     }
 }
</code></pre>

<p>The code prints the first few rounds of <code>print(paste(i,j,regerror))</code> and then it quits with this error:</p>

<pre><code> *** caught segfault ***
address 0x3ff00008, cause 'memory not mapped'
Segmentation fault (core dumped)
</code></pre>

<p><em><strong>EDIT</em></strong></p>

<pre><code>structure(list(ax = c(-0.0242214, 0.19770304, 0.01587302, -0.0374415, 
0.05079826, 0.12209738), gx = c(-0.3913043, -0.0242214, -0.4259067, 
-0.725, -0.0374415, 0.01587302), pos = c(11222, 13564, 16532, 
12543, 12534, 14354)), .Names = c(""ax"", ""gx"", ""pos""), row.names = c(NA, 
-6L), class = ""data.frame"")
</code></pre>

<p>Any ideas are appreciated.</p>
"
423,Randomly Generate a set of numbers of n length totaling x,"<p>I'm working on a project for fun and I need an algorithm to do as follows:
Generate a list of numbers of Length <code>n</code> which add up to <code>x</code></p>

<p>I would settle for list of integers, but ideally, I would like to be left with a set of floating point numbers.</p>

<p>I would be very surprised if this problem wasn't heavily studied, but I'm not sure what to look for.</p>

<p>I've tackled similar problems in the past, but this one is decidedly different in nature. Before I've generated different combinations of a list of numbers that will add up to x. I'm sure that I could simply bruteforce this problem but that hardly seems like the ideal solution. </p>

<p>Anyone have any idea what this may be called, or how to approach it? Thanks all!</p>

<p>Edit: To clarify, I mean that the list should be length N while the numbers themselves can be of any size.</p>

<p>edit2: Sorry for my improper use of 'set', I was using it as a catch all term for a list or an array. I understand that it was causing confusion, my apologies.</p>
"
424,High volume SVM (machine learning) system,"<p>I working on a possible machine learning project that would be expected to do high speed computations for machine learning using SVM (support vector machines) and possibly some ANN.</p>

<p>I'm resonably comfortable working on matlab with these, but primarly in small datasets, just for experimentation. I'm wondering if this matlab based approach will scale? or should i be looking into something else? C++ / gpu based computing? java wrapping of the matlab code and pushing it onto app engine?</p>

<p>Incidentally, there seems to be a lot fo literature on GPUs, but not much on how useful they are on machine learning applications using matlab, &amp; the cheapest CUDA enlabled GPU money can buy? is it even worth the trouble?</p>
"
425,Looping knitr options,"<p>I'm curious, is there a way to incorporate changes in knitr options in a loop?  For example, if I wanted to loop through and see how the same block of code looked in all different knitr themes, my first guess would be:</p>

<p>\documentclass{article}</p>

<pre><code>\begin{document}

&lt;&lt;test&gt;&gt;=
themes&lt;-knit_theme$get()


for (a.theme in themes){

  knit_theme$set(a.theme)

  a &lt;- 3+5
   b&lt;- sum(1:10, na.rm=T)
  for(g in 1:10) z&lt;-0
}
@

\end{document}
</code></pre>

<p>And yet, this produces some pretty odd output.  Is there a way to use loops like this, to dynamically change output, or perhaps dynamically include or not include certain chunks?</p>
"
426,Fixing an infix parser,"<p>I have a mathematical expression parser that should handle <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>^</code>, <code>(-)</code>, functions, and of course atoms (such as <code>x</code>, <code>1</code>, <code>pi</code>, etc.).  The parser is basically designed according to <a href=""https://en.wikipedia.org/wiki/Operator-precedence_parser"" rel=""nofollow"">Wikipedia's operator precedence parser</a>, which I've reproduced below; <code>parse_primary()</code> is defined elsewhere.</p>

<pre><code>parse_expression ()
    return parse_expression_1 (parse_primary (), 0)

parse_expression_1 (lhs, min_precedence)
    while the next token is a binary operator whose precedence is &gt;= min_precedence
        op := next token
        rhs := parse_primary ()
        while the next token is a binary operator whose precedence is greater
                 than op's, or a right-associative operator
                 whose precedence is equal to op's
            lookahead := next token
            rhs := parse_expression_1 (rhs, lookahead's precedence)
        lhs := the result of applying op with operands lhs and rhs
    return lhs
</code></pre>

<p>How can I modify this parser to handle <code>(-)</code> correctly?  Better yet, how can I implement a parser with support for all infix and postfix operators (<code>!</code> for instance) that I might want?  Finally, how should functions be treated?</p>

<p>I should note that <code>(-)</code> negation is distinguished in the lexer from <code>-</code> ""subtraction"", so it can be treated as a different token.</p>
"
427,Calling qplot with data.frames with string col names,"<p>I have a data.frame created with:</p>

<pre><code>d &lt;- data.frame(""name 1""=1, ""name 2""=2)
</code></pre>

<p>Calling <code>qplot(""name 1"", ""name 2"", data=d)</code> does not work, for obvious reasons. Is it possible to make it work using some mechanism? I've tried <code>as.name</code> but that also doesn't work.</p>
"
428,R aov formula error term: contradictory examples,"<p>I've seen two basic approaches to generic formulas for within-subjects designs in R/aov() (R = random, X = dependent, W? = within, B? = between):</p>

<pre><code>Pure within: 
    X ~ Error(R/W1*W2...)
or  
    X ~ (W1*W2...) + Error(R/(W1*W2...))

Mixed:
    X ~ B1*B2*... + Error(R/W1*W2...)
or  
    X ~ (B1*B2*...W1*W2...) + Error(R/(W1*W2...)+(B1*B2...))
</code></pre>

<p>That is, some advise never putting W factors outside the error term or B factors inside, while others put all (B, W) factors outside and inside, indicating in the error term which are nested within R.</p>

<p>Are these simply notational variants? Is there any reason to prefer one to the other as a default for performing ANOVA using aov()? </p>
"
429,How to map numpy arrays to one another?,"<p>I have two (A, B) boolean arrays of the same finite, but arbitrarily large, and only known at runtime shape and dimensions.</p>

<p>I want to calculate the value of a boolean function of corresponding elements in A and B and store them in C. At last I need a list of tuples where C is true.</p>

<p>How to get there?</p>

<p>I dont want to iterate over the single elements, because I dont know how many dimensions there are, there must be a better way.</p>

<pre><code>&gt;&gt;&gt; A = array([True, False, True, False, True, False]).reshape(2,3)
&gt;&gt;&gt; B = array([True, True, False, True, True, False]).reshape(2,3)
&gt;&gt;&gt; A == B
array([[ True, False, False],
       [False,  True,  True]], dtype=bool)
</code></pre>

<p>as wanted, but:</p>

<pre><code>&gt;&gt;&gt; A and B
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
</code></pre>

<p>How do I get ""A and B""?</p>

<p>I tried ""map"", ""zip"", ""nditer"" and searched for other methods unsuccessfully.</p>

<p>As for the things with the tuples, I need something like ""argmax"" for booleans, but didn't find anything either.</p>

<p>Do you know somethign, that might help?</p>
"
430,Tuple Relational Calculus or Relational Algebra Syntax verifier?,"<p>Does anybody knows of a software package where you can type a formula and it verifies sintactically (it doesn't need to check semantics).</p>

<p>Thanks.</p>
"
431,R: Changing the names in a named vector,"<p>R's named vectors are incredibly handy, however, I want to combine two vectors which contain the estimates of coefficients and the standard errors for those estimates, and both vectors have the same names:</p>

<pre><code>&gt; coefficients_estimated
     y0         Xit  (Intercept) 
    1.1         2.2          3.3 

&gt; ses_estimated
     y0         Xit  (Intercept) 
    .04         .11         .007
</code></pre>

<p>This would be easy to solve if I knew what order the elements were in for sure, but this isn't guaranteed in my script, so I can't simply do <code>names(ses_estimated) &lt;- whatever</code>. All I want to do is add either ""coef"" or ""se"" to the end of each name, and to do this, I've come up with what I think is a pretty ugly hack:</p>

<pre><code>names(coefficients_estimated) &lt;- sapply(names(coefficients_estimated),
                                        function(name)return(paste(name,""coef"",sep=""""))
                                        )
names(ses_estimated) &lt;- sapply(names(ses_estimated),
                               function(name)return(paste(name,""se"",sep=""""))
                               )
</code></pre>

<p>Is there an idomatic way to do this? Or am I going to have to stick with what I've written?</p>
"
432,J programming Language vs R Programming Language vs Incanter,"<p>Has anyone tried both <a href=""http://www.jsoftware.com/"" rel=""nofollow"">J</a> programming language form jsoftware and <a href=""http://www.r-project.org/"" rel=""nofollow"">R</a> language. After some search I faced <a href=""http://incanter.org/"" rel=""nofollow"">incanter</a> which is based on clojure. I want to learn a statistical language for data analysis. Which one do you prefer? Why? </p>

<p>Please consider conditions below, thanks.</p>

<ul>
<li>productivity</li>
<li>performance</li>
<li>community</li>
<li>library</li>
<li>syntax</li>
</ul>
"
433,How to correctly edit strings using regular expressions & R?,"<p>I imported data from tab-delimited text files into a data.frame. Now I'd like to adjust and edit the contents of a column called C1. Regular expressions were recommended to me for this purpose. At first I used:  </p>

<pre><code>for (rn in 1:length(C1))
C1s &lt;- strsplit(as.character(C1[rn]), ""; "", fixed = TRUE)[[1]]
</code></pre>

<p>to separate individual entries on examples such as these:  </p>

<ul>
<li>[Zhang, Junling] China Agr Univ, Coll Resources &amp; Environm Sci, Beijing 100094, Peoples R China; [Zhang, Junling] Univ Hohenheim, Inst Plant Nutr, D-7000 Stuttgart, Germany; [George, Eckhard] Humboldt Univ, Inst Crop Sci, Dept Plant Nutr, D-14979 Grossbeeren, Germany; [George, Eckhard] Leibniz Inst Vegetable &amp; Ornamental Crops Theodor, D-14979 Grossbeeren, Germany  </li>
<li>UNIV DORTMUND,INST PHYS,D-44221 DORTMUND,GERMANY; HUMBOLDT UNIV,INST PHYS,D-15738 ZEUTHEN,GERMANY  </li>
<li>UNIV KEELE,DEPT CHEM,KEELE ST5 5BG,STAFFS,ENGLAND; MT SINAI HOSP,MT SINAI SCH MED,DR A A FISHBERG CTR NEUROBIOL,NEW YORK,NY 10029; HUMBOLDT UNIV BERLIN,CHARITE HOSP,DEPT PATHOL &amp; CLIN BIOCHEM,BERLIN,GERMANY  </li>
<li>NIFAD,MOLEC MICROBIOL LAB,BETHESDA,MD  </li>
</ul>

<p>Then I wanted to  </p>

<ul>
<li>set everything to upper case</li>
<li>replace the strings ""Humboldt...Germany"" with ""HUMBOLDT""   replace the strings ""England"", ""Scotland"", ""Wales"", and ""North Ireland"" with ""UNITED KINGDOM""</li>
<li>replace combination of U.S. state abbreviations and ZIP codes with ""USA""</li>
<li>replace U.S. state abbreviations with ""USA""</li>
</ul>

<p>In addition I wanted to delete everything but semicolons (including the spaces after them), the words directly in front of them, and the very last word from the examples above.<br>
I used  </p>

<pre><code>gsub('[.*\\] ', ''(toupper(C1s))  
</code></pre>

<p>and  </p>

<pre><code>gsub(',\\s*', ','(toupper(C1s))  
</code></pre>

<p>, e.g., but couldn't get it to work correctly.<br>
I'd like to get the following output:  </p>

<ul>
<li>PEOPLES R CHINA; GERMANY; HUMBOLDT; GERMANY  </li>
<li>GERMANY; HUMBOLDT  </li>
<li>UNITED KINGDOM; USA; HUMBOLDT  </li>
<li>USA  </li>
</ul>

<p>So my question is: How can I achieve the results I desire?<br>
Thank you very much in advance for your consideration!</p>

<p><strong>Update and additional problem</strong>  </p>

<p>Thanks to mrdwab's helpful reply and comments I made a lot of headway.  </p>

<p>Unfortunately, only now I realized that there also are addresses such as these with more than one author in square brackets. The algorithm proposed by mrdwab doesn't work correctly with these, unfortunately.  </p>

<pre><code>&gt; test = c(""[Bocquet, F. C.; Giovanelli, L.; Abel, M.; Porte, L.; Themlin, J. -M.] Aix Marseille Univ, Inst Mat Microelect &amp; Nanosci Prov IM2NP, F-13397 Marseille 20, France; [Bocquet, F. C.; Giovanelli, L.; Abel, M.; Porte, L.; Themlin, J. -M.] CNRS, Inst Mat Microelect &amp; Nanosci Prov IM2NP, UMR 6242, Marseille, France; [Amsalem, P.; Koch, N.] Humboldt Univ, Inst Phys, D-12489 Berlin, Germany; [Petaccia, L.; Topwal, D.; Gorovikov, S.; Goldoni, A.] Sincrotrone Trieste, I-34149 Trieste, Italy"")
</code></pre>

<p>This is the result I got:</p>

<pre><code>&gt; test
[1] ""[BOCQUET,  F. C."" ""GIOVANELLI,  L.""  ""ABEL,  M.""        ""PORTE,  L.""      
[5] ""FRANCE""           ""[BOCQUET,  F. C."" ""GIOVANELLI,  L.""  ""ABEL,  M.""       
[9] ""PORTE,  L.""       ""FRANCE""           ""[AMSALEM,  P.""    ""HUMBOLDT""        
[13] ""[PETACCIA,  L.""   ""TOPWAL,  D.""      ""GOROVIKOV,  S.""   ""ITALY"" 
</code></pre>

<p>This is the result I'd like to get instead:</p>

<pre><code>[1] ""FRANCE""; ""FRANCE""; ""HUMBOLDT""; ""ITALY""
</code></pre>

<p>I tried using this in order to delete each square bracket and its contents individually:</p>

<pre><code>C1s = gsub(""(.*)[(.*)]"", ""\\2"", C1s)
</code></pre>

<p>But instead of that everything between the first opening bracket and the last closing bracket was deleted ...
Maybe it'd work if I replaced all semicolons within square brackets by commas first? I tried</p>

<pre><code>C1s = gsub(""[(.*);(.*)]"", ""[(.*),(.*)]"", C1s)
</code></pre>

<p>to accomplish that, but it didn't work.<br>
So I'd appreciate your help in that regard!</p>

<p>In addition to that I'm still stuck at yet another obstacle which I just cannot seem to resolve on my own, unfortunately ...<br>
This is my current output:  </p>

<pre><code>&gt; C1s
[1] ""PEOPLES R CHINA"" ""GERMANY""         ""HUMBOLDT""         ""GERMANY""
[5] ""GERMANY""         ""HUMBOLDT""        ""UNITED KINGDOM""   ""USA""
[9] ""HUMBOLDT""        ""USA""
&gt; dims
[1] 4 2 3 1
&gt; is.list(C1)
[1] FALSE
&gt; is.vector(C1)
[1] TRUE
</code></pre>

<p>But how exactly can I use the information in <em>dims</em> to create this desired output?:  </p>

<pre><code>[1] ""PEOPLES R CHINA""; ""GERMANY""; ""HUMBOLDT""; ""GERMANY""
[2] ""GERMANY""; ""HUMBOLDT""
[3] ""UNITED KINGDOM""; ""USA""; ""HUMBOLDT""
[4] ""USA""
</code></pre>

<p>Thank you very much in advance for your support!</p>
"
434,R data frame: Assign weights based on frequency of occurrence of values,"<p>I would like to ask you for help with my data frame. It is a vector of many phases and for every one we have names of variables. Lets say </p>

<pre><code>vec&lt;-data.frame(phase1= c(""var1"",""var2"",""var3"",""var4"",""var5"",""var6""),     
                 phase2= c(""var1"",""var3"",""var4"",""var2"",""var6"",""var5""),    
                 phase3= c(""var4"",""var3"",""var2"",""var1"",""var6"",""var5""))

 vec
  phase1 phase2 phase3
1   var1   var1   var4
2   var2   var3   var3
3   var3   var4   var2
4   var4   var2   var1
5   var5   var6   var6
6   var6   var5   var5
</code></pre>

<p>Now, lets say we are interested for the first 3 rows and therefore the weight of variable in one of them is 1/3, zero otherwise. My function would ideally output sth like that:</p>

<pre><code>          phase1 phase2 phase3
   var1   0.33   0.33    0
   var2   0.33   0       0.33
   var3   0.33   0.33    0.33
   var4   0      0.33    0.33
   var5   0      0       0
   var6   0      0       0
</code></pre>

<p>The function should also be applicable for the first 4, 5 or all 6 rows (ie the weights will change then).
Regards,
Alex</p>
"
435,How to program a complex set of interrelations?,"<p>First, I'm sorry about the title, that was the best I could come up with.</p>

<p>What I mean is something like, a program that tells me that when A goes up, B goes down, which makes C go up, which increases the likelihood that D and E will go down, sending F straight up and so on.</p>

<p>A couple people told me to look into Linear Programming, but it doesn't sound right because I don't want to <em>find</em> an answer ('solve for x to find the best way to whatever'), I want to change the value of one metric and see the ripple effect across other, connected metrics. </p>

<p>For example, suppose I have a factory and I want to estimate the impact a raise for the workers of one sector will have. A raise of 10% will increase sector morale by X, sector productivity by Y and increase costs by Z. This will force us to raise the price of our product to P, if we want to maintain our profit of Q, which might get us out of some stores and actually decrease profit by R.</p>

<p>Now, a raise of 20% over 2 years will increase sector morale by A, sector productivity by B and increase costs by C, but will lower morale from other sectors by D, unless...</p>

<p>I think you can get the idea. What I want is to figure out how to program the underlying network that makes all that happen, based on any relevant input (for example, what would have to change if I wanted to increase sector productivity? I type in the new value for productivity and watch the appropriate connected values update by themselves). </p>

<p>If this is indeed the domain of Linear Programming, could someone provide examples or an explanation centered around that model? I'm having a hard time seeing it.</p>

<p>Thanks a lot.</p>
"
436,Calculating Euler's Number in C++,"<p><em>Write a program that calculates Eulers number e. To do this, first write a function that takes a parameter n, and returns the result (1+1/n)<sup>n</sup>. The limit of this function approaches e as n approaches infinity. In your main program, write a loop that calls this function with increasing values of n. On each iteration, multiply n by 2 (if you just add 1 to n each time, the algorithm won' work) and stop when the your new approximation and your previous approximation differ by less than 1e-8. Now you have a pretty good estimate. So in main(), print your best approximation and the number n that generated it.</em></p>

<p>I have done everything up until the for-loop. I don't quite understand how am I supposed to stop for-loop after new and previous numbers are approximately the same. </p>

<p>Here is my function:</p>

<pre><code>double euler_finder(double n)
{
    return pow((1+1/n), n);
}
</code></pre>

<p>And here is my for-loop in the main method, where ??? is where I'm having a problem:</p>

<pre><code>for (int i=0; i&lt;????; i*=2) {

}
</code></pre>

<p>EDIT: The solution has been posted so here how it looks like:</p>

<pre><code>#include &lt;iostream&gt;
#include &lt;iomanip&gt;
#include &lt;cmath&gt;

using namespace std;

double euler(int n);

int main()
{
    int trialN = 4;

    double guess1, guess2;

    guess1 = euler(1);
    guess2 = euler(2);

    while(  abs(guess1-guess2) &gt; 1e-8 )
    {
        cout&lt;&lt; trialN &lt;&lt; "" "" &lt;&lt; guess2&lt;&lt;endl;
        guess1 = guess2;
        guess2 = euler( trialN );
        trialN*=2;
    }

    cout&lt;&lt;setprecision(8)&lt;&lt;""e is approximately ""&lt;&lt;guess2&lt;&lt;"" and we got it with a value of "";
    cout&lt;&lt;trialN&lt;&lt;"" for n"";

    return 0;
}

double euler(int n)
{
    return pow((1+1.0/n), n);
}
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>4 2.25
8 2.44141
16 2.56578
32 2.63793
64 2.67699
128 2.69734
256 2.70774
512 2.71299
1024 2.71563
2048 2.71696
4096 2.71762
8192 2.71795
16384 2.71812
32768 2.7182
65536 2.71824
131072 2.71826
262144 2.71827
524288 2.71828
1048576 2.71828
2097152 2.71828
4194304 2.71828
8388608 2.71828
16777216 2.71828
33554432 2.71828
67108864 2.71828
134217728 2.71828
e is approximately 2.7182818 and we got it with a value of 268435456 for n
</code></pre>
"
437,Are there statistics to know how many times a given library has been downloaded from maven central repository?,"<p>I released a little library to maven central repository and I am curious to know if this library is used by somebody.</p>

<p>Is there a way to know this? Are there some published download statistics?</p>
"
438,Can coordinates of constructable points be represented exactly?,"<p>I'd like to write a program that lets users draw points, lines, and circles as though with a straightedge and compass. Then I want to be able to answer the question, ""are these three points collinear?"" To answer correctly, I need to avoid rounding error when calculating the points.</p>

<p>Is this possible? How can I represent the points in memory?</p>

<p>(I looked into some unusual numeric libraries, but I didn't find anything that claimed to offer both exact arithmetic and exact comparisons that are guaranteed to terminate.)</p>
"
439,Manipulating indices to 2d numpy array,"<p>I can index a 2d numpy array with a tuple or even a list of tuples</p>

<pre><code>a = numpy.array([[1,2],[3,4]])
i = [(0,1),(1,0)] # edit: bad example, should have taken [(0,1),(0,1)]
print a[i[0]], a[i]
</code></pre>

<p>(Gives <code>2 [2 3]</code>)</p>

<p>However, I can not manipulate the tuples with vector arithmetic, i.e.</p>

<pre><code>k = i[0]+i[1]
</code></pre>

<p>does not give the desired <code>(1,1)</code> but concatenates.</p>

<p>On the other hand using numpy arrays for the indices, the arithmetic works, but the indexing does not work.</p>

<pre><code>i = numpy.array(i)
k = i[0]+i[1]      # ok
print a[k]
</code></pre>

<p>gives the array <code>[[3 4], [3 4]]</code> instead of the desired <code>4</code>.</p>

<p><strong>Is there a way</strong> to <strong>do vector arithmetic on the indices</strong> but also be able <strong>to</strong> index a <strong>numpy array</strong> with them (without deriving a class from tuple and overloading all the operators)? </p>

<p><a href=""http://stackoverflow.com/questions/2016842/selecting-indices-for-a-2d-array-in-numpy"">This question</a> looked promising at first but I could not figure out if I can apply it to my situation.</p>

<p>Edit (comment on accepted answer):</p>

<p>... and working on arrays of indices then works as well using map</p>

<pre><code>arr = numpy.array([[1,2,3],[4,5,6],[7,8,9]])
ids = numpy.array([(0,1),(1,0)])
ids += (0,1) # shift all indices by 1 column
print arr[map(tuple,ids.T)]
</code></pre>

<p>(confusing to me why I need the transpose, though.
Would have run into this problem above as well,
and was just fortunate with [(0,1),(0,1)])</p>
"
440,Gradient Boost Decision Tree (GBDT) or Multiple Additive Regression Tree(MART): Calculating gradient/pseudo-response,"<p>I'm implementing MART from (<a href=""http://www-stat.stanford.edu/~jhf/ftp/trebst.pdf"" rel=""nofollow"">http://www-stat.stanford.edu/~jhf/ftp/trebst.pdf</a>) algorithm 5,
My algorithm ""works"" for say less data(3000 training data file, 22 features) and J=5,10,20 (# of leaf nodes) and T = 10, 20. It gives me good result (R-Precision is 0.30 to 0.5 for training) but when I try to run on some what large training data (70K records) it gives me runtime underflow error - which I think it should be - just don't know how workaround this problem?</p>

<p>Underflow err comes here, calculating gradient of cost (or pseudo-response):
<img src=""http://i.stack.imgur.com/USBV9.png"" alt=""enter image description here""></p>

<p>here y_i are {1,-1} labels so if I just try: 2/exp(5000) its overflow in denominator!</p>

<p>Just wondering if I can ""normalize"" this or ""threshold"" this, but then I'm using this pseudo-response in calculating ""label"" (gamma in that pdf), and then those gamma to calculate model scores. </p>
"
441,Where in Python can I find real_fft() these days? It's not in numpy.fft,"<p>I have some ancient code (5 years old) and how I used to access real_fft() method was this:</p>

<pre><code>from FFT import *
real_fft(data, fft_length)
</code></pre>

<p>I guess the FFT module came with NumPy. Now, years later I installed the NumPy 1.6.1 with</p>

<pre><code>pip install numpy
</code></pre>

<p>And all I see in the docs <a href=""http://www.scipy.org/Numpy_Functions_by_Category"" rel=""nofollow"">http://www.scipy.org/Numpy_Functions_by_Category</a>, are these functions:</p>

<p>fft()</p>

<p>fftfreq()</p>

<p>fftshift()</p>

<p>ifft()</p>

<p>It is strange, because in this numpy docs, real_fft() is there:</p>

<p><a href=""http://numpy.sourceforge.net/numdoc/HTML/numdoc.htm#pgfId-304711"" rel=""nofollow"">http://numpy.sourceforge.net/numdoc/HTML/numdoc.htm#pgfId-304711</a></p>
"
442,A proper vector similarity index,"<p>I'm trying to adjust cosine similarity to determine how similar two vectors are, with respect to entries. Since the obtained measure is invariant under vector scale {(0, 1, 2) and (0, 2, 4) have cosine similarity of 1}, what would be the way to extend the similarity measure to account for the initial vector scale? I thought of multiplying by min{|v1|, |v2|}/max{|v1|, |v2|}, with |v| denoting a vector v norm, to preserve the bounds of -1 and 1. Any suggestions are highly appreciated.</p>
"
443,Combining Multiple Identically-Named Columns in R,"<p>I'd like to combine multiple columns of a data frame in R. Given data that look something like this:</p>

<pre><code>names: 123   256   192   123   256
       1     2     8     2     3
       4     3     2     9     9
       8     7     1     3     8 
</code></pre>

<p>How would I sum the elements of the identically-named columns to produce a table like so:</p>

<pre><code>names: 123   256   192
       3     5     8
       13    12    2
       11    15    1
</code></pre>

<p>Thank you very much.</p>
"
444,Extract links from html table using R,"<p>Im trying to extract the links from the following webpage <a href=""http://ipt.humboldt.org.co/"" rel=""nofollow"">http://ipt.humboldt.org.co/</a> that are of type ""Specimen"" using R. I can get R to get the table from the webpage using the following code:</p>

<pre><code>library(XML)
sitePage&lt;-htmlParse(""http://ipt.humboldt.org.co/"")
tableNodes&lt;-getNodeSet(sitePage,""//table"")
siteTable&lt;-readHTMLTable(tableNodes[[1]])
</code></pre>

<p>However the links are missing after I use the readHTML command. Anyone can help?</p>
"
445,Calculating the MSE for assessment,"<p>Let $X_1, \ldots, X_n \sim \mathcal{N}(\mu, \sigma^2)$ be the sample, when $\mu$, $\sigma$ are unknown.</p>

<p>We suggest assessment for $\sigma^2$: 
$$S^2 = \frac{\displaystyle\sum_{i=1}^n (X_i - \bar{X})^2}{n-1}$$</p>

<p>Now, I know this assesment is unbiased, so for calculating the MSE I have to calculate the expectation.</p>

<p>Any suggestions for this? I'm pretty stuck in this part.</p>
"
446,Precision issues in finding number of ways to represent a number as a sum of squares,"<p>I was trying this problem,  <a href=""http://www.spoj.pl/problems/TWOSQ/"" rel=""nofollow"">http://www.spoj.pl/problems/TWOSQ/</a> .  We have to find the number of different ways to express a number(as large as 10^15) as a sum of squares(without counting twice i.e 5^2 + 1^2 and 1^2 + 5^2 are the same). I have seen this task before and this was how I solved it earlier too. I keep getting Wrong Answer on the judge. Could some one tell me why? or suggest a differnt approach altogether. I have added comments as necessary for understanding . Thanks in advance!.</p>

<pre><code>#include&lt;cstdio&gt;
#include&lt;iostream&gt;
#include&lt;cmath&gt;
using namespace std;

int main()
{
long long X;
cin &gt;&gt; X;
const double EPS = 1e-6;
long long int count = 0;
// add EPS to avoid flooring x.99999 to x
for (int a = 0; a &lt;= sqrt(X/2) + EPS; a++)
{
    long long int b2 = X - a*a; // b^2
    long long int b = (long long int) (sqrt(b2) + EPS);
    if (abs(b - sqrt(b2)) &lt; EPS) // check b is an integer
        count++;
}
cout &lt;&lt; count &lt;&lt; endl;
</code></pre>

<p>}</p>
"
447,How do I set width of candles in candle chart using plot.xts?,"<p>I have simple OHLC data in and XTS </p>

<pre><code>SF &lt;- structure(c(1.064, 1.07, 1.071, 1.08, 1.08, 1.076, 1.078, 1.08,
1.08, 1.082, 1.081, 1.082, 1.074, 1.07, 1.073, 1.075, 1.081,
1.084, 1.092, 1.091, 1.097, 1.095, 1.099, 1.094, 1.096, 1.097,
1.096, 1.096, 1.097, 1.091, 1.078, 1.083, 1.088, 1.084, 1.081,
1.095, 1.096, 1.085, 1.074, 1.075, 1.073, 1.07, 1.068, 1.072,
1.084, 1.08, 1.081, 1.077, 1.081, 1.083, 1.084, 1.083, 1.082,
1.082, 1.075, 1.074, 1.075, 1.092, 1.086, 1.092, 1.093, 1.098,
1.102, 1.103, 1.099, 1.098, 1.1, 1.101, 1.098, 1.098, 1.1, 1.092,
1.084, 1.087, 1.088, 1.084, 1.096, 1.099, 1.097, 1.086, 1.078,
1.076, 1.076, 1.073, 1.064, 1.069, 1.071, 1.077, 1.075, 1.074,
1.078, 1.078, 1.08, 1.079, 1.078, 1.073, 1.068, 1.07, 1.069,
1.074, 1.08, 1.083, 1.089, 1.09, 1.096, 1.094, 1.092, 1.092,
1.094, 1.094, 1.09, 1.092, 1.088, 1.08, 1.076, 1.078, 1.081,
1.079, 1.08, 1.09, 1.084, 1.072, 1.073, 1.069, 1.066, 1.07, 1.067,
1.072, 1.08, 1.079, 1.076, 1.077, 1.08, 1.08, 1.082, 1.081, 1.081,
1.074, 1.072, 1.073, 1.074, 1.081, 1.084, 1.091, 1.092, 1.097,
1.097, 1.099, 1.095, 1.095, 1.097, 1.097, 1.096, 1.094, 1.091,
1.08, 1.083, 1.086, 1.083, 1.082, 1.095, 1.096, 1.086, 1.074,
1.075, 1.073, 1.071, 1.072), .Dim = c(42L, 4L), .Dimnames = list(
NULL, c(""Open"", ""High"", ""Low"", ""Close"")), index = structure(c(1353427200,
1353513600, 1353600000, 1353859200, 1353945600, 1354032000, 1354118400,
1354204800, 1354464000, 1354550400, 1354636800, 1354723200, 1354809600,
1355068800, 1355155200, 1355241600, 1355328000, 1355414400, 1355673600,
1355760000, 1355846400, 1355932800, 1356019200, 1356278400, 1356451200,
1356537600, 1356624000, 1356883200, 1357056000, 1357142400, 1357228800,
1357488000, 1357574400, 1357660800, 1357747200, 1357833600, 1358092800,
1358179200, 1358265600, 1358352000, 1358438400, 1358697600), tzone = """", tclass = c(""POSIXct"",
""POSIXt"")), .indexCLASS = c(""POSIXct"", ""POSIXt""), tclass = c(""POSIXct"",
""POSIXt""), .indexTZ = """", tzone = """", class = c(""xts"", ""zoo""))

plot.xts(SF, type='candles')
</code></pre>

<p>The candles are coming out very thin. How do I make them wider?</p>

<p>By looking into source code of plot.xts I see that xts internal function plot.ohlc.candles is called to plot OHLC data as candles, to which  ... is passed. H</p>

<pre><code>plot.ohlc.candles(x, bar.col = bar.col, candle.col = candle.col, 
            ...)
</code></pre>

<p>However, if I try to set argument width=0.5 in my plot.xts function call, I get warnings that width is not graphic parameter </p>

<pre><code>Warning messages:
1: In plot.window(...) : ""width"" is not a graphical parameter
2: In plot.xy(xy, type, ...) : ""width"" is not a graphical parameter
3: In axis(1, at = xycoords$x, labels = FALSE, col = ""#BBBBBB"", ...) :
  ""width"" is not a graphical parameter
4: In axis(1, at = xycoords$x[ep], labels = names(ep), las = 1, lwd = 1,  :
  ""width"" is not a graphical parameter
5: In axis(2, ...) : ""width"" is not a graphical parameter
6: In title(width = 0.5) : ""width"" is not a graphical parameter
There were 12 warnings (use warnings() to see them)
&gt; warnings()
Warning messages:
1: ""width"" is not a graphical parameter
2: ""width"" is not a graphical parameter
3: ""width"" is not a graphical parameter
4: ""width"" is not a graphical parameter
5: ""width"" is not a graphical parameter
6: ""width"" is not a graphical parameter
7: ""width"" is not a graphical parameter
8: ""width"" is not a graphical parameter
9: ""width"" is not a graphical parameter
10: ""width"" is not a graphical parameter
11: ""width"" is not a graphical parameter
12: ""width"" is not a graphical parameter
</code></pre>

<p>I am unable to use quantmod or xtsExtra since I am trying to plot two different plots (one timeseries and other normal XY line graph) side by side.</p>
"
448,bad value in calculation output,"<p>The following code outputs ""3"". I was expecting ""1"".</p>

<pre><code>echo $resultado.""\n""; // show 2
$valor = $resultado * ($resultado - 1 / 2);

echo $valor.""\n""; // show 3, and should be 1
</code></pre>

<p>Why does this happen?</p>
"
449,php income breakdown report problems,"<p>I have to create a report which breaksdown membership fees over the fiscal years.</p>

<p>Fiscal year starts on the 31 July
membership length is 1 or 2 years.</p>

<p>I would like to pro-rate the membership to determine how much of their dues belong in which fiscal year.</p>

<p>for example, a 1 year membership depending on when the member started, would span 2 fiscal years.</p>

<p>Ie 365 days, 50 days in fiscal year 1 and 315 in the second fiscal year. So the total would be (Dues/365) x 50 for year 1 and (Dues/365) year 2.</p>

<p>How could I reflect that in my report?</p>

<p>Thanks!</p>

<p>Edit:</p>

<p>this is an actual example form the current membership. Fiscal year is on the 31 of July</p>

<p>Member 1, dues $50, received 09/10/2009, days 730,
Breakdown Fy2010 20.82, fy2011 25.00, fy2012 4.18</p>
"
450,Looping Cox regression model over several predictor variables,"<p>I need to run cox regression model for several variables, so I want write a loop to realize it.
But it doesn't work anyway.
Below is my code used</p>

<pre><code>names(Gen) 
varlist &lt;- names(hsb2)[8:11]  ## get the variables i want to involve in loop
models &lt;- lapply(varlist, function(x) {
    coxph(substitute(Surv(Time, Status) ~ i, list(i = as.name(x))), data = Gen, ties=""efron"")
})
</code></pre>

<p>I got the error information as </p>

<pre><code>errors in terms.default(formula, special, data = data) : 
  no terms component nor attribute
</code></pre>

<p>Any one has the idea of how to solve this problem or how to write the codes?</p>
"
451,Discrete and Continuous Classifier on Sparse Data,"<p>I'm trying to classify an example, which contains discrete and continuous features. Also, the example represents sparse data, so even though the system may have been trained on 100 features, the example may only have 12.</p>

<p>What would be the best classifier algorithm to use to accomplish this? I've been looking at Bayes, Maxent, Decision Tree, and KNN, but I'm not sure any fit the bill exactly. The biggest sticking point I've found is that most implementations don't support sparse data sets <em>and</em> both discrete and continuous features. Can anyone recommend an algorithm  and implementation (preferably in Python) that fits these criteria?</p>

<p>Libraries I've looked at so far include:</p>

<ol>
<li><a href=""http://www.ailab.si/orange/"" rel=""nofollow"">Orange</a> (Mostly academic. Implementations not terribly efficient or practical.)</li>
<li><a href=""http://www.nltk.org/"" rel=""nofollow"">NLTK</a> (Also academic, although has a good Maxent implementation, but doesn't handle continuous features.)</li>
<li><a href=""http://www.cs.waikato.ac.nz/~ml/weka/"" rel=""nofollow"">Weka</a> (Still researching this. Seems to support a broad range of algorithms, but has poor documentation, so it's unclear what each implementation supports.)</li>
</ol>
"
452,Lower bounds for learning in the membership query and counterexample model,"<p>Dana Angluin (<a href=""http://www.sciencedirect.com/science/article/pii/0890540187900526"">1987</a>; <a href=""http://www.cse.iitk.ac.in/users/chitti/thesis/references/learningRegSetsFromQueriesAndCounterExamples.pdf"">pdf</a>) defines a learning model with membership queries and theory queries (counterexamples to a proposed function). She shows that a regular language that is represented by a minimal DFA of $n$ states is learnable in polynomial time (where the proposed functions are DFAs) with $O(mn^2)$ membership-queries and at most $n1$ theory-queries ($m$ is the size of the largest counter-example provided by the tutor). Unfortunately, she does not discuss lower bounds.</p>

<p>We can generalize the model slightly by assuming a magical tutor that can check equality between arbitrary functions and provide counterexamples if different. Then we can ask how hard it is to learn classes bigger than regular languages. I am interested in this generalization and the original restriction to regular languages.</p>

<p><strong>Are there any known lower bounds on the number of queries in the membership and counterexample model?</strong></p>

<p>I am interested in lower bounds on the number of membership queries, theory queries, or trade-offs between the two. I am interested in lower-bounds for any class of functions, even for more complicated classes than regular languages.</p>

<p>If there are no lower-bounds: <strong>Are there known bariers to proving query lower bounds in this model?</strong></p>

<hr>

<h3>Related questions</h3>

<p><a href=""http://cs.stackexchange.com/q/118/55"">Are there improvements on Dana Angluin's algorithm for learning regular sets</a></p>
"
453,Evaluate math expression in string? (NSString),"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stackoverflow.com/questions/4892152/what-is-a-fast-c-or-objective-c-math-parser"">What is a fast C or Objective-C math parser?</a>  </p>
</blockquote>



<p>I have a NSString which represents a calculation eg. <code>@""(10+10)*2""</code> and I want to evaluate the string as if it was actually something like this;</p>

<pre><code>double result = (10+10)*2;
</code></pre>

<p>What is the most straightforward approach to take in iOS?</p>
"
454,Numpy Install Mac Osx Python,"<p>So I am using python for my Linear Algebra course for programming. For the tools we need I am trying to install NumPy. I used this website to guide me through the installation process.
<a href=""http://math.berkeley.edu/~shaowei/python.html"" rel=""nofollow"">http://math.berkeley.edu/~shaowei/python.html</a>
I followed these steps:
1. Already had Python 2.7 installed</p>

<ol>
<li>Nose</li>
</ol>

<p>Nose is needed for running tests within Python. The website is at <a href=""http://readthedocs.org/docs/nose/en/latest/"" rel=""nofollow"">http://readthedocs.org/docs/nose/en/latest/</a>. </p>

<p>Click on the ""Download"" link on the right sidebar, and click on the folder to the latest version of Nose. For me, it led me to the folder <a href=""http://pypi.python.org/pypi/nose/1.1.2"" rel=""nofollow"">http://pypi.python.org/pypi/nose/1.1.2</a>. </p>

<p>Download the ""tar.gz"" file you see there. Unpack the file.</p>

<p>From Terminal, navigate into the folder you just unpacked and type ""python setup.py install"".
3. NumPy</p>

<p>NumPy is a python package for mathematical computations.</p>

<p>I started from the links on <a href=""http://www.scipy.org/Installing_SciPy/Mac_OS_X"" rel=""nofollow"">http://www.scipy.org/Installing_SciPy/Mac_OS_X</a> and eventually arrived at the SourceForge download site <a href=""http://sourceforge.net/projects/numpy/files/NumPy/1.6.1/"" rel=""nofollow"">http://sourceforge.net/projects/numpy/files/NumPy/1.6.1/</a>.</p>

<p>From there, I chose the Mac OS X installer ""numpy-1.6.1-py2.7-python.org-macosx10.3.dmg"". </p>

<p>From Python, run the following test to make sure NumPy is correctly installed.</p>

<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; np.test('full')
</code></pre>

<p>When I went to Idle and typed in</p>

<pre><code>&gt;&gt;&gt; import numpy as np
</code></pre>

<p>This is the error I received:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;string&gt;"", line 1, in &lt;module&gt;
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/__init__.py"", line 137, in &lt;module&gt;
    import add_newdocs
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 9, in &lt;module&gt;
    from numpy.lib import add_newdoc
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 4, in &lt;module&gt;
    from type_check import *
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 8, in &lt;module&gt;
    import numpy.core.numeric as _nx
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/core/__init__.py"", line 5, in &lt;module&gt;
    import multiarray
ImportError: dlopen(/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/core/multiarray.so, 2): no suitable image found.  Did find:
    /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/core/multiarray.so: no matching architecture in universal wrapper
</code></pre>

<p>I also tried it from the terminal and got the same error. I have a virtual environment installed - virtual env wrapper. I called the environment in the terminal by: </p>

<pre><code>source my_new_env/bin/activate
</code></pre>

<p>And then I tried to import numpy in the terminal that way, but then I got the error:</p>

<pre><code>(my_new_env)Megans-MacBook-Pro:~ MeganRCunninghan$ sudo python  -c 'import numpy; numpy.test()'
Traceback (most recent call last):
  File ""&lt;string&gt;"", line 1, in &lt;module&gt;
ImportError: No module named numpy
(my_new_env)Megans-MacBook-Pro:~ Me
</code></pre>

<p>I am not sure if I installed NumPy incorrectly or if my virtual environment is conflicting with Nose. If anyone has suggestions let me know!</p>
"
455,rbind 2 vectors of different lengths by their names,"<p>I have 2 vectors of different lengths:</p>

<pre><code>vec1 &lt;- rnorm(18, mean = 0.0018, sd = 0.0001)
names(vec1) &lt;- c(""CSF"", ""D10"", ""D13"", ""D16"", ""D18"", ""D1"", ""D21"", ""D22"", ""D3"", ""D5"", ""D7"", ""D8"", ""FGA"", ""PD"", ""PE"", ""TH"", ""TP"", ""vWA"")

vec2 &lt;- rnorm(20, mean = 0.0022, sd = 0.0002)
names(vec2) &lt;-  c(""CSF"", ""D10"", ""D12"", ""D13"", ""D16"", ""D18"", ""D19"", ""D1"", ""D21"", ""D22"", ""D2"", ""D2S"", ""D3"", ""D5"", ""D7"", ""D8"", ""FGA"", ""TH"", ""TP"", ""vWA"") 
</code></pre>

<p>I need to rbind these vectors by their names. When a name in one vector does not exist in the other, that should produce an NA.</p>

<p>Is there a simple way to do that?</p>
"
456,Use scipy.integrate.quad to integrate complex numbers,"<p>I'm using right now the scipy.integrate.quad to successfully integrate some real integrands. Now a situation appeared that I need to integrate a complex integrand. quad seems not be able to do it, as the other scipy.integrate routines, so I ask: is there any way to integrate a complex integrand using scipy.integrate, without having to separate the integral in the real and the imaginary parts? </p>

<p>Thanks.</p>
"
457,List of Defined Variables in R,"<p>I'm using R in linux, command line only.</p>

<p>Coming back to a project after some time, I have forgotten the variable names I used, and the R command history doesn't contain them. </p>

<p>I seem to remember there being a command which lists all user defined variables, but don't remember what it is, and cannot find it on the web.</p>

<p>How can I list all the user defined variables in R?</p>
"
458,compute means of a group by factor,"<p>Is there a way that this can be improved, or done more simply?</p>

<pre><code>means.by&lt;-function(data,INDEX){
  b&lt;-by(data,INDEX,function(d)apply(d,2,mean))
  return(structure(
    t(matrix(unlist(b),nrow=length(b[[1]]))),
      dimnames=list(names(b),col.names=names(b[[1]]))
  ))
}
</code></pre>

<p>The idea is the same as a SAS MEANS BY statement.  The function 'means.by' takes a data.frame and an indexing variable and computes the mean over the columns of the data.frame for each set of rows corresponding to the unique values of INDEX and returns a new data frame with with the row names the unique values of INDEX.</p>

<p>I'm sure there must be a better way to do this in R but I couldn't think of anything.</p>
"
459,Convert little endian string to integer,"<p>I have read samples out of a wave file using the wave module, but it gives the samples as a string, it's out of wave so it's little endian (for example, '`\x00').</p>

<p>What is the easiest way to convert this into a python integer, or a numpy.int16 type? (It will eventually become a numpy.int16, so going directly there is fine).</p>

<p>Code needs to work on little endian and big endian processors.</p>
"
460,bar width in ggplot2 geom_bar,"<p>I am trying to produce plots with a loop.</p>

<pre><code>l1&lt;-factor(rep(letters,4))
n1&lt;-abs(rnorm(104))*10000
b1&lt;-rep(c(""1"",""2"",""3"",""4"",""5"",""6"",""7"",""8""),c(2,2,11,24,11,20,33,1))
k1&lt;-rep((rep(c(""A"",""B"",""C"",""D""),c(2,3,4,4))),8)
my.df&lt;-data.frame(l1,b1,k1,n1)                            #make a dataframe

names(my.df)&lt;-c(""letter"",""branch"",""ltrtype"",""number"")     #factor names
library(ggplot2)

branch.list&lt;-unique(my.df$branch)
sayi&lt;-length(branch.list)                                 # list of factor:letters

for (i in 1:sayi) {

branch.iter&lt;-branch.list[i]
my.df.plot&lt;-subset(my.df,my.df$branch==branch.iter,drop=T)

my.df.plot$branch&lt;-factor(my.df.plot$branch)               #So that unused levels don't show up
my.df.plot$letter&lt;-factor(my.df.plot$letter)               #So that unused levels don't show up
my.df.plot$ltrtype&lt;-factor(my.df.plot$ltrtype)             #So that unused levels don't show up
my.df.plot$number&lt;-as.numeric(as.character(my.df.plot$number))
my.df.plot&lt;-data.frame(my.df.plot)

myfilename&lt;-paste(branch.iter,"".jpeg"",sep="""")
jpeg(file=myfilename)

cizim&lt;-ggplot(my.df.plot,aes(letter,number,fill=ltrtype))
cizim&lt;-cizim + geom_bar(width = 1, position = ""fill"", binwidth = 1) +     facet_grid(ltrtype~.)
cizim&lt;-cizim + opts(title=branch.iter)

print(cizim)
dev.off()

}
</code></pre>

<p>(Q1): When number of levels in x-axis change width of bars change
How can i prevent this and make bar width in every plot same?</p>

<p><img src=""http://img411.imageshack.us/i/95325388.jpg/"" alt=""alt text""></p>

<p><img src=""http://img411.imageshack.us/i/91510133.jpg/"" alt=""alt text""></p>

<p>(Q2): when i=7 R gives following warning:</p>

<pre><code>(data$ymin == 0)) warning(""Filling not well defined when ymin != 0"") : 
missing value where TRUE/FALSE needed
</code></pre>

<p>what can i do about it?</p>

<p>(Q3): Is there an easier way to drop unused levels in such a case so i don't have to use</p>

<pre><code> my.df.plot$branch&lt;-factor(my.df.plot$branch)
</code></pre>

<p>everytime?</p>

<p>Thanks</p>
"
461,calculating probability distribution,"<p>I have a simple (may be stupid) question. I want to calculate KullbackLeibler divergence on two documents. It requires probability distribution of each document.</p>

<p>I do not know how to calculate probability for each document. Any simple answer with layman example would be much appreciated. </p>

<p>Let's say we have follow two documents:</p>

<pre><code>1 - cross validated answers are good 
2 - simply validated answers are nice
</code></pre>

<p>(wording of the documents is just bla bla to give you an example)</p>

<p>How do we calculate probabilities for these documents? </p>

<p>Let's say we add one more document:</p>

<pre><code>3 - simply cross is not good answer
</code></pre>

<p>If we add another document then how would it impact probability distribution? </p>

<p>Thanks</p>
"
462,"Read tab delimited file with unusual characters, then write an exact copy","<h2>The problem</h2>

<p>I have a tab delimited input file that looks like so:</p>

<pre><code>Variable [1]    Variable [2]
111    Something
Nothing    222
</code></pre>

<p>The first row represents column names and the two next rows represents column values. As you can see, the column names includes both spaces and some tricky signs.</p>

<p>Now, what I want to do is to import this file into R and then output it again to a new text file, making it look exactly the same as the input. For this purpose I have created the following script (assuming that the input file is called ""Test.txt""):</p>

<pre><code>file &lt;- ""Test.txt""
x &lt;- read.table(file, header = TRUE, sep = ""\t"")
write.table(x, file = ""TestOutput.txt"", sep = ""\t"", col.names = TRUE, row.names = FALSE)
</code></pre>

<p>From this, I get an output that looks like this:</p>

<pre><code>""Variable..1.""  ""Variable..2.""
""1""    ""111""    ""Something""
""2""    ""Nothing""    ""222""
</code></pre>

<p>Now, there are a couple of problems with this output.</p>

<ol>
<li>The ""["" and ""]"" signs have been converted to dots.</li>
<li>The spaces have been converted to dots.</li>
<li>Quote signs have appeared everywhere.</li>
</ol>

<p><em>How can I make the output file look exactly the same as the input file?</em></p>

<h2>What I've tried so far</h2>

<p>Regarding problem number one and two, I've tried specifying the column names through creating an internal vector, <code>c(""Variable [1]"", ""Variable [2]"")</code>, and then using the <code>col.names</code> option for <code>read.table()</code>. This gives me the exact same output. I've also tried different encodings, through the <code>encoding</code> option for <code>table.read()</code>. If I look at the internally created vector, mentioned above, it prints the variable names as they should be printed so I guess there is a problem with the conversion between the ""text -> R"" and the ""R -> text"" phases of the process. That is, if I look at the data frame created by <code>read.table()</code> without any internally created vectors, the column names are wrong.</p>

<p>As for problem number three, I'm pretty much lost and haven't been able to figure out what I should try.</p>
"
463,R filtering out a subset,"<p>I have a data.frame A
and a data.frame B which contains a subset of A</p>

<p>How can I create a data.frame C which is data.frame A with data.frame B excluded?
Thanks for your help.</p>
"
464,"Best multivariate polynomial fit in Matlab, Mathematica or R","<p>I have a dataset (x,y) where x is a n-dimensional vector and y is an m-dimensional vector. (m=3, n>2)
My goal is to find the best polynomial in x fitting the (x,y) dataset.</p>

<p>The dimension of x is pretty big (right now it is 25), and I don't want to enter manually all the possibilities (ie x1*x3*x5, x1*x4*x6, ...). I can use Matlab, Mathematica and R. How can I do this?</p>

<p>Also, I would be interested in hearing your suggestions about the following problem: how can I choose from the result the most relevant coefficients? (maybe x1*x2 is more relevant than x2*x3)</p>

<p>Thank you</p>
"
465,Understanding custom abs function in JavaScript,"<p>I hope my logic isn't flawed but I'm reading the Definitive Guide to JavaScript and I don't understand how this custom abs function works...</p>

<pre><code>function abs(x) {
  if (x &gt;= 0) {
    return x;
  } else {
    return -x;
  }
}
</code></pre>

<p>I recrafted it using a ternary operator in an attempt to understand it...</p>

<pre><code>var res = (x &gt;= 0) ? x : -x;
return res;
</code></pre>

<p>... but I still don't get how it works.</p>

<p>Say I use -10 as x, how does it return +10? How does the sign reverse?</p>
"
466,Python augmented assignment issue,"<p>i ran into something interesting about the python augmented assignment <code>+=</code></p>

<p>it seems to be automatic data type conversion is not always done for <code>a += b</code> if a is a 'simpler' data type, while <code>a = a + b</code> seems to work always</p>

<p>cases where the conversion is done</p>

<pre><code>a = 1
b = 1j

a = 1
b = 0.5
</code></pre>

<p>case where the conversion is not done</p>

<pre><code>from numpy import array
a = array([0, 0 ,0])
b = array([0, 0, 1j])
</code></pre>

<p>after <code>a += b</code>, <code>a</code> remains as integer matrix, instead of complex matrix</p>

<p>i used to think <code>a += b</code> is the same as <code>a = a + b</code>, what is the difference of them in the underlying implementation?</p>
"
467,Statistical Methods: Analysis of Normal Measurements,"<p>Show that, if $\sigma$ is unknown, the likelihood ratio statistic for testing a value of $\alpha$ is given by $$D = n \log\left(1 + \frac{1}{n-1}T^2\right)\;,$$ where $$T = \frac{\hat{} -\alpha}{\sqrt{s^2/n}}$$</p>

<p>So far, I have the following:
$\hat\alpha=\overline{y}$ and $\hat\sigma=\sqrt{\frac{\sum \left ( y_i-\bar{y} \right )^2}{n-1}}$.</p>

<p>Now, when I plug this in to my ratio, I have:
$$D=2\left [ l(\hat{\mu}, \hat{\sigma})-l(\mu_0,\hat{\sigma}) \right ]=\frac{n(\bar{y}-\mu_0)^2}{\hat{\sigma}}=\frac{(\bar{y}-\mu_0)^2}{c\hat{\sigma}}\text{ where }c=\frac{1}{n}$$</p>

<p>So, just to clarify the following things:</p>

<ol>
<li><p>I accidentally typed $n-1$. I meant $n$ in the denominator for $\sigma$.</p></li>
<li><p>My log-likelihood function is: $l(\mu, \sigma)=-n\log(\sigma)-\frac{\sum{(y_i-\bar{y})^2}}{2\sigma^2}$</p></li>
<li><p>When I expand my ratio statistic and simplify, the logs cancel because they are identical, and I am left with the equivalent expression of $T^2$. I don't understand how I am supposed to get the expression that I am asked for. Any ideas of what I am doing wrong?</p></li>
</ol>
"
468,How can I create a histogram for all variables in a data set with minimal effort in R?,"<p>Exploring a new data set: What is the easiest, quickest way to visualise many (all) variables?</p>

<p>Ideally, the output shows the histograms next to each other with minimal clutter and maximum information. Key to this question is flexibility and stability to deal with large and different data sets. I'm using RStudio and usually deal with large and messy survey data.</p>

<p>One example which comes out of the box of <code>Hmisc</code> and works quite well here is:</p>

<pre><code>library(ggplot2)
str(mpg)

library(Hmisc)
hist.data.frame(mpg)
</code></pre>

<p>Unfortunately, somewhere else I run into problems with data lables (Error in plot.new() : figure margins too large). It also crashed for a larger data set than <code>mpg</code> and I haven't figured out how to control binning. Moreover, I'd prefer a flexible solution in <code>ggplot2</code>. Note that I just started learning R and am used to the comfortable solutions provided by commercial software.</p>

<p>More questions on this topic:</p>

<p><a href=""http://stackoverflow.com/questions/6428432/r-histogram-too-many-variables"">R histogram - too many variables</a></p>

<p>...?</p>
"
469,Automatic float32 promotion in numexpr,"<p>Consider the following NumPy array of dtype <code>float32</code>:</p>

<pre><code>In [29]: x = numpy.arange(10, dtype=numpy.float32)
</code></pre>

<p>When I multiply it by <code>2</code> using <code>pytables.Expr</code>, I get a <code>float32</code> array back:</p>

<pre><code>In [30]: tables.Expr('x * 2').eval().dtype
Out[30]: dtype('float32')
</code></pre>

<p>Yet when I multiply it by <code>2.0</code>, I get a <code>float64</code> array back:</p>

<pre><code>In [31]: tables.Expr('x * 2.0').eval().dtype
Out[31]: dtype('float64')
</code></pre>

<p>Is there any way to specify the floating-point literal in the above expression in a way that would <em>not</em> cause the result to be promoted to <code>float64</code>?</p>

<p>More generally, I have an expression using <code>float32</code> arrays, and I want to ensure that the result is also of type <code>float32</code> (I don't mind <code>float64</code> being used for intermediate calculations, but I can't afford to store the results as <code>float64</code>). How do I do this?</p>
"
470,Does the average European adult weigh 71kg?,"<p>The Daily Mail says that the average weight of both males and females in Europe is UN 70.8kg in this article: <a href=""http://www.dailymail.co.uk/sciencetech/article-2160934/Overweight-obese-threaten-world-food-security-study-warns.html"" rel=""nofollow"">http://www.dailymail.co.uk/sciencetech/article-2160934/Overweight-obese-threaten-world-food-security-study-warns.html</a> giving no sources, but mentioning the WHO about other statistics in the previous paragraph.</p>

<p>When I try finding backing for these sources I run into a dead end: <a href=""http://www.google.no/search?q=average+european+weight+70.8&amp;oq=average+european+weight+70.8&amp;sourceid=chrome&amp;ie=UTF-8"" rel=""nofollow"">http://www.google.no/search?q=average+european+weight+70.8&amp;oq=average+european+weight+70.8&amp;sourceid=chrome&amp;ie=UTF-8</a>
All of the sources of this information seem to come from newspapers reporting the same story, and so I'm afraid this circular referencing.</p>

<p>Is there any backing for this fact, or any alternative statistic? Can we trust this source? Are there reliable sources for this statistic?</p>
"
471,accessing matrix in R,"<p>i have a matrix in R as follows:</p>

<pre><code>                     YITEMREVENUE    XCARTADD XCARTUNIQADD XCARTADDTOTALRS
YITEMREVENUE         1.0000000000 -0.02630016 -0.01811156    0.0008988723      
XCARTADD            -0.0263001551  1.00000000   0.02955307   -0.0438881639   
XCARTUNIQADD        -0.0181115638  0.02955307   1.00000000    0.0917359285  
XCARTADDTOTALRS      0.0008988723 -0.04388816   0.09173593    1.0000000000       
</code></pre>

<p>i want to list out the names of the columns with negative values only.. my output should look like:</p>

<pre><code>YITEMREVENUE -  XCARTADD XCARTUNIQADD  
XCARTADD     -  YITEMREVENUE XCARTADDTOTALRS  
XCARTUNIQADD -  YITEMREVENUE   
XCARTADDTOTALRS - XCARTADD
</code></pre>

<p>is it possible in R? </p>
"
472,Sanitise R query from web users,"<p>We're building a web form which will allow (trusted) users to input their own R queries. They will be doing stats analysis against a database.</p>

<p>Questions:</p>

<ol>
<li>How dangerous is this, in its basic form? I'm new to R, so - what's the worst they could do? (Assume the database connection is unprivileged).</li>
<li>Is there an easy way to sanitise the input, to remove the biggest risks?</li>
<li>Is it possible to sanitise inputs to the point that we could open this up to the public? We couldn't risk DOS attacks, for instance.</li>
</ol>
"
473,R markdown: Accessing variable from code chunk (variable scope),"<p>In R markdown (knitr package), can I access a variable within the body of the document that was calculated in a code chunk?</p>
"
474,Predictive Modeling - Software Best Practice,"<p>I would like to know what are the best practices for building Predictive Modeling solutions organically ?
Some of the questions I have are :-</p>

<ul>
<li>If I have multiple R model files, what are efficient ways of storing them ?
<ul>
<li>Save as .Rdata files on file system</li>
<li>Serialize to a DB as binary objects</li>
</ul></li>
<li>Since data is processed to create an interim model specific format, is it helpful to use such paradigms as PMML ?</li>
<li>Also, should one consider such practices as MVC (I'm not a trained software developer, so any insights into such development practices would be very helpful)</li>
</ul>

<p>I apologize for the open-ended nature of this question. I wish to understand even simple things as recommended folder structure for data staging, model store, scripts collection and such other elements of a data mining solution.</p>

<p>I would be very grateful to members of the community for sharing their experiences and recommendations.
Thank you for your time.</p>
"
475,Numeral domains in Haskell,"<p>Haskell is more mathematical than many languages because of lambda-calculus, but I think the domains are incomplete for number: we have <code>Integer</code> and <code>Float</code>, for example, but not <code>Positive</code> or <code>Negative</code>, or <code>[1..5]</code> as a domain.
This sometimes makes functions unsafe while the compiler could have catched the type error. For example: <code>5</code>mod<code>0</code> outputs <code>*** Exception: divide by zero</code> at run-time only.
<code>mod :: Integral a =&gt; a -&gt; a -&gt; a</code> but we could have something like <code>mod :: Integral a, a != 0 =&gt; a -&gt; a -&gt; a</code>; something like a guard or an interval or another datatype... In a game, I want my character to have a positive number  for its life. Or from 0 to 100, not under, not upper. When he gets hit, I need to call the ugly <code>positive x = if x &gt; 0 then x else 0</code>. Even C has <code>signed</code> and <code>unsigned</code>.</p>

<p>Is it a weakness or are they reasons why there are no ""interval"" domains? Is there a package fixing this?</p>
"
476,Needing to find a variance from a data set below,"<p>I am trying to get the variance of the data set below. I get a negative number upon applying 1/2(2.5)^2 + 1(3)^2 + 0.5(4.5)^2 + 1(5)^2 + 1(6)^2 - (17.5)^2 = variance, but a negative and weird number.</p>

<p>Please use this data below from the link, the problem is as is and the sample space is what I also got. But I got a mean of 17.5 and not sure how to to do the variance.</p>

<p><a href=""http://answers.yahoo.com/question/index?qid=20100120103118AAld5Yx"" rel=""nofollow"">http://answers.yahoo.com/question/index?qid=20100120103118AAld5Yx</a></p>
"
477,Precomputed Kernels with LibSVM in Python,"<p>I've been searching the net for ~3 hours but I couldn't find a solution yet. I want to give a precomputed kernel to libsvm and classify a dataset, but:</p>

<ul>
<li><p>How can I generate a precomputed kernel? (for example, what is the basic precomputed kernel for <a href=""http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"">Iris data</a>?)</p></li>
<li><p>In the libsvm documentation, it is stated that:</p>

<p>For precomputed kernels, the first element of each instance must be
the ID. For example,</p>

<pre><code>        samples = [[1, 0, 0, 0, 0], [2, 0, 1, 0, 1], [3, 0, 0, 1, 1], [4, 0, 1, 1, 2]]
        problem = svm_problem(labels, samples)
        param = svm_parameter(kernel_type=PRECOMPUTED)
</code></pre></li>
</ul>

<p>What is a ID? There's no further details on that. Can I assign ID's sequentially?</p>

<p>Any libsvm help and an example of precomputed kernels really appreciated.</p>
"
478,aggregating time series in R,"<p>I have the following OHLC data (by 3-minute intervals)</p>

<pre><code>library(tseries)
library(xts)
library(quantmod)
&gt; str(tickmin)
An xts object from 2010-06-30 15:47:00 to 2010-09-08 15:14:00 containing:
  Data: num [1:8776, 1:5] 9215 9220 9205 9195 9195 ...
 - attr(*, ""dimnames"")=List of 2
  ..$ : NULL
  ..$ : chr [1:5] ""zv.Open"" ""zv.High"" ""zv.Low"" ""zv.Close"" ...
  Indexed by objects of class: [POSIXct,POSIXt] TZ: 
  xts Attributes:  
 NULL


&gt;tickmin
2010-09-08 15:02:00        20
2010-09-08 15:04:00        77
2010-09-08 15:08:00        86
2010-09-08 15:11:00         7
2010-09-08 15:14:00        43
&gt; start(tickmin)
[1] ""2010-06-30 15:47:00 EDT""
&gt; end(tickmin)
[1] ""2010-09-08 15:14:00 EDT""
</code></pre>

<p>I am trying to aggregate it using the following:</p>

<pre><code>&gt; by &lt;-timeSequence(from = start(tickmin), to = end(tickmin), format=""%Y-%m-%d %H%M"", by = ""day"")
&gt;by
[61] [2010-08-29 19:47:00] [2010-08-30 19:47:00] [2010-08-31 19:47:00]
[64] [2010-09-01 19:47:00] [2010-09-02 19:47:00] [2010-09-03 19:47:00]
[67] [2010-09-04 19:47:00] [2010-09-05 19:47:00] [2010-09-06 19:47:00]
[70] [2010-09-07 19:47:00]

&gt; aggregate(Vo(tickmin),by,sum)
Error: length(time(x)) == length(by[[1]]) is not TRUE
</code></pre>

<p>..would appreciate any suggestions on how I can fix the error.</p>
"
479,Rock Paper Scissors for arbitrary odd number of elements,"<p>How do I efficiently create a rock-scissors-paper game for n elements, 
where n is any odd number >=3. </p>

<p>In other words, I want a non-transitive complete ordering of n elements 
such that each element is greater than (n-1)/2 other elements and each 
element is lesser than (n-1)/2 other elements. </p>
"
480,"Can I access the colours of the R console, from R code?","<p>I sometimes have up to 4 sessions of R, each running a different piece of software, between which I must change often. It would be helpful if the colour of the console background (or text) were to be different depending on which code was loaded, automatically, so that I could locate the window easily on my (cluttered) desktop and avoid typing in the wrong commands into the wrong console in my fast moving financial activities. </p>

<p>Any way to do this from within R code? Currently I am menuing up and changing the colours manually but it's tedious, especially that, under Windows, the R colour dialogue box is 4 lines deep and there are something like 500 colours. </p>
"
481,"How to calculate the 95% lower bound, T and P","<p>For a minitab table like this:</p>

<p><img src=""http://i.stack.imgur.com/2PGis.jpg"" alt=""enter image description here""></p>

<p>How can I compute the 95% Lower Bound, T and P values? What formula do I have to use?</p>
"
482,In R can I find the environment associated with a lazy argument?,"<p>Sorry this is a little complicated.</p>

<p>I want to capture an argument expression, but also know which environment it should be evaluated in. Something like this:</p>

<pre><code>make.promise = function(x = print(b), b = 7) {
    expr = substitute(x)
    env  = parent.frame()

    function() {
        eval(expr, env)
    }
}

p1 = (
    function() {
        a = 2
        make.promise(print(a))
    }
)()

p2 = make.promise()
</code></pre>

<p>The problem is, if no argument is supplied for <code>x</code>, its environment becomes the local environment of <code>make.promise()</code>, and I don't know how to detect that. Is there a function other than <code>substitute</code> I could use that also captures the environment?</p>
"
483,"how to implement this Integrate[x^n E^(x - 1), {x, 0, 1}] in matlab","<p>hi I've search and read but  I couldn't solve this one in matlab.
please help me out to solve this problem 
integral_0^1 x^n e^(x-1) dx</p>
"
484,Sweave for python,"<p>I've recently started using <a href=""http://www.stat.uni-muenchen.de/~leisch/Sweave/"">Sweave</a>* for creating reports of analyses run with R, and am now looking to do the same with my python scripts. </p>

<p>I've found references to <a href=""http://romainfrancois.blog.free.fr/index.php?post/2009/01/21/Python-and-Sweave"">embedding python in Sweave</a> docs, but that seems like a bit of a hack. Has anyone worked out a better solution, or is there an equivalent for python I'm not aware of?</p>

<p>* <em>Sweave is a tool that allows to embed the R code for complete data analyses in latex documents</em></p>
"
485,Matrix multiplication gives unsual result in Python (SciPy/PyLab),"<p>I'm new to Python, and a bit rusty with my linear algebra, so perhaps this is a simple question. I'm trying to implement a Taylor Series expansion on a Matrix to compute exp(A), where A is just a simple 3x3 matrix. The formula, BTW for this expansion is sum( A^n / n! ).</p>

<p>My routine works alright up to n=9, but at n=10, the numbers in the Matrix suddenly become negative. This is the problem.</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>A**9
      matrix([[ 250130371,  506767656,  688136342],
              [ 159014912,  322268681,  437167840],
              [ 382552652,  775012944, 1052574077]])</p>
      
      <p>A**10
      matrix([[-1655028929,  1053671123, -1327424345],
              [ 1677887954,  -895075635,   319718665],
              [ -257240602,  -409489685, -1776533068]])</p>
    </blockquote>
  </blockquote>
</blockquote>

<p>Intuitively A^9 * A should produce larger numbers for each member of the matrix, but as you can see, A^10 isn't giving that result. </p>

<p>Any ideas?</p>

<pre><code>from scipy import *
from numpy import *
from numpy.linalg import *
#the matrix I will use to implement exp(A)
A = mat('[1 3 5; 2 5 1; 2 3 8]')
#identity matrix
I = mat('[1 0 0; 0 1 0; 0 0 1]')
#first step in Taylor Expansion (n=0)
B = I
#second step in Taylor Expansion (n=1)
B += A
#start the while loop in the 2nd step
n = 2
x=0
while x&lt;10:
    C = (A**n)/factorial(n)
    print C
    print "" ""
    n+=1
    B+= C
    print B
    x+=1

print B
</code></pre>

<p>Thanks for any help you can give!</p>
"
486,find point of x contiguous responses,"<p>I'm trying to find the point at which participants reach 8 contiguous responses in a row. The data is from a category learning task so the variable would look like:</p>

<pre><code>R&gt; data
 [1] 1 1 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1..
</code></pre>

<p>I'm trying to find the trial number at which participants achieved our criterion for learning - 8 correct responses in a row (1 represent a correct response). So i would want to return 18 from the above example, since it's on the 18th trial that the participant reached 8 correct responses in row.</p>

<p>Sorry if this has been answered elsewhere. I looked around fair bit and found a few similar problems but nothing that I could figure out how to directly apply to my problem. I just started using R today (switching from SPSS) so I'm still learning. </p>

<p>Thanks in advance! Let me know if i need to provide more detail.</p>
"
487,Weak hypotheses in boosting method,"<p>What is the weak hypotheses generated during boosting?</p>
"
488,tweaking scale heatmap with ggplot2,"<p>I am a complete newbie with <code>R</code> and please forgive me if this already has been asked a gazillion times. I am trying to make a <code>heatmap</code> using <code>R</code>, following this <a href=""http://learnr.wordpress.com/2010/01/26/ggplot2-quick-heatmap-plotting/"" rel=""nofollow"">example</a>, which are <code>tsvs</code>.</p>

<p>This is an example. </p>

<pre><code>name sam1 sam2
a     0.2  0
b     0.1  0.05
c     0.3  0.06
</code></pre>

<p>Sorry, I can not post the graph that I get (because I am a newbie). </p>

<p>When the graph is made the scale is between 0 to 1 (the data is rescaled between 0 to 1 in <code>heatmap</code>), however I do not have any values bigger than 0.3 in my files, hence I want to know if it is possible to have a scale between 0  to 0.3 in <code>heatmap</code>. I am not sure if I am providing enough details here, please let me know if I need to put in some more details here. </p>

<p>basically I am using</p>

<pre><code>a &lt;- read.table(file = ""name"", sep =""\t"", header =T) 

a.m &lt;- melt(a)

a.m &lt;- ddply(a.m, .(variable), transform,  rescale = rescale(value))

(p &lt;- ggplot(a.m, aes(variable, transposons)) + 
      geom_tile(aes(fill = rescale), colour = ""yellow"") + 
      scale_fill_gradient(low = ""yellow"",  high = ""darkgreen""))
</code></pre>

<p>Any help is most appreciated, thanks in advance.    </p>
"
